{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Red Hat OpenShift on IBM Z and LinuxONE Workshop","text":"<p>Welcome to the Red Hat OpenShift on IBM Z and LinuxONE workshop. Below you can find the workshop agenda, presentations, and lab documentation.</p>"},{"location":"#agenda","title":"Agenda","text":"Activity Duration Presentation 1 - Overview ~ 45 minutes Break ~ 10 minutes Presentation 2 - Technical Overview ~ 45 minutes Connect to environment as a group 5-10 minutes Hands-on, self-paced labs Remainder of day <p>The lab environments will be available the day following the workshop.</p> <p>For example, If the workshop is on a Thursday, the environments will be available until 9PM ET Friday.</p>"},{"location":"#presentations","title":"Presentations","text":"<ul> <li>Presentation 1 - High Level Overview of Red Hat OpenShift on IBM Z</li> <li>Presentation 2 - Technical Overview</li> </ul>"},{"location":"#lab","title":"Lab","text":"<p>The lab can be found at the following GitHub link. This is a public repository, so feel free to come back to it at any time. </p> <ul> <li>OpenShift Overview Lab</li> </ul>"},{"location":"#workshop-environment-architecture","title":"Workshop Environment Architecture","text":"<p>Please visit this page to see the architecture of the workshop's lab environment.</p>"},{"location":"#workshop-owners","title":"Workshop Owners","text":"<ul> <li>Matt Mondics</li> <li>Paul Novak</li> <li> <p>Jacob Emery</p> </li> <li> <p>Email All Workshop Owners</p> </li> </ul>"},{"location":"lab-assignments/","title":"Lab Assignments","text":"<p>Please follow the connection instructions that are included in the OpenShift Overview Lab.</p>"},{"location":"lab/","title":"Hands-On Lab","text":"<p>You can find the lab instructions here: OpenShift Overview Lab.</p>"},{"location":"prerequisites/","title":"Prerequisites","text":""},{"location":"prerequisites/#github-account","title":"GitHub Account","text":"<p>If you wish to complete Deploying an Application from Source Code, you must have your own GitHub account. You can create one create one by clicking the Sign Up button on the GitHub homepage.</p>"},{"location":"workshop-architecture/","title":"Workshop Architecture Diagram","text":"<p>The OpenShift (OCP) on IBM Z environment used in this workshop is detailed in the diagram above.</p> <p>Note that this is not the recommended OpenShift architecture for high availablity or production. For OCP on Z reference architectures navigate to this link.</p> <p>The entire lab environment is behind the Washington Systems Center VPN. You are given a RHEL virtual machine with the Cisco VPN client installed and running which provides access to the WSC environment.</p> <p>There are 4 OpenShift clusters sharing running on this z14 sharing z/VM, IFLs, and other underlying infrastructure.</p> <p>Each OCP cluster is made up of 3 Control Planes and 3 Compute Nodes as shown in OCP Cluster 1 in the diagram. The Control Planes and Compute Nodes have a minimum of the resources shown for OCP cluster 1, although some clusters have more than the resources listed because of some more intensive applications running on them (Instana, Red Hat Advanced Cluster Security, Red Hat Quay, etc).</p> <p>All clusters are running on a z/VM 7.3 SSI cluster on a single LPAR of an IBM z16 (again, not recommended outside of POC/demo).</p> <p>There are various other support servers running as Linux guests that you use during these labs. These are outside of the OCP cluster itself, but take care of tasks such as LDAP, NFS storage, and a server with the <code>oc</code> command line installed that will let you connect to the various OpenShift clusters.</p>"},{"location":"lab001/lab001-1/","title":"Exploring the OpenShift Console","text":"<p>The OpenShift Container Platform web console is a user interface accessible from a web browser.</p> <p>Developers can use the web console to visualize, browse, and manage the contents of projects.</p> <p></p> <p>Administrators can use the web console to manage and monitor applications running on the cluster, along with the cluster itself.</p> <p></p> <p>The web console can be customized to suit an organization's needs, and when you log into the web console, you will only see the cluster resources that are available to you as allowed by the OpenShift Role Based Access Control (RBAC).</p> <p>The web console runs as a group of pods on the control plane nodes in the <code>openshift-console</code> project, along with a service exposed as a route. Like any other OpenShift applicationm the service is an internal load balancer that directs traffic to one of the OpenShift console pods. The route is what allows external access into the service, and it is the address you connect to when accessing the OpenShift web console.</p> <p>The OpenShift web console is a great example of how OpenShift itself is managed the same way as any other application running on the cluster.</p>"},{"location":"lab001/lab001-2/","title":"Connect to OCP and Authenticate","text":"<ol> <li> <p>In your Windows virtual machine, open a Firefox web browser.</p> </li> <li> <p>In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/</p> Note <p>You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized.</p> <p>Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe).</p> <p>You will likely need to do this twice due to how OpenShift reroutes Oauth requests. </p> Expand for screenshot <p> </p> <p>You will now see the OpenShift console login page.</p> <p></p> </li> <li> <p>Log in with the OpenShift credentials provided to you on the Lab Assignments page.</p> <p>Hint</p> <p>Your OpenShift credentials will be something like the following:</p> <ul> <li> <p>Username: userNN (where NN is your user number)</p> </li> <li> <p>Password: p@ssw0rd</p> </li> </ul> </li> </ol>"},{"location":"lab001/lab001-3/","title":"The Administrator Perspective","text":"<p>Take a moment to notice the following elements in the navigation bar:</p> <p></p> <p>Note</p> <p>These buttons display on each page of the OpenShift console. Note that the Applications button might be missing from your screen, depending on your credentials and which applications are currently deployed on the cluster.</p> <p>By default, the menu on the left side of the page should be activated and displaying the cluster menu.</p> <ol> <li> <p>In the left-side menu, select the Administrator perspective if it isn't already showing.</p> <p></p> <p>With the administrator menu showing, you are provided with a broad range of options to manage the OpenShift cluster and the applications running on it.</p> <p></p> Expand to Learn More About the Different Views <ul> <li> <p>Developer / Administrator toggle.  This lets you flip between which of the two perspectives you want to use.</p> </li> <li> <p>Home: Provides overview of projects, resources, and events in the scope of your credentials.</p> </li> <li> <p>Operators: Provides access to the OperatorHub to install new operators and also lets you view operators that are already installed.</p> </li> <li> <p>Workloads: Expands to provide access to many Kubernetes and OpenShift objects, such as pods, deployments, secrets, jobs and more.</p> </li> <li> <p>Networking: Provides access to services, routes, and ingresses required for external access to the cluster.</p> </li> <li> <p>Storage: Provides access to storage objects in the OpenShift cluster, such as PersistentVolumeClaims.</p> </li> <li> <p>Builds: View and create Build objects \u2013 use to transform input parameters into resulting objects.</p> </li> <li> <p>Pipelines: View and create Pipelines \u2013 Tekton-based CI/CD processes and objects. This will be missing if not installed in your OpenShift cluster.</p> </li> <li> <p>Monitoring: Access cluster resource Monitoring, Metrics, and Alerting.</p> </li> <li> <p>Compute: Access cluster infrastructure \u2013 Control &amp; Compute Nodes, Machines, and more.</p> </li> <li> <p>User Management: Access and manage Users, Groups, Roles, RoleBindings, Service Accounts, and more.</p> </li> <li> <p>Administration: View and edit cluster settings.</p> </li> </ul> <p>The Administrator perspective is the default view for the OpenShift console for users who have an administrative access level. This perspective provides visibility into options related to cluster administration, as well as a broader view of the projects associated with the currently logged-in user.</p> </li> <li> <p>In the Menu, click Home -&gt; Projects.</p> <p></p> <p>The rest of the page is populated by projects. A project has been created for you to work in named userNN-project (where NN is your user number).</p> <p>Note</p> <p>Any project starting with openshift- or kube- contain the workloads running the OpenShift platform itself.</p> </li> <li> <p>Click the userNN-project hyperlink (where NN is your user number).</p> <p>Tip</p> <p>With so many Projects displayed, you can use the search bar to find yours more easily.</p> <p>You will now see the Dashboard for your project.</p> <p></p> </li> <li> <p>Look through the Overview tab of your project.</p> <p>This displays information about what\u2019s going on in your project, such as CPU and memory usage, any alerts or crashlooping pods, an inventory of all the Kubernetes resources deployed in the project, and more. You won\u2019t see much information yet, as no workloads should be running in this project.</p> </li> <li> <p>Click the Workloads tab to the right of YAML.</p> <p>This page displays all of the workloads in your project, so it\u2019s empty for now.</p> Note <p>All objects in OpenShift are generated using YAML files. YAML (standing for Yet Another Markup Language) is meant to be a human-readable language for configuration files. Any OpenShift object such as Deployments, Services, Routes, and nearly everything else can be modified by directly editing their YAML file in either the console or command line.</p> <p>Workloads are typically created by developers, so in the next section, you will swap to the developer perspective to deploy a an application. You will return to the administrator perspective later in this lab.</p> </li> </ol>"},{"location":"lab001/lab001-4/","title":"The Developer Perspective","text":"<ol> <li> <p>In the left-side Menu, click the Administrator dropdown, and select Developer.</p> <p></p> <p>The Developer perspective provides views and workflows specific to developer use cases, while hiding many of the cluster management options typically used by administrators. This perspective provides developers with a streamlined view of the options they typically use.</p> <p></p> Expand to Learn More About the Different Views <ul> <li> <p>+Add: Clicking on this will open a prompt letting you add a workload to your current project.</p> </li> <li> <p>Topology: Displays all of the deployed workloads in the currently selected project.</p> </li> <li> <p>Observe: Lets you view the monitoring dashboard for the current project.</p> </li> <li> <p>Search: Used to search for any type of API resource present in this project, provided you have access to that resource type.</p> </li> <li> <p>Builds: This will let you view or create Build Configurations in the currently selected project.</p> </li> <li> <p>Pipelines: View and create Pipelines \u2013 Tekton-based CI/CD processes and objects.</p> </li> <li> <p>Helm: Displays the Helm releases in this project, or prompts you to install one from the catalog if none are present.</p> </li> <li> <p>Project: Takes you to your project overview page, the project inventory, events, utilization, and more.</p> </li> <li> <p>Config Maps: Displays Config Maps for your project, which store non-confidential data in key-value pairs.</p> </li> <li> <p>Secrets: Displays Secrets for your project. Used to store sensitive, confidential data in key-value pairs, tokens, or passwords.</p> </li> </ul> </li> </ol> <p>Switching to the Developer perspective takes you to the Topology view. If no workloads are deployed in the selected project, options to start building an application or visit the +Add page or are displayed.</p> <p>If you ended up on a page other than Topology, continue with step 1 below anyways.</p> <ol> <li> <p>Click the +Add button in the menu.</p> <p></p> Expand to learn about Deployment Methods <p>There are multiple methods of deploying workloads from the OpenShift web browser.</p> <ul> <li>Samples: Red Hat provides sample applications in various languages. Use these to see what a pre-made application running in OpenShift can look like.</li> <li>From Git: Use this option to import an existing codebase in a Git repository to create, build, and deploy an application.</li> <li>From Devfile: Similar to From Git, use this option to import a Devfile from your Git repository to build and deploy an application.</li> <li>Container Image: Use existing images from an image stream or registry to deploy it.</li> <li>From Catalog: Explore the Developer Catalog to select the required applications, services, or source to image builders and add it to your project.</li> <li>From Dockerfile: Import a dockerfile from your Git repository to build and deploy an application.</li> <li>YAML: Use the editor to add YAML or JSON definitions to create and modify resources.</li> <li>Database: Filters the Developer Catalog to display only the databases it contains.</li> <li>Operator Backed: Deploy applications that are managed by Operators. Many of these will come from the OperatorHub.</li> <li>Helm Chart: Deploy applications defined by Helm Charts, which provide simple installations, upgrades, rollbacks, and generally reduced complexity.</li> <li>Pipeline: Create a Tekton-based Pipeline to automate application creation and delivery using OpenShift\u2019s built-in CI/CD capabilities.</li> </ul> <p>In the next section, you will deploy an application from the OpenShift Developer Catalog.</p> </li> </ol>"},{"location":"lab001/lab001-5/","title":"Deploy from the Developer Catalog","text":"<p>In this section, you will be building a sample application from a template. The template will create two pods:</p> <ul> <li> <p>A Node.js inventory management application from source code in GitHub</p> </li> <li> <p>A PostgreSQL database from a container image</p> </li> </ul> Info <p>A container image holds a set of software that is ready to run, while a container is a running instance of a container image. Images can be hosted in registries, such as the OpenShift internal registry, the Red Hat registry, Docker Hub, or a private registry of your own.</p> <ol> <li> <p>Click the All Services option in the Developer Catalog section on the +Add page.</p> <p>This brings up the OpenShift Developer catalog containing all types of applications you can deploy including Operators, Helm Charts, Templates, and more.</p> </li> <li> <p>Find and click the Node.js + PostgreSQL (Ephemeral) tile.</p> <p>Tip</p> <p>You can search for Node.js + PostgreSQL (Ephemeral) in the search bar.</p> <p></p> </li> <li> <p>Click Instantiate Template on the next screen that appears.</p> <p>You are brought to a page full of configurable parameters that you can edit if necessary. Notice that all of the required fields on this page automatically populate. You can read through all of the options, but there is no need to edit any of them for this lab.</p> </li> <li> <p>Click the Create button at the bottom of the page.</p> <p>You will now be taken to the topology view, where you will see two icons \u2013 one for each of the two workload pods that the template will create. If you don\u2019t see the icons right away, you may need to refresh your browser window.</p> Info <p>The Node.js application will take a minute or so to fully deploy, while the PostgreSQL application will deploy in just a few seconds. The reason for this difference is that the Node.js application is being built (containerized) from javascript source code located in the GitHub repository located here: https://github.com/nodeshift-starters/nodejs-rest-http-crud.git into a container image, and then deployed. If you would like to watch the steps that OpenShift is taking to build the containerized application, click the circle labeled nodejs-postgresql-example, click the Resources tab, and click View Logs in the Builds section. Alternatively, you can click on the small circular button to the bottom-left of the Node.js icon on the topology page.</p> <p>The PostgreSQL application, on the other hand, is deployed from a pre-built container image hosted in quay.io, so it takes much less time to start up.</p> <p>You will know that both applications are successfully deployed and running when each icon has a solid blue circle.</p> <p></p> </li> <li> <p>Click the icon for the nodejs-postgresql-example application. This will bring up a window on the right side of the screen with information about your DeploymentConfig.</p> </li> <li> <p>Click the Details tab if it is not already selected.</p> <p></p> <p>Here you\u2019ll see information about your DeploymentConfig. Notice that many of the fields such as Labels, Update Strategy, and more have been populated with default values. These can be modified.</p> </li> <li> <p>Click the Actions dropdown.</p> <p></p> <p>Many application configurations can be modified from this menu, along with other tasks such as starting or pausing a rollout, or deleting the deployment configuration.</p> </li> <li> <p>Click the up arrow next to the blue circle.</p> <p></p> <p>This scales your application from one pod to two pods.</p> <p></p> Note <p>This is a simple demonstration of horizontal scaling with Kubernetes. You now have two instances of your pod running in the OpenShift cluster. Traffic to the Node.js application will now be distributed to each pod, and if for some reason a pod is lost, that traffic will be redistributed to the remaining pods until a Kubernetes starts another. If a whole compute node is lost, Kubernetes will move the pods to different compute nodes.</p> <p>OpenShift and Kubernetes also support autoscaling of pods based on CPU or memory consumption, but that is outside the scope of this lab.  </p> </li> <li> <p>Click the Resources tab.</p> <p></p> <p>Notice the two pods associated with your Node.js application. On this page, you\u2019ll see more information about your pods, any build configurations currently running or completed, and the services/ports associated with the pod.</p> </li> <li> <p>Click the route address at the bottom of the resources tab.</p> Expand for a Tip <p>You could also access this route by clicking on the external link icon associated with your Node.js pod on the Topology view.</p> <p></p> <p></p> <p>If you see the page above, your Node.js application is up and running. You just deployed a Node.js application from source code residing in GitHub, and connected it to a PostgreSQL container deployed from a container image pulled from quay.io into OpenShift running on an IBM Z server.</p> </li> <li> <p>Add your favorite fruit and a quantity in the two fields and click Save.</p> <p>You are now interacting with the fruit stock management application that\u2019s shipped with the Node.js source code. When you edit the fruit inventory, the fruit name and quantity are stored in the PostgreSQL database in the other pod that makes up this application. Because we deployed the ephemeral Postgres pod, there is no persistent storage backing this data. Whenever the Postgres pod is regenerated or scales up or down, all changes will be lost. Making these changes persistent (in the case of a stateful application like a database) is the topic for another lab.</p> <p>In the next section you will navigate back to the Administrator perspective to see the overview of your project with a workload running.</p> </li> </ol>"},{"location":"lab001/lab001-6/","title":"View Workload from the Administrator Perspective","text":"<ol> <li> <p>In the left-side menu, select the Administrator perspective.</p> <p></p> </li> <li> <p>Navigate back to your project by clicking Menu -&gt; Home -&gt; Projects -&gt; userNN-project.</p> <p>The overview page now displays data about the CPU and Memory Usage, new objects in your project inventory, and new activity in the events panel.</p> <p></p> </li> <li> <p>Click View Events under the right-side panel.</p> <p></p> <p>This page is populated with all of the events associated with your project, including errors, container creation messages, pod scaling and deletion, and much more. You can filter by type, category, or by searching for keywords.</p> Note <p>Feel free to click through a few more pages from the left-side main menu. You\u2019ll notice a few of them have objects created as a part of the Node.js-PostgreSQL application, such as Workloads -&gt; Pods, Networking -&gt; Services and Routes, Builds -&gt; Image Streams. These were all created as part of the template package.</p> </li> </ol>"},{"location":"lab001/lab001-7/","title":"Cleaning Up","text":"<ol> <li> <p>Navigate back your project as you did in the previous section (or by clicking your browser\u2019s back button).</p> </li> <li> <p>Find the Inventory on the project page which lists all of the objects created as part of your application.</p> </li> <li> <p>Click the Deployment Configs hyperlink.</p> <p></p> </li> <li> <p>For both of the 2 DeploymentConfigs that appear click the three dots on the right side of the screen, and then click Delete Deployment Config.</p> <p></p> <p>This will delete some, but not all of the resources created by the application template. The running pods will be stopped and deleted, but some other components will remain. This is not a problem in the case of these labs, so don't worry if you see lingering objects in future labs.</p> </li> </ol>"},{"location":"lab002/lab002-1/","title":"The OpenShift Command Line (oc)","text":"<p>The OpenShift command line <code>oc</code> is a command line tool that can be used to create applications and manage OpenShift projects. <code>oc</code> is ideal in situations where you:</p> <ul> <li>Work directly with project source code.</li> <li>Script OpenShift Container Platform operations.</li> <li>Are restricted by bandwidth resources and cannot use the web console.</li> </ul> <p>Furthermore, many people familiar with Linux and/or Kubernetes tend to find the <code>oc</code> command line an easier and more efficient method of performing tasks, rather than the web-based console.</p> <p>Like with the OpenShift web console, the OpenShift command line includes functions both for developers and for administrators.</p>"},{"location":"lab002/lab002-2/","title":"Log into OpenShift Using the CLI","text":"<p>In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line <code>oc</code> installed, so you don\u2019t have to install it on your Windows VM terminal.</p> <ol> <li> <p>Open a Terminal session</p> </li> <li> <p>ssh into the Linux Guest server: <code>ssh userNN@192.168.176.61</code> (where NN is your user number).</p> </li> <li> <p>When prompted, enter your password: <code>p@ssw0rd</code> and hit enter.</p> Example Output <p></p> </li> <li> <p>In Firefox, navigate to the following URL to request an API token:</p> <p>https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request</p> </li> <li> <p>Enter your OpenShift credentials when prompted.</p> <ul> <li> <p>Username: userNN</p> </li> <li> <p>Password: p@ssw0rd</p> </li> </ul> </li> <li> <p>Click the \u201cDisplay Token\u201d hyperlink.</p> <p></p> </li> <li> <p>Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d.</p> <p></p> </li> <li> <p>Paste this command back in your terminal session and press enter.</p> <pre><code>oc login --token=&lt;YOUR_TOKEN_HERE&gt; --server=https://api.atsocppa.dmz:6443\n</code></pre> <p>Important</p> <p>If you\u2019re prompted to use an insecure connection, type Y and hit enter.</p> Example Output <pre><code>user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443\nLogged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided.\n\nYou have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"user01-project\".\n</code></pre> <p>You are now logged into the cluster via the command line, and you are told which project you are using.</p> <p>If you\u2019re in a project other than userNN-project, use the following command to move into it: <code>oc project userNN-project</code>, where NN is your user number.</p> </li> </ol>"},{"location":"lab002/lab002-3/","title":"Overview of the OpenShift CLI","text":"<ol> <li> <p>In your terminal, enter the command:</p> <pre><code>oc --help\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc --help\nOpenShift Client\n\nThis client helps you develop, build, deploy, and run your applications on any\nOpenShift or Kubernetes cluster. It also includes the administrative\ncommands for managing a cluster under the 'adm' subcommand.\n\nUsage:\noc [flags]\n\nBasic Commands:\nlogin           Log in to a server\nnew-project     Request a new project\nnew-app         Create a new application\nstatus          Show an overview of the current project\nproject         Switch to another project\nprojects        Display existing projects\nexplain         Documentation of resources\n\nBuild and Deploy Commands:\nrollout         Manage a Kubernetes deployment or OpenShift deployment config\nrollback        Revert part of an application back to a previous deployment\nnew-build       Create a new build configuration\nstart-build     Start a new build\n</code></pre> <p>The <code>--help</code> flag will display all of the available options the oc CLI.</p> </li> <li> <p>Enter the following command</p> <pre><code>oc new-app --help\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc new-app --help\nCreate a new application by specifying source code, templates, and/or images\n\nThis command will try to build up the components of an application using images, templates, or code\nthat has a public repository. It will lookup the images on the local Docker installation (if\navailable), a container image registry, an integrated image stream, or stored templates.\n\nIf you specify a source code URL, it will set up a build that takes your source code and converts\nit into an image that can run inside of a pod. Local source must be in a git repository that has a\nremote repository that the server can see. The images will be deployed via a deployment\nconfiguration, and a service will be connected to the first public port of the app. You may either\nspecify components using the various existing flags or let new-app autodetect what kind of\ncomponents you have provided.\n\nIf you provide source code, a new build will be automatically triggered. You can use 'oc status' to\ncheck the progress.\n\nUsage:\noc new-app (IMAGE | IMAGESTREAM | TEMPLATE | PATH | URL ...) [flags]\n\nExamples:\n# List all local templates and image streams that can be used to create an app\noc new-app --list\n\n# Create an application based on the source code in the current git repository (with a public\nremote) and a Docker image\noc new-app . --docker-image=repo/langimage\n</code></pre> <p>The <code>--help</code> flag now displays all of the available options for the oc new-app command. If you get confused about any of the commands we use in this workshop, or just want more information, using this flag is a good first step.</p> </li> </ol>"},{"location":"lab002/lab002-4/","title":"Deploy Container Image from the CLI","text":"<p><code>oc new-app</code> is a powerful and commonly used command in the OpenShift CLI. It has the ability to deploy applications from components that include:</p> <ul> <li>Source or binary code</li> <li>Container images</li> <li>Templates</li> </ul> <p>The set of objects created by <code>oc new-app</code> depends on the artifacts passed as an input.</p> <ol> <li> <p>Run the following command to start an HTTP server (httpd) deployment from a template:</p> <pre><code>oc new-app --template=httpd-example\n</code></pre> Example Output <p>```text user01@lab061:~$ oc new-app --template=httpd-example \u2192 Deploying template \"openshift/httpd-example\" to project user00-project</p> <pre><code>Apache HTTP Server\n---------\nAn example Apache HTTP Server (httpd) application that serves static content. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/httpd-ex/blob/master/README.md.\n\nThe following service(s) have been created in your project: httpd-example.\n\nFor more information about using this template, including OpenShift considerations, see https://github.com/sclorg/httpd-ex/blob/master/README.md.\n\n* With parameters:\n    * Name=httpd-example\n    * Namespace=openshift\n    * Memory Limit=512Mi\n    * Git Repository URL=https://github.com/sclorg/httpd-ex.git\n    * Git Reference=\n    * Context Directory=\n    * Application Hostname=\n    * GitHub Webhook Secret=RWOich7rQxeyDc5X4R8AWHivcTHdOhTjCUWDLFe1 # generated\n    * Generic Webhook Secret=4lAUcEAQJu2cCyq2MX0sQ0uIlmsYbAGf5XUM7unN # generated\n</code></pre> <p>\u2192 Creating resources ...     service \"httpd-example\" created     route.route.openshift.io \"httpd-example\" created     imagestream.image.openshift.io \"httpd-example\" created     buildconfig.build.openshift.io \"httpd-example\" created     deploymentconfig.apps.openshift.io \"httpd-example\" created \u2192 Success     Access your application via route 'httpd-example-user00-project.apps.atsocppa.dmz'     Build scheduled, use 'oc logs -f buildconfig/httpd-example' to track its progress.     Run 'oc status' to view your app.        ```</p> Note <p>Notice a few things:</p> <ul> <li> <p>OpenShift went out and found a template that matches your desired deployment \u2013 httpd-example.</p> </li> <li> <p>You\u2019re told what exactly is going to be created and what it will be named.</p> </li> <li> <p>Those objects are then created within your project space.</p> </li> <li> <p>You\u2019re told that the application was successfully deployed and also exposed via a route. This mean that it is accessible from outside the cluster.</p> </li> </ul> </li> <li> <p>Run the following command to view the app in your project space:</p> <pre><code>oc status\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc status\nIn project user00-project on server https://api.atsocppa.dmz:6443\n\nhttp://httpd-example-user00-project.apps.atsocppa.dmz (svc/httpd-example)\ndc/httpd-example deploys istag/httpd-example:latest &lt;-\n    bc/httpd-example source builds https://github.com/sclorg/httpd-ex.git on openshift/httpd:2.4-el8\n    deployment #1 deployed about a minute ago - 1 pod\n\nView details with 'oc describe &lt;resource&gt;/&lt;name&gt;' or list resources with 'oc get all'.\n</code></pre> <ol> <li>Now run the following command to see all of the objects that were built:</li> </ol> <pre><code>oc get all\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc get all\nNAME                         READY   STATUS      RESTARTS   AGE\npod/httpd-example-1-build    0/1     Completed   0          2m41s\npod/httpd-example-1-deploy   0/1     Completed   0          2m7s\npod/httpd-example-1-pgxxp    1/1     Running     0          2m3s\n\nNAME                                    DESIRED   CURRENT   READY   AGE\nreplicationcontroller/httpd-example-1   1         1         1       2m7s\n\nNAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/httpd-example   ClusterIP   172.30.232.138   &lt;none&gt;        8080/TCP   2m42s\n\nNAME                                               REVISION   DESIRED   CURRENT   TRIGGERED BY\ndeploymentconfig.apps.openshift.io/httpd-example   1          1         1         config,image(httpd-example:latest)\n\nNAME                                           TYPE     FROM   LATEST\nbuildconfig.build.openshift.io/httpd-example   Source   Git    1\n\nNAME                                       TYPE     FROM          STATUS     STARTED         DURATION\nbuild.build.openshift.io/httpd-example-1   Source   Git@744d904   Complete   2 minutes ago   38s\n\nNAME                                           IMAGE REPOSITORY                                                                        TAGS     UPDATED\nimagestream.image.openshift.io/httpd-example   default-route-openshift-image-registry.apps.atsocppa.dmz/user00-project/httpd-example   latest   2 minutes ago\n\nNAME                                     HOST/PORT                                        PATH   SERVICES        PORT    TERMINATION   WILDCARD\nroute.route.openshift.io/httpd-example   httpd-example-user00-project.apps.atsocppa.dmz          httpd-example   &lt;all&gt;                 None\n</code></pre> <p>These are the objects that OpenShift told us would be created, and they all work together to run the application. While they\u2019re all important pieces of this puzzle, pods are where the application code is actually running. Let\u2019s narrow down on our pods.</p> Note <p>You might also have objects left over from other labs if they were not completely cleaned out. This is okay and the objects for different applications will not interfere with one another due to their use of labels.</p> </li> <li> <p>Run the command:</p> <pre><code>oc get pods\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc get pods\nNAME             READY   STATUS      RESTARTS   AGE\nhttpd-example-1-build    0/1     Completed   0          3m12s\nhttpd-example-1-deploy   0/1     Completed   0          2m38s\nhttpd-example-1-r8dpw    1/1     Running     0          2m34s        \n</code></pre> <p>The <code>oc new-app</code> command created three pods. One ending with <code>build</code>, one ending with <code>deploy</code> and the other ending with a randomly-generated string of 5 characters (<code>r8dpw</code> in the screenshot above). They are all associated with your httpd deployment, but two are in a Completed status, and one is Running. The Completed pods had one simple job each \u2013 build the application pod, and deploy the application pod. The <code>Running</code> pod is the running httpd server, and that's what we're interested in.</p> </li> <li> <p>Run the following command to see the logs for the build pod:</p> <pre><code>oc logs pod/httpd-example-1-build\n</code></pre> Expand to see Example Output <pre><code>time=\"2022-09-07T15:06:46Z\" level=info msg=\"Not using native diff for overlay, this may cause degraded performance for building images: kernel has CONFIG_OVERLAY_FS_REDIRECT_DIR enabled\"\nI0907 15:06:46.532335       1 defaults.go:102] Defaulting to storage driver \"overlay\" with options [mountopt=metacopy=on].\nCaching blobs under \"/var/cache/blobs\".\nTrying to pull image-registry.openshift-image-registry.svc:5000/openshift/httpd@sha256:2b81d44b025f1ec399b0af0f4b9f1f315ecccd86db739d33e49a24138d831669...\nGetting image source signatures\nCopying blob sha256:f305bb7e80751f62007e416634d5fbdde5a09dfbeec8ead01c4a030521406fd1\nCopying blob sha256:8c8566bb888e9a3890647fe37bcd0794a98256331552368fb2e120c3a71769ba\nCopying blob sha256:b8a244553a3955b277a7f2af3459c46bc4504302e707c95233a0ce97e90fe066\nCopying blob sha256:5050a44fec5224351630e11142efe5b28eb680db5080aa01459d643b38af870f\nCopying config sha256:646dbe8bd260d5e2d2f6cb488f38fa4bb569efe09f25dd926e6dbe6e7c1c02e2\nWriting manifest to image destination\nStoring signatures\nGenerating dockerfile with builder image image-registry.openshift-image-registry.svc:5000/openshift/httpd@sha256:2b81d44b025f1ec399b0af0f4b9f1f315ecccd86db739d33e49a24138d831669\nAdding transient rw bind mount for /run/secrets/rhsm\nSTEP 1/9: FROM image-registry.openshift-image-registry.svc:5000/openshift/httpd@sha256:2b81d44b025f1ec399b0af0f4b9f1f315ecccd86db739d33e49a24138d831669\nSTEP 2/9: LABEL \"io.openshift.build.commit.date\"=\"Tue Jul 19 16:22:14 2022 +0200\"       \"io.openshift.build.commit.id\"=\"744d904524011619b3a8a16cb5e535fed6233998\"       \"io.openshift.build.commit.ref\"=\"master\"       \"io.openshift.build.commit.message\"=\"Merge pull request #36 from sclorg/issue_35\"       \"io.openshift.build.source-location\"=\"https://github.com/sclorg/httpd-ex.git\"       \"io.openshift.build.image\"=\"image-registry.openshift-image-registry.svc:5000/openshift/httpd@sha256:2b81d44b025f1ec399b0af0f4b9f1f315ecccd86db739d33e49a24138d831669\"       \"io.openshift.build.commit.author\"=\"Petr Hracek &lt;phracek@redhat.com&gt;\"\nSTEP 3/9: ENV OPENSHIFT_BUILD_NAME=\"httpd-example-1\"     OPENSHIFT_BUILD_NAMESPACE=\"user00-project\"     OPENSHIFT_BUILD_SOURCE=\"https://github.com/sclorg/httpd-ex.git\"     OPENSHIFT_BUILD_COMMIT=\"744d904524011619b3a8a16cb5e535fed6233998\"\nSTEP 4/9: USER root\nSTEP 5/9: COPY upload/src /tmp/src\nSTEP 6/9: RUN chown -R 1001:0 /tmp/src\nSTEP 7/9: USER 1001\nSTEP 8/9: RUN /usr/libexec/s2i/assemble\n---&gt; Enabling s2i support in httpd24 image\n    AllowOverride All\n---&gt; Installing application source\n=&gt; sourcing 20-copy-config.sh ...\n=&gt; sourcing 40-ssl-certs.sh ...\n---&gt; Generating SSL key pair for httpd...\nSTEP 9/9: CMD /usr/libexec/s2i/run\nCOMMIT temp.builder.openshift.io/user00-project/httpd-example-1:2fda11d0\ntime=\"2022-09-07T15:07:04Z\" level=warning msg=\"Adding metacopy option, configured globally\"\nGetting image source signatures\nCopying blob sha256:92f1e97901b556ffa9ce24be4eface85f849909605cfa3935b03a713480bb8b1\nCopying blob sha256:353a53d37adccad35f2b4aeacca9d81fe75bb082bbc75f2162e31e1f13fd75f3\nCopying blob sha256:d00bac8c7787cb4f8283a257cd267a82e9463330c55123ab1f0f104f109ab93a\nCopying blob sha256:a5ccc3f34f400bdb85cb724ad36a8ace6aaa941b300a2264565f874d494f844e\nCopying blob sha256:d0d25de2e87a45b4e828f6d245dc8f319bcc55755bca88a75ebaba8fa9118c74\nCopying config sha256:b7384dd55cc6c0879aa13de7c5bf4dbd118ea84eb5946ad5849f9874b29cbe3d\nWriting manifest to image destination\nStoring signatures\n--&gt; b7384dd55cc\nSuccessfully tagged temp.builder.openshift.io/user00-project/httpd-example-1:2fda11d0\nb7384dd55cc6c0879aa13de7c5bf4dbd118ea84eb5946ad5849f9874b29cbe3d\n\nPushing image image-registry.openshift-image-registry.svc:5000/user00-project/httpd-example:latest ...\nGetting image source signatures\nCopying blob sha256:d0d25de2e87a45b4e828f6d245dc8f319bcc55755bca88a75ebaba8fa9118c74\nCopying blob sha256:b8a244553a3955b277a7f2af3459c46bc4504302e707c95233a0ce97e90fe066\nCopying blob sha256:8c8566bb888e9a3890647fe37bcd0794a98256331552368fb2e120c3a71769ba\nCopying blob sha256:5050a44fec5224351630e11142efe5b28eb680db5080aa01459d643b38af870f\nCopying blob sha256:f305bb7e80751f62007e416634d5fbdde5a09dfbeec8ead01c4a030521406fd1\nCopying config sha256:b7384dd55cc6c0879aa13de7c5bf4dbd118ea84eb5946ad5849f9874b29cbe3d\nWriting manifest to image destination\nStoring signatures\nSuccessfully pushed image-registry.openshift-image-registry.svc:5000/user00-project/httpd-example@sha256:bd19230bffb03932d7fcf2ead029d18347bbc13aecea376bf718a3c53bd7d364\nPush successful\n</code></pre> <p>The build pod log lets you see all of the steps that were taken to containerize the httpd source code. If you are familiar with containerization, you will notice that each step is the output from a line in a Dockerfile/Containerfile. You could have built this image yourself and then deployed it to OpenShift, but why not let OpenShift do the heavy working for us?</p> </li> <li> <p>Run the following command to see the logs for the httpd server pod:</p> <pre><code>oc logs pod/httpd-example-1-XXXXX\n</code></pre> <p>Where <code>XXXXX</code> is your unique string of characters that you saw in the <code>oc get pods</code> output.</p> Example Output <pre><code>=&gt; sourcing 10-set-mpm.sh ...\n=&gt; sourcing 20-copy-config.sh ...\n=&gt; sourcing 40-ssl-certs.sh ...\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.128.2.72. Set the 'ServerName' directive globally to suppress this message\n[Wed Sep 07 15:07:23.012881 2022] [ssl:warn] [pid 1:tid 4396165818656] AH01909: 10.128.2.72:8443:0 server certificate does NOT include an ID which matches the server name\n[Wed Sep 07 15:07:23.013141 2022] [:notice] [pid 1:tid 4396165818656] ModSecurity for Apache/2.9.2 (http://www.modsecurity.org/) configured.\n[Wed Sep 07 15:07:23.013147 2022] [:notice] [pid 1:tid 4396165818656] ModSecurity: APR compiled version=\"1.6.3\"; loaded version=\"1.6.3\"\n[Wed Sep 07 15:07:23.013150 2022] [:notice] [pid 1:tid 4396165818656] ModSecurity: PCRE compiled version=\"8.42 \"; loaded version=\"8.42 2018-03-20\"\n[Wed Sep 07 15:07:23.013156 2022] [:notice] [pid 1:tid 4396165818656] ModSecurity: LUA compiled version=\"Lua 5.3\"\n[Wed Sep 07 15:07:23.013160 2022] [:notice] [pid 1:tid 4396165818656] ModSecurity: YAJL compiled version=\"2.1.0\"\n[Wed Sep 07 15:07:23.013163 2022] [:notice] [pid 1:tid 4396165818656] ModSecurity: LIBXML compiled version=\"2.9.7\"\n[Wed Sep 07 15:07:23.013172 2022] [:notice] [pid 1:tid 4396165818656] ModSecurity: Status engine is currently disabled, enable it by set SecStatusEngine to On.\n[Wed Sep 07 15:07:23.118901 2022] [ssl:warn] [pid 1:tid 4396165818656] AH01909: 10.128.2.72:8443:0 server certificate does NOT include an ID which matches the server name\n[Wed Sep 07 15:07:23.119089 2022] [lbmethod_heartbeat:notice] [pid 1:tid 4396165818656] AH02282: No slotmem from mod_heartmonitor\n[Wed Sep 07 15:07:23.124907 2022] [mpm_event:notice] [pid 1:tid 4396165818656] AH00489: Apache/2.4.37 (Red Hat Enterprise Linux) OpenSSL/1.1.1g configured -- resuming normal operations\n[Wed Sep 07 15:07:23.124925 2022] [core:notice] [pid 1:tid 4396165818656] AH00094: Command line: 'httpd -D FOREGROUND'\n10.128.2.1 - - [07/Sep/2022:15:07:25 +0000] \"GET / HTTP/1.1\" 200 37451 \"-\" \"kube-probe/1.23\"\n10.128.2.1 - - [07/Sep/2022:15:07:34 +0000] \"GET / HTTP/1.1\" 200 37451 \"-\" \"kube-probe/1.23\"\n</code></pre> </li> </ol> <p>In the next section, you will navigate to the exposed route to see the application that you just deployed.</p>"},{"location":"lab002/lab002-5/","title":"Access the httpd Server in a Browser","text":""},{"location":"lab002/lab002-5/#background","title":"Background","text":"<p>As you saw in the previous section, OpenShift deployed other Kubernetes objects along with pods when you ran the <code>oc new-app</code> command. Two of these objects - services and routes - provide access into the running pods from both inside and outside of the cluster. </p> <p>Because pods are designed to rapidly start, stop, move, and scale, and the fact that pods each have their own IP address, you can't rely on IP addresses to access running applications like you typically can for non-containerized workloads. Therefore we need another way to reliably access the application pods, and this is what services and routes provide.</p> <p>Services are essentially load balancers internal to the OpenShift cluster. Whenever a request is made to a service, it is then directed to a pod based on label matching. Unlike pod IP addresses, services are static and do not change over time. However, services are only internal to the cluster and do not provide access from external users. </p> <p>Routes, on the other hand, expose services to the outside world. Routes provide ingress to the cluster and are they way that end users applications running in pods.</p>"},{"location":"lab002/lab002-5/#accessing-the-httpd-server","title":"Accessing the httpd Server","text":"<ol> <li> <p>In the OpenShift CLI, find the route in your <code>userNN-project</code>. </p> <pre><code>oc get route\n</code></pre> Example Output <pre><code>NAME            HOST/PORT                                        PATH   SERVICES        PORT    TERMINATION   WILDCARD\nhttpd-example   httpd-example-user00-project.apps.atsocppa.dmz          httpd-example   &lt;all&gt;                 None\n</code></pre> <p>From this output, you see that you have a route named <code>httpd-example</code>, it has an automatically generated hostname (that can be changed if you wish), and you see that it references a service named <code>httpd-example</code>. The port for this service/route is not shown because it is the default for HTTP web services, <code>8080</code>. </p> </li> <li> <p>Navigate to this route in a web browser, but make sure you add <code>http://</code> to the beginning.</p> <p>For example, userNN's URL would be: http://httpd-example-userNN-project.apps.atsocppa.dmz.</p> <p></p> <p>Your request to the <code>httpd-example</code> route was passed to the <code>httpd-example</code> service which uses labels to point to the <code>httpd-example-1-XXXXX</code> pod. If this pod is recreated with a new IP address, you as the end user will not notice any difference or need to connect through any other method, the route will still work. Similarly, if you scale the application so there are multiple pods with different IP addresses (as you will in the next section), the OpenShift load balancer will take the requests and direct them to specific pods based on your desired load-balancing algorithm (e.g. <code>random</code>, <code>source</code>, <code>roundrobin</code>, <code>leastconn</code>).</p> </li> </ol>"},{"location":"lab002/lab002-6/","title":"Working with Pods","text":"<p>One of the main benefits of using containers and Kubernetes-based cloud platforms like OpenShift is the ability to scale horizontally \u2013 rapidly duplicating or deleting pods to meet a desired state.</p> Information <p>One of the core concepts of Kubernetes is the Declarative State. Users declare what resources they want, and Kubernetes does whatever it can to make that happen. Scaling is one example of this.</p> <p>Scaling essentially creates copies of the application in order to distribute traffic to multiple instances and/or compute nodes for high availability and load balancing.</p> <ol> <li> <p>Enter the following command to get the name of your httpd deploymentconfig (dc)</p> <pre><code>oc get dc\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc get dc\nNAME            REVISION   DESIRED   CURRENT   TRIGGERED BY\nhttpd-example   1          1         1         config,image(httpd-example:latest)\n</code></pre> <p>Your deploymentconfig named <code>httpd-example</code> has a count desired = current = 1.</p> </li> <li> <p>Scale the httpd deployment to 3 replicas:</p> <pre><code>oc scale dc/httpd-example --replicas=3\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc scale dc/httpd-example --replicas=3\ndeploymentconfig.apps.openshift.io/httpd-example scaled\n</code></pre> </li> <li> <p>Enter the following command again to see the scaled application.</p> <pre><code>oc get dc\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc get dc\nNAME    REVISION   DESIRED   CURRENT   TRIGGERED BY\nhttpd-example   1          3         3         config,image(httpd-example:latest)\n</code></pre> <p>This output is telling you that OpenShift knows that you want three copies (pods) of httpd, and it is successfully meeting that declared state.</p> </li> <li> <p>Enter the following command again to see your three pods:</p> <pre><code>oc get pods\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc get pods\nNAME             READY   STATUS      RESTARTS   AGE\nhttpd-example-1-92k2t    1/1     Running     0          29s\nhttpd-example-1-build    0/1     Completed   0          56m\nhttpd-example-1-deploy   0/1     Completed   0          56m\nhttpd-example-1-pgxxp    1/1     Running     0          56m\nhttpd-example-1-q77kh    1/1     Running     0          29s\n</code></pre> <p>Two of the pods will have a shorter age than the original one \u2013 these are the two new pods that were just created when you scaled the application.</p> </li> <li> <p>Dig into the pods a little bit further by entering the following command:</p> <pre><code>oc describe pod/httpd-example-1-XXXXX\n</code></pre> <p>Where <code>XXXXX</code> is one of your unique strings of characters.</p> Example Output <pre><code>Name:         httpd-example-1-92k2t\nNamespace:    user00-project\nPriority:     0\nNode:         worker-1.atsocppa.dmz/192.168.176.141\nStart Time:   Wed, 07 Sep 2022 12:02:52 -0400\nLabels:       deployment=httpd-example-1\n              deploymentconfig=httpd-example\n              name=httpd-example\n</code></pre> <p>This command gives you all kinds of information about your pod. Notice the <code>Node:</code> field that begins with <code>worker-#</code>.</p> </li> <li> <p>Run the same command again, but on a different pod this time:</p> <pre><code>oc describe pod/httpd-example-1-YYYYY\n</code></pre> <p>Where <code>YYYYY</code> is one of your other unique strings of characters. Pick one different than the previous step.</p> Example Output <pre><code>Name:         httpd-example-1-pgxxp\nNamespace:    user00-project\nPriority:     0\nNode:         worker-2.atsocppa.dmz/192.168.176.142\nStart Time:   Wed, 07 Sep 2022 11:07:14 -0400\nLabels:       deployment=httpd-example-1\n            deploymentconfig=httpd-example\n            name=httpd-example\n</code></pre> <p>It is likely (but not guaranteed) that this pod has been placed on a different compute node than the first pod you described. The reason for this is that you have three compute nodes in this OpenShift cluster, and Kubernetes balances the load for this application across multiple nodes.</p> <p>Alternatively, you can achieve a similar result by adding adding different flags onto an <code>oc get</code> command.</p> </li> <li> <p>Run the following command to get more information about the three pods:</p> <pre><code>oc get pods -o wide\n</code></pre> <pre><code>user01@lab061:~$ oc get pods -o wide\nNAME                     READY   STATUS      RESTARTS   AGE     IP             NODE                    NOMINATED NODE   READINESS GATES\nhttpd-example-1-92k2t    1/1     Running     0          2m37s   10.129.3.32    worker-1.atsocppa.dmz   &lt;none&gt;           &lt;none&gt;\nhttpd-example-1-build    0/1     Completed   0          58m     10.128.2.70    worker-2.atsocppa.dmz   &lt;none&gt;           &lt;none&gt;\nhttpd-example-1-deploy   0/1     Completed   0          58m     10.128.2.71    worker-2.atsocppa.dmz   &lt;none&gt;           &lt;none&gt;\nhttpd-example-1-pgxxp    1/1     Running     0          58m     10.128.2.72    worker-2.atsocppa.dmz   &lt;none&gt;           &lt;none&gt;\nhttpd-example-1-q77kh    1/1     Running     0          2m37s   10.131.1.102   worker-0.atsocppa.dmz   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>The <code>-o wide</code> flag reveals more columns that are typically hidden from the <code>oc get pods</code> command, including which node the pods are running on.</p> </li> </ol> <p>You could also use the <code>-o</code> flag to pull specific data out of the pod details with YAML, JSON, custom columns, or go-template filters, such as:</p> <pre><code>user01@lab061:~$ oc get pod httpd-example-1-92k2t -o go-template='{{.spec.nodeName}}{{\"\\n\"}}'\nworker-1.atsocppa.dmz\n</code></pre> <p>Or: </p> <pre><code>user01@lab061:~$ oc get pod httpd-example-1-92k2t -o jsonpath='{.spec.nodeName}'\nworker-1.atsocppa.dmz\n</code></pre> <p>Or:</p> <pre><code>user01@lab061:~$ oc get pod httpd-example-1-92k2t -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName\nNAME              NODE\nhttpd-example-1-92k2t   worker-1.atsocppa.dmz\n</code></pre> <p>Or: </p> <pre><code>user01@lab061:~$ oc get pod httpd-example-1-92k2t -o yaml | grep nodeName\n  nodeName: worker-1.atsocppa.dmz\n</code></pre> <p>As you can see from the examples above, the <code>oc</code> command line is extremely flexible and able to provide the required information in a myriad of ways. This applies not only to <code>oc get</code> commands, but also commands used to deploy or modify applications and pieces of the OpenShift infrastructure itself. This means that OpenShift can easily be integrated into different development and automation pipelines whether that is by developers or administrators.</p>"},{"location":"lab002/lab002-7/","title":"Administrative CLI Commands","text":"<p>If you\u2019ve already completed Exploring the OpenShift Console, you\u2019ll remember that there are both developer and administrator perspectives. The same is true in the OpenShift CLI. </p> <p>The <code>oc adm</code> command gives cluster administrators the ability to check logs, manage users, groups, policies, certificates, and many other tasks usually associated with administrative roles.</p> <ol> <li> <p>Issue the following command to see all of the OpenShift administrator commands.</p> <pre><code>oc adm --help\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc adm --help\nAdministrative Commands\n\nActions for administering an OpenShift cluster are exposed here.\n\nUsage:\noc adm [flags]\n\nCluster Management:\nupgrade                            Upgrade a cluster\ntop                                Show usage statistics of resources on the server\nmust-gather                        Launch a new instance of a pod for gathering debug information\n\nNode Management:\ndrain                              Drain node in preparation for maintenance\ncordon                             Mark node as unschedulable\nuncordon                           Mark node as schedulable\ntaint                              Update the taints on one or more nodes\nnode-logs                          Display and filter node logs\n\nSecurity and Policy:\nnew-project                        Create a new project\npolicy                             Manage cluster authorization and security policy\ngroups                             Manage groups\ncertificate                        Approve or reject certificate requests\npod-network                        Manage pod network\n\nMaintenance:\nprune                              Remove older versions of resources from the server\nmigrate                            Migrate data in the cluster\n\nConfiguration:\ncreate-kubeconfig                  Create a basic .kubeconfig file from client certs\ncreate-bootstrap-project-template  Create a bootstrap project template\ncreate-login-template              Create a login template\ncreate-provider-selection-template Create a provider selection template\ncreate-error-template              Create an error page template\n\nOther Commands:\nbuild-chain                        Output the inputs and dependencies of your builds\ncompletion                         Output shell completion code for the specified shell (text or zsh)\nconfig                             Change configuration files for the client\nverify-image-signature             Verify the image identity contained in the image signature\n</code></pre> Note <p>Your userNN credential has the privileges required to run some, but not all of these commands.</p> </li> <li> <p>Run the following administrative command to show see usage statistics for pods in your project.</p> <pre><code>oc adm top pods\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc adm top pods\nNAME            CPU(cores)   MEMORY(bytes)   \nhttpd-example-1-92k2t   0m           32Mi\nhttpd-example-1-pgxxp   1m           37Mi\nhttpd-example-1-q77kh   2m           33Mi\n</code></pre> <p>As OpenShift clusters grow in production, administrative commands like this one become more and more essential to keep everything running smoothly.</p> </li> </ol>"},{"location":"lab002/lab002-8/","title":"Cleaning Up","text":"<ol> <li> <p>Double check that you are in your own userNN-project by issuing the command:</p> <pre><code>oc project\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc project\nUsing project \"user01-project\" on server \"https://api.atsocppa.dmz:6443\".\n</code></pre> </li> <li> <p>Once you\u2019re sure you\u2019re in your own project, issue the following command to delete all objects associated with your application labeled httpd-example.  </p> <pre><code>oc delete all --selector app=httpd-example -o name\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc delete all --selector app=httpd-example -o name\nreplicationcontroller/httpd-example-1\nservice/httpd-example\ndeploymentconfig.apps.openshift.io/httpd-example\nbuildconfig.build.openshift.io/httpd-example\nbuild.build.openshift.io/httpd-example-1\nimagestream.image.openshift.io/httpd-example\nroute.route.openshift.io/httpd-example\\\n</code></pre> </li> <li> <p>Run the following command to check that all of your httpd application resources were deleted:</p> <pre><code>oc get all\n</code></pre> Example Output <pre><code>user01@lab061:~$ oc get all\nNo resources found.\nuser00@lab061:~$\n</code></pre> </li> <li> <p>(Optional) If there are leftover resources from other labs that you would like to delete, run the command:</p> <pre><code>oc delete all --all\n</code></pre> </li> </ol>"},{"location":"lab003/lab003-1/","title":"The z/OS Cloud Broker","text":"<p>The IBM z/OS Cloud Broker is an IBM offering that connects z/OS services running on an IBM Z backend to a frontend container platform providing self-service access and consumption of these services to developers. This allows developers to provision their own z/OS resources directly from the OpenShift console \u2013 without the need for z/OS skills or direct access.</p> <p></p> <p>The services available for the z/OS Cloud Broker to expose into OpenShift are:</p> z/OS Connect EE Db2 CICS IMS MQ WLP Provision / deprovision z/OS Connect   Servers.     Start/Stop z/OS Connect Servers Provision / deprovision Db2   subsystems, schemas, and databases +     snapshot / restore Provision / deprovision CICS regions Provision / deprovision IMS TM/DB   systems Provision / deprovision MQ Queue   Manager subsystem WebSphere Liberty Profile server provisioning, start/stop   server <p>In this lab, you will be provisioning a WebSphere Liberty Profile (WLP) server on z/OS using the z/OS Cloud Broker on OpenShift. </p>"},{"location":"lab003/lab003-2/","title":"Connect to OpenShift and Authenticate","text":"<ol> <li> <p>In your virtual machine desktop, open a Firefox web browser.</p> </li> <li> <p>In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/.</p> Note <p>You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized.</p> <p>Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe).</p> <p>You will likely need to do this twice due to how OpenShift reroutes Oauth requests. </p> Expand for screenshot <p> </p> <p>You will now see the OpenShift console login page.</p> <p></p> </li> <li> <p>Log in with the OpenShift credentials provided to you on the Lab Assignments page.</p> <p>Hint</p> <p>Your OpenShift credentials will be something like the following:</p> <ul> <li> <p>Username: userNN (where NN is your user number)</p> </li> <li> <p>Password: p@ssw0rd</p> </li> </ul> </li> </ol>"},{"location":"lab003/lab003-3/","title":"Deploy Liberty for z/OS Using the z/OS Cloud Broker","text":"<p>With the z/OS Cloud Broker and OpenShift, provisioning z/OS resources is as easy as clicking on a tile in the OpenShift Developer Catalog.</p> <ol> <li> <p>Enter the Developer Perspective, if you aren\u2019t there already.</p> </li> <li> <p>Make sure that you\u2019re working under the z/OS Cloud Broker project <code>atg-zoscb</code>.</p> <p> Important </p> <p>Unlike other labs, this lab uses a shared project for all lab attendees. Please pay close attention to naming conventions so you do not end up deleting other attendees\u2019 provisioned services.</p> <p></p> </li> <li> <p>Click the +Add button in the left-side menu.</p> </li> <li> <p>Click the All Services option under the Developer Catalog section.</p> <p></p> </li> <li> <p>Search the catalog for Liberty for z/OS and click on it.</p> </li> <li> <p>Click the Create button at the bottom of the page.</p> <p>Information</p> <p>This Liberty service does not live inside this OpenShift cluster. It is, in fact, a template for a z/OS Liberty instance that z/OSMF has found and displayed. When you provision an instance, it will spin up the service in a completely different z/OS LPAR separate from the Linux LPAR where this OpenShift cluster is running.</p> </li> <li> <p>All of the required fields will automatically populate for you, but rename the wlp service to <code>userNN-wlp</code> where <code>NN</code> is your user number.</p> <p> Important </p> <p>Please double check that you have correctly typed your user number for userNN. Remember that you are using a shared project for this lab, and nothing is stopping you from interfering with another lab participant's provisioned service if you use the wrong name.</p> <p></p> </li> <li> <p>Click the create button.</p> <p>You will be brought to the topology page. After you click create, you will need to navigate to the service instance page:</p> </li> <li> <p>Switch to the Administrator Perspective -&gt; Operators in the menu bar -&gt; Installed Operators -&gt; Liberty for z/OS -&gt; Liberty for z/OS tab -&gt; Click on the instance with your user number NN.</p> <p>You will end up on a screen that looks like the following:</p> <p></p> <p>Notice two things:</p> <ul> <li> <p>Depending on how quickly you navigated to this page and how long the WLP instance takes to provision, your status will be either Pending or Succeeded.</p> </li> <li> <p>Once it\u2019s Succeeded, you will have a link to your Dashboard.</p> </li> </ul> <p>OpenShift is telling you that the service is either provisioned in z/OS, or in the process of being provisioned. While you don\u2019t have access to z/OSMF, the following is what you would see over in the z/OSMF console:</p> <p></p> <p>If you want to look at the z/OSMF console, ask an instructor and they will give you a tour.</p> <p>This service will take a minute or two to provision. Wait until you see the following messages on the service instance page:</p> <p></p> <p></p> <p>Over on z/OSMF page again, this is what one would see:</p> <p></p> <p>And in z/OS itself, the following task is started:</p> <p></p> <p>You have just successfully provisioned a Liberty instance on z/OS, without leaving the OpenShift console.</p> </li> <li> <p>From the OpenShift WLP instance page, click the Dashboard URL hyperlink.</p> </li> <li> <p>Click the Log in with OpenShift button.</p> </li> <li> <p>You might get a security challenge here. If you do, make sure that both of the two checkboxes are checked, and click Allow Selected Permissions.</p> <p>You will be taken to the dashboard for your z/OS Liberty instance. This page will be referred to as the Dashboard tab.</p> <p></p> <p>The right side of the page contains information about your WLP service and the z/OS system it\u2019s running on.</p> <p>The left side of the page contains buttons you can use to perform various actions. You will use a few of them shortly.</p> </li> <li> <p>Scroll to the bottom of the right-hand column, and locate the <code>IP_ADDRESS</code> variable.</p> Hint <p>It's <code>192.168.176.154</code>. That\u2019s the IP address of the z/OS system on which z/OSMF and Liberty for z/OS are hosted.</p> </li> <li> <p>Scroll up a bit and locate the <code>HTTP_PORT</code> variable. It\u2019s just about in the middle of the column.</p> Hint <p>It's something like <code>9XXX</code>, where <code>XXX</code> will be unique for each user.</p> </li> <li> <p>Keeping the Dashboard tab open, open a new browser tab.</p> </li> <li> <p>In the new tab, navigate to <code>&lt;IP_ADDRESS&gt;:&lt;HTTP_PORT&gt;</code></p> Hint <p>It will look something like <code>192.168.176.154:9XXX</code>, where the <code>XXX</code> is unique for each user.</p> <p>You should see the default Liberty homepage. This is the Liberty service you just provisioned on z/OS.</p> <p></p> </li> <li> <p>Staying in this \u201cWelcome to Liberty\u201d tab, add the following string to the end of the URL: <code>/CloudTestServlet</code></p> </li> <li> <p>Press enter.</p> <p>That will take you to a sample application that was deployed into the Liberty z/OS instance you provisioned.  You will see something like this:</p> <p></p> <p>Note the date and timestamp. It should be the current time (in U.S. Eastern time format).</p> </li> <li> <p>Reload the browser tab.  You should see the time-stamp change.</p> <p>Do not close this tab.</p> </li> <li> <p>Return to the Dashboard tab, which had all the information about the provisioned instance in it.</p> </li> </ol>"},{"location":"lab003/lab003-4/","title":"Stop and Restart Liberty for z/OS from OCP","text":"<p>In the Dashboard tab, you should see the following on the left side of the screen:</p> <p></p> <ol> <li> <p>Click the \"Run\" button that's associated with \"Stop\".</p> </li> <li> <p>Click the \u201cAction History\u201d button above.</p> <p>Depending how quickly you click on this button, you\u2019ll see either:</p> <p></p> <p>If the stop is in progress, or:</p> <p></p> <p>if the stop completed before you looked at the Action History.</p> <p>Over in z/OSMF, one would see:</p> <p></p> </li> <li> <p>Once you see in the Action History that the stop has completed, go back to the tab where the timestamp application was (<code>192.168.176.154:9XXX</code>, if you accidentally closed it).</p> </li> <li> <p>Reload this page.</p> <p>You will see the following:</p> <p></p> </li> <li> <p>Go back to the Dashboard tab and click the \u201cRun\u201d button that\u2019s associated with \u201cStart\u201d.</p> <p></p> <p>This will trigger a workflow over in z/OSMF to start the server.</p> </li> <li> <p>Click \u201cAction History\u201d and refresh until you see that the Start workflow is complete.</p> <p></p> </li> <li> <p>Go back to the tab with the timestamp application and reload the page.</p> <p>You should see the time and date with the current time shown:</p> <p></p> <p>This indicates that the server is back up and serving pages</p> </li> </ol>"},{"location":"lab003/lab003-5/","title":"Cleaning Up","text":"<ol> <li> <p>Close the tab with the timestamp application.</p> </li> <li> <p>Close the Dashboard tab.</p> </li> <li> <p>Navigate back to your userNN-wlp instance</p> Hint <p>Administrator -&gt; Operators -&gt; Installed Operators -&gt; Liberty for z/OS -&gt; Liberty for z/OS tab</p> </li> <li> <p>Click the three dots to the far right of your provisioned service and click Delete WLP.</p> <p></p> <p>Over in z/OSMF, that will trigger a de-provision operation:</p> <p></p> </li> </ol> <p>When the operation is complete, you will see</p> <p></p> <p>On z/OSMF, the Liberty z/OS server instance has been de-provisioned, which means it was stopped and the file system location for the server instance removed.</p>"},{"location":"lab004/lab004-1/","title":"Deploying an Application from Source Code","text":"<p>OpenShift is designed for users with various responsibilities, backgrounds and skillsets. Most broadly, OpenShift is designed for two main groups \u2013 administrators and developers. Furthermore, there are different types of administrators, and different types of developers.</p> <p>As much of the Information Technology world moves toward cloud technology as the consumption model for enterprise computing, developers are required to make a shift in the tools they use to perform their work. At the heart of almost every cloud platform there are two of these new, core technologies \u2013 containers and container orchestrators.</p> Note <p>We won\u2019t be specifically covering these technologies in this lab, but you\u2019ve probably heard of them. Docker is the most popular container runtime, and Kubernetes is the most popular container orchestrator. For the curious, OpenShift replaced Docker containers with CRI-O containers when moving from v3.11 to v4.1 (although Docker containers will still work in OpenShift 4.x).</p> <p>However, not every developer wants (or needs) to learn these new technologies in order to take advantage of them. In fact, OpenShift enables developers with no container experience at all to simply provide their source code (written in Javascript, Python, Go, etc.) and let OpenShift build the container for them using its Source-to-Image (S2I) capability.</p> <p></p> <p>OpenShift's S2I capability allows developers to focus on developing their application and leaves the containerization process to OpenShift. Using the S2I tooling and Builder Images loaded into the OpenShift image registry, the developer does not need to create a Dockerfile, use any podman or docker commands, or do anything else that is usually required to make a container image out of application source code.</p>"},{"location":"lab004/lab004-2/","title":"Exploring GitHub and the Example Health Source Code","text":"<p>In this lab, you will be deploying a sample healthcare application called Example Health. The application is written in JavaScript, and it\u2019s loaded into IBM\u2019s GitHub repository.</p> <ol> <li> <p>In Firefox, navigate to https://github.com/IBM/node-s2i-openshift</p> <p></p> <p>This is an IBM repository that contains everything you need in order to deploy the application \u2013 including a <code>README.md</code> file with information and instructions, additional files required for the source code to work, and the source code itself. Let\u2019s look at the source code now.</p> </li> <li> <p>Open the site folder.</p> <p></p> </li> <li> <p>Open the app.js file.</p> <p></p> <p>This is the source code for the frontend application that OpenShift will build into a container. Notice that it is NOT any sort of container image, Dockerfile, or YAML file itself \u2013 rather, it is written in Javascript. Feel free to look through the code.</p> </li> <li> <p>Click on the node-s2i-openshift hyperlink to get back to the main repository page.</p> <p>Your URL should again be https://github.com/IBM/node-s2i-openshift</p> <p>You\u2019ll need to make a fork of this repository so you have your own copy to work with. To do so, you\u2019ll first need to sign into GitHub.</p> </li> <li> <p>Click the Sign In button in the top right.</p> </li> <li> <p>Log in with YOUR OWN GitHub credentials.</p> Note <p>If you don\u2019t have a GitHub account already, please create one and then sign in with it.</p> <p>After a successful login, you will be taken back to the main repository page. Now you can create your own fork of the repository.</p> </li> <li> <p>Click the Fork button on the left side of the page.</p> <p></p> <p>When complete, you will be taken to your forked repository page</p> <p>Notice that while everything else looks basically  the same, the URL has changed from https://github.com/IBM/node-s2i-openshift</p> <p>to:</p> <p>https://github.com/YOUR_GITHUB_USERNAME/node-s2i-openshift</p> </li> <li> <p>Leave this browser tab open, and open another.</p> </li> </ol>"},{"location":"lab004/lab004-3/","title":"Connect to OCP and Authenticate","text":"<ol> <li> <p>In your virtual machine desktop, open a Firefox web browser.</p> </li> <li> <p>In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/.</p> Note <p>You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized.</p> <p>Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe).</p> <p>You will likely need to do this twice due to how OpenShift reroutes Oauth requests. </p> Expand for screenshot <p> </p> <p>You will now see the OpenShift console login page.</p> <p></p> </li> <li> <p>Log in with the OpenShift credentials provided to you on the Lab Assignments page.</p> <p>Hint</p> <p>Your OpenShift credentials will be something like the following:</p> <ul> <li> <p>Username: userNN (where NN is your user number)</p> </li> <li> <p>Password: p@ssw0rd</p> </li> </ul> </li> </ol>"},{"location":"lab004/lab004-4/","title":"Edit the Source Code and Push an Update","text":"<ol> <li> <p>Switch to the Developer Perspective, if not already on it.</p> <p></p> </li> <li> <p>Change into your userNN-project if not already in it.</p> <p></p> </li> <li> <p>Click the +Add button from the left-side menu.</p> <p></p> </li> <li> <p>Click the Import From Git option in the Git Repository section of the +Add page.</p> <p></p> </li> <li> <p>In the Git Repo URL field, enter the URL of your forked repository.</p> <p>It will look something like: https://github.com/YOUR_GITHUB_USERNAME/node-s2i-openshift</p> <p></p> </li> <li> <p>Click the Show Advanced Git Options hyperlink.</p> </li> <li> <p>In the Context Dir field, enter <code>/site</code>.</p> <p></p> Recall <p>This is the folder in the GitHub repository that you dug into to view the source code.</p> </li> <li> <p>For Builder Image, select the Node.js tile. It is likely that OpenShift will detect the programming language in the GitHub repository and automatically select the Node.js builder image for you. Continue on if this is the case.</p> </li> <li> <p>Scroll to the bottom of this page and click the Create button.</p> <p>You will be taken to the Topology page, which will show your new application along with three smaller circular buttons that can be used to perform different actions against the application.</p> </li> <li> <p>Click the circular Node.js application icon.</p> <p>At first, the icon will be all white and you will see \u201cBuild #1 is running\u201d in the righthand panel. If you wish, you can watch the logs for the running Build to see everything it\u2019s doing. After a minute or two, the icon will have a green check mark next to it, indicating the Build is complete. Once the Build is complete, your Pod will be created. You can also watch the logs for this, if you wish. About 10 seconds later, a solid blue ring will appear around the edge of the circular icon, indicating that the pod is up and running.</p> <p></p> Note <p>Feel free to click on the View Logs hyperlink to watch everything that the build is creating. When complete, the log will display <code>Push Successful</code>, and you can return to the Topology page by clicking the link on the left side of the page.</p> <p>If you clicked off of the Node.js application Resources tab, click on the circular icon again, then click on the Resources tab.</p> </li> <li> <p>Click the Route URL \u2013 beginning with http://</p> <p></p> <p>You will be taken to the Example Health application.</p> <p></p> </li> <li> <p>Log into the Example Health application using the following credentials:</p> <ul> <li> <p>Username: <code>test</code></p> </li> <li> <p>Password: <code>test</code></p> </li> </ul> <p></p> <p>All of the data in this application is simulated to look similar to the health records of an insurance company. Feel free to explore the application and notice the multiple tabs it contains.</p> <p>The JavaScript code you looked at in your forked GitHub repository was containerized by OpenShift\u2019s S2I function and deployed onto the OpenShift cluster.</p> <p>Now that your application is running in a container in OpenShift, let\u2019s see how an application developer can make a change to the source code, and then push the update to the running application.</p> <p>We\u2019ll make a simple change in a few lines of text to demonstrate. As we see in the Example Health application, there is a section with Personal Information.</p> <p></p> </li> <li> <p>In your forked GitHub repository, navigate to the source code again:</p> </li> <li> <p>Make sure you are in your own fork of the repository.</p> </li> <li> <p>From https://github.com/YOUR_GITHUB_USERNAME/node-s2i-openshift, click on the site folder.</p> </li> <li> <p>Click on the app.js file.</p> </li> <li> <p>With the app.js file open, click on the edit button pointed out in the picture below.</p> <p></p> </li> <li> <p>Scroll down to line 55, which displays the patient name.</p> </li> <li> <p>Edit lines 55-60 as you wish, modifying the text strings for Name, Age, Gender, etc.</p> <pre><code>if (CURRENTMODE == MODE.TEST) {\n    patientdata = {\n    \"personal\": {\n        \"name\": \"Matt Mondics\",\n        \"age\": 29,\n        \"gender\": \"male\",\n        \"street\": \"1234 State St.\",\n        \"city\": \"Cleveland\",\n        \"zipcode\": \"44106\"\n    },    \n</code></pre> </li> <li> <p>Click Commit Changes at the bottom of the page.</p> </li> </ol> <p>You just edited the source code, but you still need to push the update to the running application.</p> <ol> <li> <p>Back in the OpenShift console, navigate to the Topology page -&gt; Click the Node.js application icon -&gt; Click the Resources tab -&gt; Click the Start Build button.</p> <p></p> <p>A new Build #2 will be created. As with the first build, you can view the build logs to watch everything it\u2019s doing, or you can simply wait for the console to display <code>Build #2 is complete</code> and your Pod <code>Running</code>.</p> </li> <li> <p>When the Pod is <code>running</code>, refresh the Example Health browser tab.</p> <p></p> </li> </ol> <p>Your code changes have been pushed to the running Example Health application.</p>"},{"location":"lab004/lab004-5/","title":"Cleaning Up","text":"<p>There is no easy way to delete all of these objects from the OpenShift console. This is a much easier task in the OpenShift command line.</p> <ol> <li> <p>In the OpenShift CLI, make sure you are in your own project (i.e. userNN-project) and run the following command:</p> <pre><code>oc delete all --selector app=node-s2i-openshift -o name\n</code></pre> Note <p>If you are not connected to the OpenShift command line, refer to Using the OpenShift Command Line.</p> </li> </ol> <p>In this lab, you have exposed JavaScript source code in a GitHub repository to an OpenShift cluster, which containerized that JavaScript code into a container image, and then deployed it as a container running in a pod. You then made a code change to the JavaScript code and pushed an update to the application while it was running</p>"},{"location":"lab005/lab005-1/","title":"Monitoring, Metering, and Metrics","text":"<p>A significant architectural shift toward containers is underway and, as with any architectural shift, this brings new operational challenges. It can be challenging for many of the legacy monitoring tools to monitor container platforms in fast moving, often ephemeral environments. The good news is newer cloud-based offerings can ensure monitoring solutions are as scalable as the services being built and monitored. These new solutions have evolved to address the growing need to monitor your stack from the bottom to the top.</p> <p>From an operations point of view, infrastructure monitoring tools collect metrics about the host or container, such as CPU load, available memory and network I/O.</p> <p>The default monitoring stack is the 3-pronged open source approach of, Grafana, Alertmanager, and Prometheus.</p> <p>Prometheus gives you finely grained metrics at a huge scale. With the right configuration, Prometheus can handle millions of time series.</p> <p>Grafana can visualize the data being scraped by Prometheus. Grafana comes with pre-built dashboards for typical use cases, or you can create your own custom ones.</p> <p>Alertmanager forwards alerts to a service such as Slack or another webhook . Alertmanager can use metadata to classify alerts into groups such as errors, notifications, etc.</p> <p>The Grafana-Alertmanager-Prometheus monitoring stack provides a highly configurable, open source option to monitor Kubernetes workloads.</p> <p></p>"},{"location":"lab005/lab005-2/","title":"Connect to OCP and Authenticate","text":"<ol> <li> <p>In your virtual machine desktop, open a Firefox web browser.</p> </li> <li> <p>In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/.</p> Note <p>You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized.</p> <p>Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe).</p> <p>You will likely need to do this twice due to how OpenShift reroutes Oauth requests. </p> Expand for screenshot <p> </p> <p>You will now see the OpenShift console login page.</p> <p></p> </li> <li> <p>Log in with the OpenShift credentials provided to you on the Lab Assignments page.</p> <p>Hint</p> <p>Your OpenShift credentials will be something like the following:</p> <ul> <li> <p>Username: userNN (where NN is your user number)</p> </li> <li> <p>Password: p@ssw0rd</p> </li> </ul> </li> </ol>"},{"location":"lab005/lab005-3/","title":"Using OpenShifft Metrics (Prometheus)","text":"<p>OpenShift provides a web interface to Prometheus, which enables you to run Prometheus Query Language (PromQL) queries and visualize the metrics on a plot. This functionality provides an extensive overview of the cluster state and helps to troubleshoot problems.</p> <ol> <li> <p>In the OpenShift console, switch to the Administrator perspective if you are not already on it.</p> <p></p> </li> <li> <p>In the menu bar on the left side of the page, click Observe and then Metrics.</p> <p></p> <p>You will be taken to a Prometheus interface within the OpenShift console.</p> <p></p> <p>Once you enter a query, the graph will populate.</p> </li> <li> <p>Enter the following string in the query bar:</p> <pre><code>namespace:container_memory_usage_bytes:sum\n</code></pre> </li> <li> <p>Hit your enter key or click the associated query result that is returned.</p> <p>The string will populate the query text box.</p> </li> <li> <p>If the graph and table are not automatically populated. click the blue \"Run Queries\" button.</p> <p>The graph should now display the memory usage over time for each namespace.</p> <p></p> </li> <li> <p>Scroll down the page to the table displaying each namespace and its memory usage in bytes.</p> <p></p> <p>Your table will look different depending on what work is being done in the OpenShift cluster at the time.</p> <p>Notice that you have observability of the entire OpenShift cluster, even though you cannot access or edit projects other than your own. In other words, you have read-only access to the full OpenShift cluster via the Observability stack, but you read-write access within your userNN-project.</p> <p>OpenShift passes around a massive amount of data to run itself and the applications running on top of it. Prometheus is an extremely powerful data source that can return results for millions of time strings with extremely granular precision.</p> <p>Because of OpenShift\u2019s vast data production and Prometheus\u2019 ability to process it, certain queries can produce simply too much data to be useful. Because Prometheus makes use of labels, we can use these labels to filter data to make better sense of it.</p> </li> <li> <p>Modify your query to the following:</p> <pre><code>namespace:container_memory_usage_bytes:sum{namespace=\"userNN-project\"}\n</code></pre> <p>Important</p> <p>Make sure you change the one instance of <code>NN</code> to your user number.</p> <p>Also, notice that they are squiggly brackets <code>{}</code> in the query, not regular parentheses.</p> </li> <li> <p>Click Run Queries</p> <p></p> <p>Your graph is now displaying the memory usage over time for your own project. If you see a \u201cNo datapoints found\u201d message, select a longer timeframe using the dropdown menu in the top left of the graph.</p> Note <p>If you skipped ahead to this lab without completing the others, it\u2019s possible that your project has not had workload deployed in it for more than the maximum time frame. If this is the case, run a simple application in your project, and you will see the data start to populate (refer to Exploring the OpenShift Console for help with this.)</p> </li> </ol> <p>As you might have noticed, working directly with Prometheus can be tedious and requires specific PromQL queries that aren\u2019t the easiest to work with. That\u2019s why people typically use Prometheus for its data source functionality, and then move to Grafana for the data visualization.</p>"},{"location":"lab005/lab005-4/","title":"Using the In-Browser Grafana Dashboards","text":"<ol> <li> <p>From the OpenShift menu, navigate to Observability -&gt; Dashboards.</p> <p></p> <p>This takes you to an in-browser user interface for the Grafana monitoring solution. By default, there are various preconfigured dashboards for common use cases.</p> <p></p> </li> <li> <p>Click the \"Dashboard\" dropdown in the top-left of the page, and select another that is of interest to you.</p> </li> </ol>"},{"location":"lab005/lab005-5/","title":"Connect to Grafana","text":"<ol> <li> <p>To utilize further Grafana functions, navigate to the Grafana UI at the following address.</p> <p>https://grafana-openshift-monitoring.apps.atsocppa.dmz/</p> Expand for More Information <p>Where is this URL coming from? It is exposed service (or route) for the Grafana service. You could open a terminal and run the following command to find the URLs to Prometheus, Grafana, and Alertmanager:</p> <pre><code>root # ===&gt; oc -n openshift-monitoring get routes\nNAME                HOST/PORT                              \nalertmanager-main   alertmanager-main-openshift-monitoring.apps.atsocppa.dmz\ngrafana             grafana-openshift-monitoring.apps.atsocppa.dmz\nprometheus-k8s      prometheus-k8s-openshift-monitoring.apps.atsocppa.dmz\n</code></pre> Information <p>You might see a security challenge if the cluster has not yet been accessed from your workstation. Accept the challenge to continue.</p> <p>You should now see login page prompting your OpenShift credentials.</p> </li> <li> <p>Log into Grafana using your OpenShift credentials.</p> <ul> <li>Username: userNN</li> <li>Password: p@ssw0rd</li> </ul> <p>Notice that the credentials you use to log into Grafana are the same as those you use to log into OpenShift itself. OpenShift\u2019s role-bases access control (RBAC) functionality extends to the monitoring stack, so administrators can control who can see this part of the environment.</p> </li> </ol>"},{"location":"lab005/lab005-6/","title":"Using Grafana Dashboards","text":"<p>Once logged into Grafana, you\u2019ll be taken to the Home Dashboard from which you can navigate to your starred or recently viewed dashboards. You can also install various types of plugins from the official Grafana list and also from third-party sources from this page.</p> <p></p> <ol> <li> <p>In the Grafana left-side menu, click the Browse option.</p> <p></p> </li> <li> <p>Expand the Default dashboards if they aren\u2019t already visible.</p> <p></p> <p>A list of the recent and pre-installed dashboards will pop up. Notice that you can search dashboards by keyword in the search bar up top, or filter by labels on the right side.</p> <p></p> </li> <li> <p>Click the Kubernetes / Compute Resources / Cluster link.</p> <p></p> <p>You will see a dashboard populated with information related to the cluster\u2019s compute resources such as CPU and memory utilization. This dashboard displays CPU usage and CPU quota/memory requests by namespace.</p> </li> <li> <p>Click the CPU Usage dropdown above the first graph in this dashboard and click View.</p> Information <p>Alternatively, you can click on the CPU Usage graph to activate it, and hit the <code>V</code> key on your keyboard.</p> <p>This will bring up a full screen view of the graph to more easily see details.</p> <p></p> </li> <li> <p>Click a random namespaces in the chart\u2019s legend.</p> <p></p> </li> <li> <p>Hold the Shift key and click a few more namespaces.</p> <p></p> <p>This will display only the CPU usage for the few namespaces you selected.</p> </li> <li> <p>Click the Share dashboard button in the top right of the page.</p> <p></p> <p>From here, you can share a snapshot of the graph either internally or externally. When creating a snapshot to share externally, sensitive data will be stripped.</p> Note <p>If you try to share or export a graph here, you will find that it\u2019s unsuccessful.</p> <p>The userNN profiles have administrator-viewer credentials, so you are limited to the features you can actually change. A profile with full cluster administrator authority would have more access to Grafana functions such as sharing graphs and snapshots, creating their own custom dashboards, editing and saving pre-built dashboards, and installing various plugins and other tools that extend Grafana\u2019s built-in features.</p> </li> <li> <p>Close this browser window when you are ready to move on.</p> </li> </ol>"},{"location":"lab005/lab005-7/","title":"Using OpenShift Alerts with Alertmanager","text":"<p>Alerting with Prometheus is separated into two parts. Alerting rules in Prometheus send alerts to Alertmanager. Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email or chat platforms like Slack.</p> <p></p> <p>An example rules file with an alert would be:</p> <pre><code>groups:\n- name: example\n  rules:\n  - alert: HighRequestLatency\n    expr: job:request_latency_seconds:mean5m{job=\"myjob\"} &gt; 0.5\n    for: 10m\n    labels:\n      severity: page\n    annotations:\n      summary: High request latency\n</code></pre> <p>The optional <code>for</code> clause causes Prometheus to wait for a certain duration between first encountering a new expression output vector element and counting an alert as firing for this element. In this case, Prometheus will check that the alert continues to be active during each evaluation for 10 minutes before firing the alert. Elements that are active, but not firing yet, are in the pending state.</p> <p>The <code>labels</code> clause allows specifying a set of additional labels to be attached to the alert. Any existing conflicting labels will be overwritten.</p> <p>The <code>annotations</code> clause specifies a set of informational labels that can be used to store longer additional information such as alert descriptions or runbook links.</p> <ol> <li> <p>In the menu bar on the left side of the OpenShift console, click Observe and then Alerting.</p> <p>You will be taken to an Alertmanager interface within the OpenShift console.</p> <p></p> </li> <li> <p>Click the Alerting Rules tab to see the 100+ alerts that are not currently firing (hopefully!)</p> <p></p> <p>These alerts come pre-built with the monitoring stack, and they will start firing if triggered. This list includes alerts for critical operators going down, pods crash-looping, nodes being unreachable, and many more. Feel free to look through them.</p> </li> </ol>"},{"location":"lab006/lab006-1/","title":"Using Persistent Storage - MongoDB and NodeJS","text":"<p>In production Kubernetes clusters, many applications are stateful - their data needs to persist even if their pods regenerate or scale up/down. In this lab, you will see how this is achieved using PersistentVolumes (PV) and PersistentVolumeClaims (PVC).</p> <p>OpenShift on IBM Z supports various types of persistent storage, including Spectrum Scale, OpenShift Data Foundation, and NFS, which is what we will use in this lab. In this OpenShift cluster, there is a Dynamic Provisioner installed which automatically creates PersistentVolumes and their required NFS paths whenever a PersistentVolumeClaim is created. This provides a more hands-off approach for OpenShift developers and administrators so that nobody needs to manually create a PV or do anything in the external NFS server. </p> <p></p> <p>In this lab, you will deploy an application consisting of two components, a containerized Node.js web application and a containerized MongoDB instance, which you will back with persistent storage. Using the Node.js web application, you will be able to query the database, as well as insert new data into it.</p> <p></p> <p>To deploy the Node.js application, you will build and run the container from a Dockerfile residing in a GitHub repository.</p> <p>To deploy MongoDB, you will pull a MongoDB image from  and run it. The image in quay.io is the official MongoDB container image pulled from Docker Hub and moved to the Quay registry. This was done simply because Dockerhub has rate limits on pull requests from their public repository."},{"location":"lab006/lab006-2/","title":"Connect to OCP and Authenticate","text":"<ol> <li> <p>In your virtual machine desktop, open a Firefox web browser.</p> </li> <li> <p>In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/.</p> Note <p>You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized.</p> <p>Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe).</p> <p>You will likely need to do this twice due to how OpenShift reroutes Oauth requests. </p> Expand for screenshot <p> </p> <p>You will now see the OpenShift console login page.</p> <p></p> </li> <li> <p>Log in with the OpenShift credentials provided to you on the Lab Assignments page.</p> <p>Hint</p> <p>Your OpenShift credentials will be something like the following:</p> <ul> <li> <p>Username: userNN (where NN is your user number)</p> </li> <li> <p>Password: p@ssw0rd</p> </li> </ul> </li> </ol>"},{"location":"lab006/lab006-3/","title":"Create a PersistentVolumeClaim","text":"<p>As described in a previous section, a PersistentVolume has been already been predefined for each lab user. Next you will create a PersistentVolumeClaim that will bind to one of the available PersistentVolumes.</p> <ol> <li> <p>Change to the Administrator perspective, if not already there.</p> </li> <li> <p>In the left-side menu, click Storage -&gt; PersistentVolumeClaims and double check that you're on your userNN-project.</p> <p></p> </li> <li> <p>Click \u201cCreate Persistent Volume Claim\u201d.</p> <p></p> <p>The Create Persistent Volume Claim form has four fields, and you\u2019ll need to manually change a few of them.</p> <ul> <li> <p>For Storage Class, leave the default, <code>nfs-client</code></p> </li> <li> <p>For Persistent Volume Claim Name, change the value to pvc-userNN (Replacing NN with your user #).</p> </li> <li> <p>For Access Mode, select Shared Access (RWX).</p> </li> <li> <p>For Size, change the value to 2 Gi.</p> </li> </ul> <p>Your form should look like the following:</p> <p></p> </li> <li> <p>Click the create button.</p> <p>You\u2019ll be brought to the Overview for your newly created Persistent Volume Claim. The status of your claim should be Bound.</p> <p></p> Note <p>If your PersistentVolumeClaim does not bind almost immediately to a PersistentVolume, you likely did not fill out the fields as described above. You can delete your persistent volume claim and try again by clicking on the Actions dropdown and selecting Delete PersistentVolumeClaim.</p> </li> </ol>"},{"location":"lab006/lab006-4/","title":"Deploy MongoDB from a Container Image","text":"<p>In this section, you will be deploying a container using a MongoDB container image from quay.io. A container image holds a set of software that is ready to run, while a container is a running instance of a container image. Images can be hosted in registries, such as the quay.io registry, Docker Hub, or a private registry of your own.</p> <ol> <li> <p>Toggle to the Developer Perspective and ensure you are in the correct userNN-project.</p> <p></p> </li> <li> <p>Click Add+ from the left-side menu.</p> </li> <li> <p>Click the Container Images option.</p> <p>This brings up a new page which prompts you for an image name and further configurable parameters further down the page. Only the image name is required, and the rest will automatically populate for you.</p> </li> <li> <p>In the search bar for Image Name from external registry, type <code>quay.io/mmondics/mongo</code>.</p> <p></p> <p>A green check mark and a \u201cvalidated\u201d message will appear in the search bar, indicating that the MongoDB image has been found and validated in Quay.</p>  Important  <p>The fields below will automatically populate, but it is imperative that you change the Name field for your MongoDB service, or else the Node.js app will not be able to find and connect to the database.</p> </li> <li> <p>For Application Name, leave the default value.</p> </li> <li> <p>Replace the value of the Name field with mongodb</p> <p>Leave the rest of the form unchanged.</p> <p></p> </li> <li> <p>Click the Create button.</p> <p>You will now be taken to the Topology view, where you will see an icon for your new MongoDB deployment.</p> </li> <li> <p>Click the icon for the mongodb deployment.</p> <p>This will bring up a window on the right side of the screen with information about your deployment.</p> </li> <li> <p>Select the Details tab, if not already selected.</p> <p></p> <p>Depending on how quickly you clicked the icon, it will display either 1 pod, or 0 scaling to 1. If it has not scaled up to 1 pod yet, it will after a few seconds. However, we\u2019re going to be adding and removing storage, so we will kill the pod once it comes up.</p> </li> <li> <p>Click the Down Arrow to reduce the pod count to zero.</p> <p></p> <p>When a MongoDB pod is created, two storage volumes are attached to it. Let\u2019s examine those.</p> </li> <li> <p>Click the mongodb deployment in the right-side window.</p> <p></p> </li> <li> <p>Once on the Deployment Details page, scroll down Volumes section.</p> <p>Upon its creation, the pod created two volumes, mongodb-1 and mongodb-2. By default, MongoDB stores data in the <code>/data/db</code> directory, which is where the mongodb-2 volume has been mounted. This volume is not persistent. If the pod gets deleted, all of the stored data will be lost.</p> <p>To make your MongoDB data persistent, you will delete mongodb-2 and instead mount your persistent volume claim at <code>/data/db</code>.</p> </li> <li> <p>Click the three dots for the mongodb-2 volume.</p> </li> <li> <p>Click Remove Volume.</p> <p></p> <p>Now, you\u2019ll add persistent storage, mounting the volume at <code>/data/db</code>.</p> </li> <li> <p>Still on the Deployment Details page, scroll back up to the top and click the Actions dropdown.</p> </li> <li> <p>Click Add Storage.</p> <p></p> <p>To add storage to your MongoDB deployment, you will need to fill out a couple of fields.</p> </li> <li> <p>Use Existing Claim should already be selected.</p> </li> <li> <p>From the Select Claim dropdown menu, select pvc-userNN.</p> </li> <li> <p>For Mount Path, enter <code>/data/db</code>.</p> </li> <li> <p>For Subpath, enter your userNN, where NN is your user number.</p> <p></p> </li> <li> <p>Click the Save button.</p> <p>You will now be returned to the Deployment Details page. In the same way that you reduced the pod count to zero, you will now bring it back up to one.</p> </li> <li> <p>Click the Up Arrow to increase the pod count to one.</p> <p></p> </li> <li> <p>Scroll Down to the section labeled Volumes, and you\u2019ll see the persistent volume now mounted at <code>/data/db</code>.</p> <p></p> <p>Now you\u2019re ready to deploy the Node.js web application.</p> </li> </ol>"},{"location":"lab006/lab006-5/","title":"Deploy Node.js Application from a Dockerfile","text":"<p>In this portion of the lab, you will deploy a Node.js web application created by the ATG. You will be building your deployment from a Dockerfile residing in a GitHub repository. Through the web application, you will be able to insert data into and query the MongoDB database you just created. For this lab the database will store sample name and email pairs displayed as a list of user information in the web UI.</p> <p>First, you need to deploy the application.</p> <ol> <li> <p>Staying in the Developer Perspective, click +Add from the left-side menu.</p> </li> <li> <p>Click the Import from Git tile.</p> <p>Fill out the form as follows:</p> </li> <li> <p>In the Git Repo URL Field, enter:</p> <pre><code>https://github.com/mmondics/mongodb-app\n</code></pre> <p>You should see a \u201cValidated\u201d message below the URL field.</p> </li> <li> <p>Ensure that the value in the Application field is mongo-app.</p> </li> <li> <p>Replace the value in the Name field with nodejs-app.</p> </li> <li> <p>Leave Deployment checked.</p> <p></p> </li> <li> <p>Click the Create button. \u2003</p> <p>Your Node.js application will now pull the Dockerfile from GitHub and begin its build. You will be returned to the Topology view. You should see mongodb and nodejs-app grouped together in mongo-app. When the build is complete, you will see a blue ring form around nodejs-app. You can also check its status by clicking on the nodejs-app icon and examining the Details panel.</p> </li> <li> <p>Once the build is complete, click the Open URL button at the top right of the nodejs-app icon.</p> <p></p> <p>This button is simply a shortcut to the route that was created as part of the deployment.</p> <p>You are brought to the following landing page for your Node.js application:</p> <p></p> </li> </ol> <p>In the next section, you will insert data into and query your MongoDB database.</p>"},{"location":"lab006/lab006-6/","title":"Interacting with MongoDB from Node.js Web Application","text":"<p>You should be on the \u201cHello World\u201d landing page of your Node.js web application. If you have moved off of this screen, refer to the previous section for instructions on how to access the application.</p> <p>Since your application has not yet been used, the MongoDB database of user data will be empty. We can use the Node.js frontend application to insert data into the linked MongoDB pod and the persistent storage backing it.</p> <ol> <li> <p>In the Node.js application, click the Add a New User button.</p> <p></p> <p>You will be brought to a new page titled Add New User.</p> </li> <li> <p>Enter a sample username and email and click Submit.</p> <p></p> <p>You will be brought to a page titled User List which displays the entire contents of your database. Feel free to add additional users.</p> <p></p> <p>The data you just entered through the NodeJS web application is now stored in a MongoDB database backed by persistent storage on our NFS server. Now, let\u2019s test that our data will persist if we simulate a database crash by deleting our MongoDB pod.</p> </li> <li> <p>Return to the OpenShift Console Developer Perspective and navigate to the Topology View.</p> </li> <li> <p>Click the MongoDB icon.</p> </li> <li> <p>Click the down arrow to reduce the pod count to zero, terminating the MongoDB pod.</p> <p></p> </li> <li> <p>Return to the Node.js web application and refresh the page.</p> <p>The page will not connect, and if you wait long enough you will get a 504 Gateway Time-out error as the database no longer exists and no connection can be made.</p> <p></p> </li> <li> <p>Back in OpenShift, click the up arrow to increase the pod count back to 1.</p> </li> <li> <p>Return to the Node.js web application and refresh the page again.</p> <p>Your data still exists, even though the MongoDB pod was terminated and replaced by a completely new one.</p> <p>In this section, a new MongoDB pod was created. Since you mounted NFS persistent storage at <code>/data/db</code> in the original MongoDB pod, your data persisted even when the original MongoDB pod was deleted and replaced with a new one. Without persistent storage, the new MongoDB pod would have contained an empty database.</p> </li> </ol>"},{"location":"lab006/lab006-7/","title":"Cleaning Up","text":"<p>There is no easy way to delete all of these objects from the OpenShift console. This is a much easier task in the OpenShift command line.</p> <ol> <li> <p>In the OpenShift CLI, make sure you are in your own project (i.e. userNN-project) and run the following command:</p> <pre><code>oc delete all --all\n</code></pre> Note <p>If you are not connected to the OpenShift command line, refer to Using the OpenShift Command Line.</p> </li> </ol> <p>This will delete most of the objects in your project, but not the Persistent Volume Claim you created.</p> <ol> <li> <p>To delete the PVC, run the command:</p> <pre><code>oc delete pvc/pvc-userNN\n</code></pre> <p>Where <code>NN</code> is your user number.</p> </li> </ol>"},{"location":"lab007/lab007-1/","title":"Deploying an Application with the Open Liberty Operator","text":"<p>Note: this lab is a modified version of the GitHub repository here:</p> <p>https://github.com/OpenShift-Z/openliberty-operator-ocpz</p> <p>Open Liberty:</p> <ul> <li>is a lightweight, open framework for building fast and efficient cloud-native Java microservices</li> <li>is fast to start up with low memory footprint and live reload for quick iteration.</li> <li>is simple to add and remove features from the latest versions of MicroProfile and Java EE.</li> <li>requires zero migration lets you focus on what's important, not the APIs changing under you.</li> </ul> <p>The Open Liberty Operator can be used to deploy and manage Open Liberty applications into OpenShift clusters. You can also perform Day-2 operations such as gathering traces and dumps using the operator.</p> <p>Because the Open Liberty Operator watches all namespaces in the OpenShift cluster, workshop users are not required to deploy the Operator itself. It has already been deployed in the openshift-operators project.</p>"},{"location":"lab007/lab007-2/","title":"Log into OpenShift Using the CLI","text":"<p>In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line <code>oc</code> installed, so you don\u2019t have to install it on your Windows VM terminal.</p> <ol> <li> <p>Open a Terminal session</p> </li> <li> <p>ssh into the Linux Guest server:</p> <pre><code>ssh userNN@192.168.176.61\n</code></pre> <p>Where <code>NN</code> is your user number.</p> </li> <li> <p>When prompted, enter your password: <code>p@ssw0rd</code> and hit enter.</p> Example Output <p></p> </li> <li> <p>In Firefox, navigate to the following URL to request an API token:</p> <p>https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request</p> </li> <li> <p>Enter your OpenShift credentials when prompted.</p> <ul> <li> <p>Username: <code>userNN</code></p> </li> <li> <p>Password: <code>p@ssw0rd</code></p> </li> </ul> </li> <li> <p>Click the \u201cDisplay Token\u201d hyperlink.</p> <p></p> </li> <li> <p>Copy the contents of the first text box beginning with <code>oc login</code> and ending with <code>6443</code>.</p> <p></p> </li> <li> <p>Paste this command back in your terminal session and press enter.</p> <pre><code>oc login --token=&lt;YOUR_TOKEN_HERE&gt; --server=https://api.atsocppa.dmz:6443\n</code></pre> <p>Important</p> <p>If you\u2019re prompted to use an insecure connection, type Y and hit enter.</p> Example Output <pre><code>user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443\nLogged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided.\n\nYou have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"user01-project\".\n</code></pre> <p>You are now logged into the cluster via the command line, and you are told which project you are using.</p> <p>If you\u2019re in a project other than userNN-project, use the following command to move into it: <code>oc project userNN-project</code>, where NN is your user number.</p> </li> </ol>"},{"location":"lab007/lab007-3/","title":"Cloning the GitHub Repository and Reviewing its Contents","text":"<p>In the terminal session, you should have been automatically placed in your home directory <code>/home/userNN</code> (where NN is your user number).</p> <ol> <li> <p>Run the command <code>pwd</code> to check your current working directory.</p> Example Output <pre><code>user01@lab061:~$ pwd\n/home/user01\n</code></pre> </li> <li> <p>If you are in any other directory, change into the correct home directory using the command:</p> <pre><code>cd /home/userNN\n</code></pre> <p>Where <code>NN</code> is your user number.</p> Example Output <pre><code>user01@lab061:~$ cd /home/user01\nuser01@lab061:~$ pwd\n/home/user01\n</code></pre> </li> <li> <p>In your home directory, clone the Open Liberty Operator repository using the command:</p> <pre><code>git clone https://github.com/mmondics/openliberty-operator-ocpz\n</code></pre> Example Output <pre><code>user01@lab061:~$ git clone https://github.com/mmondics/openliberty-operator-ocpz\nCloning into 'openliberty-operator-ocpz'...\nremote: Enumerating objects: 70, done.\nremote: Counting objects: 100% (70/70), done.\nremote: Compressing objects: 100% (68/68), done.\nremote: Total 70 (delta 30), reused 2 (delta 1), pack-reused 0\nUnpacking objects: 100% (70/70), done.\nChecking connectivity... done.\n</code></pre> </li> <li> <p>This will create a new directory called <code>openliberty-operator-ocpz</code>. Change into this directory using the command:</p> <pre><code>cd openliberty-operator-ocpz\n</code></pre> </li> <li> <p>Then list its contents using the command:</p> <pre><code>ls -l\n</code></pre> Example Output <pre><code>user01@lab061:~$ cd openliberty-operator-ocpz\nuser01@lab061:~/openliberty-operator-ocpz$ ls -l\ntotal 24\n-rw-r--r-- 6 user01 users 4096 Sep  8 14:33 README.md\ndrwxr-xr-x 6 user01 users 4096 Sep  8 14:33 admin-ol-operator-install\ndrwxr-xr-x 6 user01 users 4096 Sep  8 14:33 images\ndrwxr-xr-x 6 user01 users 4096 Sep  8 14:33 ol-app-install\n</code></pre> Expand for More Information <p>If you navigate to the GitHub in a web browser (https://github.com/mmondics/openliberty-operator-ocpz), you will notice that the sub-directories in your Linux session reflect the folders contained in the repository.</p> File Description README.md Contains the content displayed on the GitHub page for this   repository. You can read through this README file if you want to get more   information about this lab. admin-ol-operator-install Directory used to install the Open Liberty Operator onto   the OpenShift cluster. Since this has been done ahead of time, you won\u2019t be   using this directory. images Contains the images referenced in the README.md file and   displayed on the GitHub page for this repository. ol-app-install Contains all of the files needed to build, push, and   deploy the Mod Resorts sample application. This is where we will be doing our   work for this lab. </li> <li> <p>Change into the <code>ol-app-install</code> directory using the command:</p> <pre><code>cd ol-app-install\n</code></pre> </li> <li> <p>List its contents using the command:</p> <pre><code>ls -l\n</code></pre> Example Output <pre><code>user01@lab061:~$ cd ol-app-install\nuser01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ls -l\ntotal 8416\n-rwxr--r\u20141 user01 users      845 Sep  8 14:33 1-build.sh\n-rwxr--r\u20141 user01 users      367 Sep  8 14:33 2-deploy.sh\n-rwxr--r\u20141 user01 users      612 Sep  8 14:33 3-cleanup.sh\n-rwxr--r\u20141 user01 users      142 Sep  8 14:33 Dockerfile\n-rwxr--r\u20141 user01 users      291 Sep  8 14:33 app-mod-withroute_cr.yaml\n-rwxr--r\u20141 user01 users      636 Sep  8 14:33 env\n-rwxr--r\u20141 user01 users   858364 Sep  8 14:33 modresorts-1.0.war\n-rwxr--r\u20141 user01 users      687 Sep  8 14:33 server.xml\n</code></pre> <p>This directory contains 8 files that you will use to install the Mod Resorts sample application.</p> Expand for More Information File Description <code>1-build.sh</code> shell script that contains commands to log into the   OpenShift Cluster and the internal registry, create a new project, build the   container image and then push it into the registry. <code>2-deploy.sh</code> shell script that contains commands to create the   OpenLibertyApplication custom resource based off of the image build and   pushed by 1-build.sh. <code>3-cleanup.sh</code> shell script that will clean up the OpenLibertyApplication   created by the previous scripts and delete the new project created by 1-build.sh. <code>Dockerfile</code> referenced by 1-build.sh to build the container image. <code>app-mod-withroute_cr.yaml</code> referenced by 2-deploy.sh to create the   OpenLibertyApplication custom resource. <code>env</code> environment variables sourced by the shell scripts for   various commands. <code>modresorts-1.0.war</code> a collection of JAR-files, JavaServer Pages, Java   Servlets, Java classes, etc\u2026 that together constitute the sample Mod Resorts   application. <code>server.xml</code> used in conjunction with the .war file to create the web   application. </li> </ol>"},{"location":"lab007/lab007-4/","title":"Using the Open Liberty Operator to Install an Application","text":"<ol> <li> <p>Edit the environment variables file to match your user number (NN).</p> <pre><code>sed -i 's/NN/YOUR_USER_NUMBER/g' env\n</code></pre> <p>Important</p> <p>Make sure that you replace YOUR_USER_NUMBER in the command above.</p> </li> <li> <p>Run the command <code>cat env</code> to check that the two instances of <code>NN</code> were properly replaced with your user number.</p> <p>With your modified environment variables file env, you\u2019re ready to run the shell script <code>1-build.sh</code> that will use Buildah to build a container image from the Dockerfile, and Podman to push the image into OpenShift\u2019s internal registry.</p> <p>Before running this script, take a look at the steps it will go through.</p> </li> <li> <p>View the script contents using the command:</p> <pre><code>cat 1-build.sh\n</code></pre> Example Output <pre><code>#!/bin/text\n\nunset KUBECONFIG\n\n. ./env\n\necho \"Logging into OpenShift\"\noc login $OPENSHIFT_API_URL \\\n    --username=$OPENSHIFT_USERNAME \\\n    --password=$OPENSHIFT_PASSWORD \\\n    --insecure-skip-tls-verify=true\n\necho \"Logging into OpenShift image registry\"\npodman login \\\n    --username $OPENSHIFT_USERNAME \\\n    --password $(oc whoami -t) \\\n    --tls-verify=false \\\n    $OPENSHIFT_REGISTRY_URL\n\necho \"Switch to $OPENSHIFT_PROJECT\"\noc project $OPENSHIFT_PROJECT\n\necho \"Building the container image\"\nbuildah build-using-dockerfile \\\n-t ${OPENSHIFT_REGISTRY_URL}/$OPENSHIFT_PROJECT/app-modernization:v1.0.0 \\\n    .\n\necho \"Pushing the container image to the OpenShift image registry\"\npodman push --tls-verify=false \\\n${OPENSHIFT_REGISTRY_URL}/${OPENSHIFT_PROJECT}/app-modernization:v1.0.0\n</code></pre> <p>This shell script:</p> <ul> <li>Logs you into the OpenShift cluster.</li> <li>Logs you into the OpenShift cluster\u2019s internal image registry.</li> <li>Switches to your project, if not currently working in it.</li> <li>Builds the container image using the Dockerfile contained in your working directory.</li> <li>Pushes the new container image from step 4 to the cluster\u2019s internal image registry.</li> </ul> Note <p>Note that you are welcome to enter each command manually and individually, but the scripting is there to minimize the opportunity for typos and other errors. If you do enter each command manually, make sure to replace each variable in BLUE with the actual value itself. Also, notice that the forward slash  simply breaks a single command into multiple lines.</p> </li> <li> <p>You might notice that the <code>Dockerfile</code> is doing the brunt of the work in this script to build the container image itself. Take a look at this too, using the command:</p> <pre><code>cat Dockerfile\n</code></pre> Example Output <pre><code>FROM quay.io/mmondics/open-liberty:latest\nCOPY --chown=1001:0 modresorts-1.0.war /config/dropins\nCOPY --chown=1001:0 server.xml /config/\n</code></pre> <p>This <code>Dockerfile</code> pulls the Open Liberty Java EE 8 image from Quay.io then adds the <code>modresorts-1.0.war</code> binary and <code>server.xml</code> configuration file to the base image.</p> <p>Back in the <code>ol-app-install</code> working directory, you can now run the script that brings all of these pieces together.</p> </li> <li> <p>Run the script using the command:</p> <pre><code>./1-build.sh\n</code></pre> Example Output <pre><code>user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ./1-build.sh\nLogging into Openshift\nLogin successful.\n\nYou have access to 170 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"user01-project\".\nLogging into Openshift image registry\nLogin Succeeded!\nSwitch to user01-project\nAlready on project \"user01-project\" on server \"https://api.atsocppa.dmz:6443\".\nBuilding the container image\nSTEP 1: FROM quay.io/mmondics/open-liberty:javaee8-ubi-min\nSTEP 2: COPY --chown=1001:0 modresorts-1.0.war /config/dropins\nSTEP 3: COPY --chown=1001:0 server.xml /config/\nSTEP 4: COMMIT default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/app-modernization:v1.0.0\nGetting image source signatures\nCopying blob d20db2e30c33 skipped: already exists\nCopying blob 4ec5f0a55d74 skipped: already exists\n\n... cut from screenshot ...\n\nCopying config c6e6f3bf6b done\nWriting manifest to image destination\nCopying config c6e6f3bf6b done\nWriting manifest to image destination\nStoring signatures\nuser01@lab061:~/openliberty-operator-ocpz/ol-app-install$\n</code></pre> <p>Your container image is now built and pushed into OpenShift\u2019s internal registry.</p> </li> <li> <p>View your new image in the registry using the command:</p> <pre><code>podman images\n</code></pre> Example Output <pre><code>user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ podman images\nREPOSITORY                                                                                 TAG     IMAGE ID       CREATED          SIZE\ndefault-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/app-modernization  v1.0.0  5970fc63cb46   16 seconds ago   472 MB\n</code></pre> <p>With the app-modernization image in the OpenShift internal registry, it can now be used to deploy an application into the cluster.</p> <p>You have one more file to edit before deploying the Mod Resorts sample application.</p> </li> <li> <p>Edit the custom resource file using the command:</p> <pre><code>sed -i 's/NN/YOUR_USER_NUMBER/g' app-mod-withroute_cr.yaml\n</code></pre> <p>Important</p> <p>Make sure that you replace YOUR_USER_NUMBER in the command above.</p> </li> <li> <p>Run the command:</p> <pre><code>cat app-mod-withroute_cr.yaml\n</code></pre> <p>And make sure that the two instances of <code>NN</code> were properly replaced with your user number.</p> Expand for More Information <p>What exactly is this YAML file?</p> <p>A Custom Resource Definition (CRD) object defines a new, unique object in the cluster and lets the Kubernetes API server handle its entire lifecycle. Custom Resource (CR) objects are created from CRDs that have been added to the cluster by a cluster administrator, allowing all cluster users to add the new resource type into projects.</p> <p>So in this case, a CRD was created ahead of time of the kind: OpenLibertyApplication, and you are creating a CR from that CRD. </p> <p>With the modified app-mod-withroute_cr.yaml file, you\u2019re ready to run the second script, <code>2-deploy.sh</code>. Before running it, take a look at what all it will do.</p> </li> <li> <p>Run the command:</p> <pre><code>cat 2-deploy.sh\n</code></pre> Example Output <pre><code>#!/bin/text\n\nunset KUBECONFIG\n\n. ./env\n\necho \"Logging into OpenShift\"\noc login $OPENSHIFT_API_URL \\\n    --username=$OPENSHIFT_USERNAME \\\n    --password=$OPENSHIFT_PASSWORD \\\n    --insecure-skip-tls-verify=true\n\necho \"Creating OpenLiberty Custom Resource\"\noc -n $OPENSHIFT_PROJECT create -f app-mod-withroute_cr.yaml\n</code></pre> <p>This shell script:</p> <ul> <li>Logs you into the OpenShift cluster.</li> <li>Creates an object from the Custom Resource (CR) YAML file you just edited.</li> </ul> </li> <li> <p>Run this shell script with the command:</p> <pre><code>./2-deploy.sh\n</code></pre> Example Output <pre><code>user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ./2-deploy.sh\nLogging into Openshift\nLogin successful.\n\nYou have access to 170 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"user01-project\".\nCreating Openliberty Custom Resource Definition\nopenlibertyapplication.openliberty.io/appmod created\n</code></pre> <p>Your Custom Resource named <code>appmod</code> of kind <code>OpenLibertyApplication</code> has been <code>created</code> in your project, <code>userNN-project</code>.  The creation of this CR resulted in the Open Liberty Operator deploying a pod, deployment, replicaset, and a service that is exposed as a route.</p> </li> <li> <p>View all of the created objects using the command:</p> <pre><code>oc get all\n</code></pre> Example Output <pre><code>user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ oc get all\nNAME                          READY   STATUS    RESTARTS   AGE\npod/appmod-5959fb64b5-fqkvg   1/1     Running   0          16s\n\nNAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nservice/appmod   ClusterIP   172.30.26.44   &lt;none&gt;        9080/TCP   17s\n\nNAME                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/appmod   1/1     1            1           16s\n\nNAME                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/appmod-5959fb64b5   1         1         1       16s\n\nNAME                                           IMAGE REPOSITORY                    TAGS     UPDATED\nimagestream.image.openshift.io/app-modernization   default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/app-modernization   v1.0.0   29 seconds ago\n\nNAME                              HOST/PORT                 PATH  SERVICES   PORT  TERMINATION WILDCARD\nroute.route.openshift.io/appmod   modresort-user01.apps.atsocppa.dmz   /resorts   appmod     9080-tcp     None\n</code></pre> </li> </ol>"},{"location":"lab007/lab007-5/","title":"Access the Application in a Browser","text":"<p>Your application is running and accessible via the exposed route.</p> <ol> <li> <p>In a web browser, navigate to the route:</p> <p>http://modresort-userNN.apps.atsocppa.dmz/resorts/</p> <p>Where NN is your user number.</p> <p></p> </li> </ol> <p>The demo application used (mod resorts) is admittedly a simple use-case with no dependencies on external resources, so you will notice that most of the links do not function. In real-world applications, there will be dependencies on external resources which can be integrated using various OpenShift and Kubernetes objects such as ConfigMaps, Volume mounts, secrets, etc.</p>"},{"location":"lab007/lab007-6/","title":"Cleaning Up","text":"<p>When you\u2019re ready to wrap up this lab, return to your terminal to run the last shell script.</p> <ol> <li> <p>Run the cleanup script with the command:</p> <pre><code>./3-cleanup.sh\n</code></pre> Example Output <pre><code>user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ./3-cleanup.sh\nLogging into OpenShift\nLogin successful.\n\nYou have access to 169 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"user01-project\".\nLogging into OpenShift image registry\nLogin Succeeded!\nDeleting Openliberty app\nopenlibertyapplication.openliberty.io \"appmod\" deleted\nDeleting imagestream\nimagestream.image.openshift.io \"app-modernization\" deleted\n</code></pre> </li> </ol> <p>Your OpenLiberty app consisting of a pod, deployment, service, route, and the imagestream used to create the pod have all been deleted.</p>"},{"location":"lab008/lab008-1/","title":"Deploying an Application with Quarkus Red Hat Runtime","text":"<p>Note</p> <p>For more Quarkus guides, see the official site here: https://quarkus.io/guides/</p> <p></p> <p>Quarkus is a full-stack, Kubernetes-native Java framework optimized specifically for containers and Kubernetes environments. It is designed to work with popular Java standards, frameworks, and libraries like Eclipse MicroProfile, Spring Apache Kafka, RESTEasy, and many more.</p> <p>Quarkus was designed for developers with the intent to be easy to use with features that work well with little to no configuration. It includes many features for developers, such as live coding so you can immediately check the effect of code changes and quickly troubleshoot them.</p> <p>Quarkus was built around a container-first philosophy, meaning it\u2019s optimized for lower memory usage and faster startup times. Quarkus builds applications to consume 1/10<sup>th</sup> the memory when compared to traditional Java, and has a much faster startup time (as much as 300 times faster), both of which greatly reduce the cost of cloud resources.</p> <p>In this lab, you will explore these features by:</p> <ul> <li>Creating a new Quarkus project</li> <li>Configuring the Quarkus application for OpenShift</li> <li>Deploying the Quarkus application to OpenShift</li> </ul>"},{"location":"lab008/lab008-2/","title":"Log into OpenShift Using the ClI","text":"<p>In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line <code>oc</code> installed, so you don\u2019t have to install it on your Windows VM terminal.</p> <ol> <li> <p>Open a Terminal session</p> </li> <li> <p>ssh into the Linux Guest server:</p> <pre><code>ssh userNN@192.168.176.61\n</code></pre> <p>Where <code>NN</code> is your user number.</p> </li> <li> <p>When prompted, enter your password: <code>p@ssw0rd</code> and hit enter.</p> Example Output <p></p> </li> <li> <p>In Firefox, navigate to the following URL to request an API token:</p> <p>https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request</p> </li> <li> <p>Enter your OpenShift credentials when prompted.</p> <ul> <li> <p>Username: <code>userNN</code></p> </li> <li> <p>Password: <code>p@ssw0rd</code></p> </li> </ul> </li> <li> <p>Click the \u201cDisplay Token\u201d hyperlink.</p> <p></p> </li> <li> <p>Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d.</p> <p></p> </li> <li> <p>Paste this command back in your terminal session and press enter.</p> <pre><code>oc login --token=&lt;YOUR_TOKEN_HERE&gt; --server=https://api.atsocppa.dmz:6443\n</code></pre> <p>Important</p> <p>If you\u2019re prompted to use an insecure connection, type Y and hit enter.</p> Example Output <pre><code>user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443\nLogged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided.\n\nYou have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"user01-project\".\n</code></pre> <p>You are now logged into the cluster via the command line, and you are told which project you are using.</p> <p>If you\u2019re in a project other than userNN-project, use the following command to move into it: <code>oc project userNN-project</code>, where NN is your user number.</p> </li> </ol>"},{"location":"lab008/lab008-3/","title":"Creating and Reviewing the Quarkus Project","text":"<p>In the terminal session, you should have been automatically placed in your home directory <code>/home/userNN</code> (where <code>NN</code> is your user number).</p> <ol> <li> <p>Run the command <code>pwd</code> to check your current working directory.</p> Example Output <pre><code>user01@lab061:~$ pwd\n/home/user01\n</code></pre> </li> <li> <p>If you are in any other directory, change into the correct home directory using the command:</p> <pre><code>cd /home/userNN\n</code></pre> <p>Where NN is your user number.</p> Example Output <pre><code>user01@lab061:~$ cd /home/user01\nuser01@lab061:~$ pwd\n/home/user01\n</code></pre> <p>We will start off by creating a Maven Project. Maven is a powerful project management tool based on POM (Project Object Model). It is a tool used by Java developers to simplify and add structure to their day-to-day work by implementing dependency and documentation into their Java applications.</p> <p>You can read more about Maven on their official site here: https://maven.apache.org/index.html.</p> <p>The following command uses the Maven Quarkus Plugin to create a basic Maven project for you in the <code>openshift-quickstart</code> subdirectory. It generates:</p> <ul> <li> <p>The Maven structure including the <code>pom.xml</code></p> </li> <li> <p>An org.acme.rest.GreetingResource resource exposed on <code>/greeting</code></p> </li> <li> <p>An associated unit test</p> </li> <li> <p>A landing page that is accessible on http://localhost:8080 after starting the application</p> </li> <li> <p>Example Dockerfiles for both native and jvm modes</p> </li> <li> <p>The application configuration file</p> </li> </ul> <p>Note that the forward slash  simply breaks the command into multiple lines for readability. Also note that if you do not specify the variables for projectGroupId, projectArtifactId, etc., the Maven installer will prompt you for them.</p> </li> <li> <p>In your home directory, run the following command:</p> <pre><code>mvn io.quarkus:quarkus-maven-plugin:1.8.3.Final:create \\\n-DprojectGroupId=org.acme \\\n-DprojectArtifactId=openshift-quickstart \\\n-DclassName=\"org.acme.rest.GreetingResource\" \\\n-Dpath=\"/greeting\" \\\n-Dextensions=\"resteasy\"\n</code></pre> Example Output <pre><code>user01@lab061:~$ user01@lab061:~$ mvn io.quarkus:quarkus-maven-plugin:1.8.3.Final:create \\\n&gt;     -DprojectGroupId=org.acme \\\n&gt;     -DprojectArtifactId=openshift-quickstart \\\n&gt;     -DclassName=\"org.acme.rest.GreetingResource\" \\\n&gt;     -Dpath=\"/greeting\" \\\n&gt;     -Dextensions=\u201dresteasy\u201d\n\n[INFO] Scanning for projects...\n[INFO] \n[INFO] ------------------&lt; org.apache.maven:standalone-pom &gt;-------------------\n[INFO] Building Maven Stub Project (No POM) 1\n[INFO] --------------------------------[ pom ]---------------------------------\n[INFO] \n[INFO] --- quarkus-maven-plugin:1.8.3.Final:create (default-cli) @ standalone-pom ---\n[INFO] \n[INFO] Maven Wrapper version 0.5.6 has been successfully set up for your project.\n[INFO] Using Apache Maven: 3.6.3\n[INFO] Repo URL in properties file: https://repo.maven.apache.org/maven2\n[INFO] \n[INFO] ========================================================================================\n[INFO] Your new application has been created in /home/user01/openshift-quickstart\n[INFO] Navigate into this directory and launch your application with mvn quarkus:dev\n[INFO] Your application will be accessible on http://localhost:8080\n[INFO] ========================================================================================\n[INFO] \n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  12.644 s\n[INFO] Finished at: 2020-10-19T11:35:57-04:00\n[INFO] ------------------------------------------------------------------------\n</code></pre> </li> <li> <p>As the installation says, an application has been created under the <code>openshift-quickstart</code> sub-directory. Change into this directory using the command:</p> <pre><code>cd openshift-quickstart\n</code></pre> </li> <li> <p>Then view its structure using the command:</p> <pre><code>tree\n</code></pre> Example Output <pre><code>    user01@lab061:~$ cd openshift-quickstart\n    user01@lab061:~/openshift-quickstart$ tree\n    total 24\n    .\n    |-- README.md\n    |-- mvnw\n    |-- mvnw.cmd\n    |-- pom.xml\n    `-- src\n        |-- main\n        |   |-- docker\n        |   |   |-- Dockerfile.fast-jar\n        |   |   |-- Dockerfile.jvm\n        |   |   `-- Dockerfile.native\n        |   |-- java\n        |   |   `-- org\n        |   |       `-- acme\n        |   |           `-- rest\n        |   |               `-- GreetingResource.java\n        |   `-- resources\n        |       |-- META-INF\n        |       |   `-- resources\n        |       |       `-- index.html\n        |       `-- application.properties\n        `-- test\n            `-- java\n                `-- org\n                    `-- acme\n                        `-- rest\n                            |-- GreetingResourceTest.java\n                            `-- NativeGreetingResourceIT.java\n\n    15 directories, 12 files\n</code></pre> <p>You can see that the command created the <code>pom.xml</code> file, Dockerfiles for both JVM and native modes, your GreetingResource.java file exposed at <code>/greeting</code>, and the associated test resources.</p> </li> <li> <p>Let\u2019s take a look at the <code>pom.xml</code> file that was created as a part of the installation. View the file using the command:</p> <pre><code>cat pom.xml\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-quickstart$ cat pom.xml\n\n...omitted...\n&lt;dependencyManagement&gt;\n    &lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;${quarkus.platform.group-id}&lt;/groupId&gt;\n        &lt;artifactId&gt;${quarkus.platform.artifact-id}&lt;/artifactId&gt;\n        &lt;version&gt;${quarkus.platform.version}&lt;/version&gt;\n        &lt;type&gt;pom&lt;/type&gt;\n        &lt;scope&gt;import&lt;/scope&gt;\n    &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n&lt;/dependencyManagement&gt;\n...omitted...\n\n&lt;build&gt;\n    &lt;plugins&gt;\n    &lt;plugin&gt;\n        &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n        &lt;artifactId&gt;quarkus-maven-plugin&lt;/artifactId&gt;\n        &lt;version&gt;${quarkus-plugin.version}&lt;/version&gt;\n        &lt;executions&gt;\n        &lt;execution&gt;\n            &lt;goals&gt;\n            &lt;goal&gt;generate-code&lt;/goal&gt;\n            &lt;goal&gt;generate-code-tests&lt;/goal&gt;\n            &lt;goal&gt;build&lt;/goal&gt;\n            &lt;/goals&gt;\n        &lt;/execution&gt;\n        &lt;/executions&gt;\n    &lt;/plugin&gt;\n    &lt;plugin&gt;\n        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n        &lt;version&gt;${compiler-plugin.version}&lt;/version&gt;\n    &lt;/plugin&gt;\n    &lt;plugin&gt;\n        &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;\n        &lt;version&gt;${surefire-plugin.version}&lt;/version&gt;\n        &lt;configuration&gt;\n        &lt;systemPropertyVariables&gt;\n            &lt;java.util.logging.manager&gt;org.jboss.logmanager.LogManager&lt;/java.util.logging.manager&gt;\n            &lt;maven.home&gt;${maven.home}&lt;/maven.home&gt;\n        &lt;/systemPropertyVariables&gt;\n        &lt;/configuration&gt;\n    &lt;/plugin&gt;\n    &lt;/plugins&gt;\n&lt;/build&gt;\n</code></pre> <p>The snippets above show the import of the Quarkus BOM, which allows you to omit the version on the different Quarkus dependencies. In addition, you can see the quarkus-maven-plugin responsible for the packaging of the application and providing the development mode.</p> <p>Next look at the dependencies section.</p> Example Output <pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-resteasy&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-junit5&lt;/artifactId&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n    &lt;groupId&gt;io.rest-assured&lt;/groupId&gt;\n    &lt;artifactId&gt;rest-assured&lt;/artifactId&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n    &lt;/dependency&gt;\n</code></pre> <p>You can see we are using Quarkus extensions which allow the development and testing of REST applications:</p> <p>During the installation, the <code>openshift-quickstart/src/main/java/org/acme/rest/GreetingResource.java</code> file was created. This is a simple REST endpoint, returning \u201chello\u201d to requests at <code>/greeting</code>.</p> </li> <li> <p>From the <code>openshift-quickstart</code> directory, view this file with the command:</p> <pre><code>cat src/main/java/org/acme/rest/GreetingResource.java\n</code></pre> Example Output <pre><code>package org.acme.rest;\n\nimport javax.ws.rs.GET;\nimport javax.ws.rs.Path;\nimport javax.ws.rs.Produces;\nimport javax.ws.rs.core.MediaType;\n\n@Path(\"/greeting\")\npublic class GreetingResource {\n\n    @GET\n    @Produces(MediaType.TEXT_PLAIN)\n    public String hello() {\n        return \"hello\";\n    }\n}\n</code></pre> <p>You can see that this file is simply telling the application to return \u201chello\u201d at the <code>/greeting</code> path.</p> </li> </ol>"},{"location":"lab008/lab008-4/","title":"Configure the Application for OpenShift","text":"<p>One of the great things about Quarkus is the plethora of extensions it provides out of the box. Quarkus extensions are comparable to Maven dependencies that allow for much easier use and integration into 3<sup>rd</sup> party projects.</p> <p>We will be using the Quarkus OpenShift extension. The OpenShift extension is actually a wrapper that brings together the kubernetes and container-image-s2i extensions with defaults specific to OpenShift.</p> <ol> <li> <p>In your terminal session, add the OpenShift extension to you application with the command:</p> <pre><code>./mvnw quarkus:add-extension -Dextensions=\"openshift\"\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-quickstart$ ./mvnw quarkus:add-extension -Dextensions=\"openshift\"\n[INFO] Scanning for projects...\n[INFO] \n[INFO] -------------------&lt; org.acme:openshift-quickstart &gt;--------------------\n[INFO] Building openshift-quickstart 1.0-SNAPSHOT\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO] \n[INFO] --- quarkus-maven-plugin:1.9.0.CR1:add-extension (default-cli) @ openshift-quickstart ---\n? Extension io.quarkus:quarkus-openshift has been installed\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  11.356 s\n[INFO] Finished at: 2020-10-23T15:42:02-04:00\n[INFO] ------------------------------------------------------------------------\n</code></pre> <p>This command added the following dependency to your <code>pom.xml</code> file:</p> Example Output <pre><code>    &lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-openshift&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n</code></pre> <p>This dependency is the generic quarkus.openshift extension, but it can be further customized with essentially any further nested dependency for OpenShift or Kubernetes objects you need. Some examples are in the table below, and the full list is here: https://quarkus.io/guides/kubernetes#openshift.</p> Expand for more Information Property Type quarkus.openshift.version String quarkus.openshift.env-vars Map quarkus.openshift.replicas int quarkus.openshift.service-account String quarkus.openshift.host String quarkus.openshift.ports Map quarkus.openshift.pvc-volumes Map quarkus.openshift.image-pull-policy ImagePullPolicy quarkus.openshift.image-pull-secrets String[] quarkus.openshift.liveness-probe Probe quarkus.openshift.readiness-probe Probe quarkus.openshift.expose boolean <p>You will be using a few of these customizations when you deploy the application to OpenShift in the next step.</p>"},{"location":"lab008/lab008-5/","title":"Deploy the Application onto OpenShift","text":"<p>Let\u2019s now take our local application and use the Quarkus extension we just added to build and deploy a containerized application onto OpenShift.</p> <ol> <li> <p>In the openshift-quickstart directory, run the command:</p> <pre><code>./mvnw clean package \\\n-Dquarkus.kubernetes.deploy=true \\\n-Dquarkus.kubernetes-client.trust-certs=true \\\n-Dquarkus.openshift.expose=true\n</code></pre> <p>The <code>-Dquarkus</code> flags in this command are telling Maven to deploy the application into the Kubernetes (OpenShift) cluster, trust the certificates, and expose the application service as a route, eliminating the need to run an <code>oc expose svc</code> to make the service endpoint accessible outside of the cluster.</p> <p>Information</p> <p>This command may take a few minutes to complete.</p> Example Output <pre><code>user01@lab061:~/openshift-quickstart$ ./mvnw clean package \\\n    -Dquarkus.kubernetes.deploy=true \\\n    -Dquarkus.kubernetes-client.trust-certs=true \\\n    -Dquarkus.openshift.expose=true\n[INFO] Scanning for projects...\n[INFO] \n[INFO] -------------------&lt; org.acme:openshift-quickstart &gt;--------------------\n[INFO] Building openshift-quickstart 1.0.0-SNAPSHOT\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO] \n...omitted...\n\n[INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] The deployed application can be accessed at: http://openshift-quickstart-user01-project.apps.atsocppa.dmz\n[INFO] [io.quarkus.deployment.QuarkusAugmentor] Quarkus augmentation completed in 94655ms\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  03:19 min\n[INFO] Finished at: 2021-03-09T12:12:14-05:00\n[INFO] ------------------------------------------------------------------------\n</code></pre> <p>The previous command builds a jar file locally, connects to the OpenShift cluster you previously logged into, triggers a container image build, pushes that container image into the OpenShift internal registry, generates OpenShift/Kubernetes resources including a Service, Route, DeploymentConfig, and your running application pod.</p> </li> <li> <p>View all of the created objects with the following command:</p> <pre><code>oc get all\n</code></pre> Example Output <pre><code>user01@lab061: ~/openshift-quickstart$ oc get all\nNAME                                READY   STATUS      RESTARTS   AGE\npod/openshift-quickstart-1-build    0/1     Completed   0          9m10s\npod/openshift-quickstart-1-deploy   0/1     Completed   0          8m13s\npod/openshift-quickstart-1-ndp8h    1/1     Running     0          8m10s\n\nNAME                                           DESIRED   CURRENT   READY   AGE\nreplicationcontroller/openshift-quickstart-1   1         1         1       8m13s\n\nNAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/openshift-quickstart   ClusterIP   172.30.28.241   &lt;none&gt;        8080/TCP   8m14s\n\nNAME                                                  REVISION   DESIRED   CURRENT   TRIGGERED BY\ndeploymentconfig.apps.openshift.io/openshift-quickstart   1          1         1         image(openshift-quickstart:1.0-SNAPSHOT)\n\nNAME                                                  TYPE     FROM     LATEST\nbuildconfig.build.openshift.io/openshift-quickstart   Source   Binary   1\n\nNAME                                              TYPE     FROM     STATUS     STARTED         DURATION\nbuild.build.openshift.io/openshift-quickstart-1   Source   Binary   Complete   9 minutes ago   55s\n\nNAME                    IMAGE REPOSITORY                                 TAGS                                      UPDATED\nimagestream.image.openshift.io/openjdk-11             default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/openjdk-11             1.3,1.3-3,1.3-3.1591609340 + 18 more...   9 minutes ago\nimagestream.image.openshift.io/openshift-quickstart   default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/openshift-quickstart   1.0-SNAPSHOT                              8 minutes ago\n\nNAME           HOST/PORT                        PATH      SERVICES               PORT   TERMINATION  \nroute.route.openshift.io/openshift-quickstart  openshift-quickstart-user01-project.apps.atsocppa.dmz   /      openshift-quickstart   8080\n</code></pre> <p>Each of these objects were created because of the <code>-Dquarkus.kubernetes.deploy=true</code> and <code>-Dquarkus.openshift.expose=true</code> flags provided in the previous command. There are many more OpenShift objects and object properties that can be created by passing different flags, such as liveliness probes, environment variables, secrets, persistent storage, and more.</p> <p>If you have one running pod, your application has successfully deployed and is accessible at the route.</p> </li> <li> <p>In a web browser, navigate to your route:</p> Hint <p>It will be http://openshift-quickstart-userNN-project.apps.atsocppa.dmz/ where NN is your user number.</p> <p></p> <p>Your Quarkus application is now deployed as a container in OpenShift.</p> <p>Earlier we looked at the GreetingResource.java REST endpoint and its return of \u201chello\u201d in the command line. We can do the same thing in the web browser.</p> </li> <li> <p>Add <code>/greeting</code> to the end of your route.</p> Hint <p>http://openshift-quickstart-userNN-project.apps.atsocppa.dmz/greeting where NN is your user number.</p> <p></p> <p>In this lab, you have created a Quarkus application locally, containerized the application and deployed it onto an OpenShift cluster running on IBM Z, and accessed it from a public route.</p> <p>The speed, agility, and ease with which we\u2019re able to edit and redeploy applications using the Quarkus runtime creates tremendous value in time savings, allowing developers and operations staff to minimize downtime and keep applications up to date. Further, the simplicity of integration using Quarkus extensions creates great opportunity for customization and implementations tailored to fit a variety of needs.</p> </li> </ol>"},{"location":"lab008/lab008-6/","title":"Cleaning Up","text":"<ol> <li> <p>Double check that you are in your own userNN-project by issuing the command:</p> <pre><code>oc project\n</code></pre> Example Output <pre><code>user01@lab061 ~/openshift-quickstart $ oc project\nUsing project \"user01-project\" on server \"https://api.atsocppa.dmz:6443\".\n</code></pre> </li> <li> <p>Once you\u2019re sure you\u2019re in your own project, issue the following command to delete all objects associated with your application labeled app.kubernetes.io/name=openshift-quickstart.  </p> <pre><code>oc delete all --selector app.kubernetes.io/name=openshift-quickstart\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-quickstart $ oc delete all --selector app.kubernetes.io/name=openshift-quickstart\npod \"openshift-quickstart-1-ztfq9\" deleted\nreplicationcontroller \"openshift-quickstart-1\" deleted\nservice \"openshift-quickstart\" deleted\ndeploymentconfig.apps.openshift.io \"openshift-quickstart\" deleted\nbuildconfig.build.openshift.io \"openshift-quickstart\" deleted\nbuild.build.openshift.io \"openshift-quickstart-1\" deleted\nimagestream.image.openshift.io \"openjdk-11\" deleted\nimagestream.image.openshift.io \"openshift-quickstart\" deleted\nroute.route.openshift.io \"openshift-quickstart\" deleted\n</code></pre> </li> <li> <p>To check that all of your mongo application resources were deleted, run the command:</p> <pre><code>oc get all\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-quickstart $ oc get all\nNo resources found.\nuser00@lab061:~$\n</code></pre> </li> </ol> Note <p>If there are leftover resources from other labs that you would like to delete, run the command:</p> <pre><code>oc delete all --all \n</code></pre>"},{"location":"lab009/lab009-0/","title":"Using OpenShift Pipelines","text":"<p>Red Hat OpenShift Pipelines is a cloud-native, continuous integration and continuous delivery (CI/CD) solution based on Kubernetes resources. It uses Tekton building blocks to automate deployments across multiple platforms by abstracting away the underlying implementation details. Tekton introduces a number of standard custom resource definitions (CRDs) for defining CI/CD pipelines that are portable across Kubernetes distributions.</p>"},{"location":"lab009/lab009-0/#key-features","title":"Key features","text":"<ul> <li>Red Hat OpenShift Pipelines is a serverless CI/CD system that runs pipelines with all the required dependencies in isolated containers.</li> <li>Red Hat OpenShift Pipelines are designed for decentralized teams that work on microservice-based architecture.</li> <li>Red Hat OpenShift Pipelines use standard CI/CD pipeline definitions that are easy to extend and integrate with the existing Kubernetes tools, enabling you to scale on-demand.</li> <li>You can use Red Hat OpenShift Pipelines to build images with Kubernetes tools such as Source-to-Image (S2I), Buildah, Buildpacks, and Kaniko that are portable across any Kubernetes platform.</li> <li>You can use the OpenShift Container Platform Developer console to create Tekton resources, view logs of pipeline runs, and manage pipelines in your OpenShift Container Platform namespaces.</li> </ul>"},{"location":"lab009/lab009-0/#what-is-tekton","title":"What is Tekton?","text":"<p>Tekton is an open source project that provides a framework to create cloud-native CI/CD pipelines quickly. As a Kubernetes-native framework, Tekton makes it easier to deploy across multiple cloud providers or hybrid environments. By leveraging the Custom Resource Definitions (CRDs) in Kubernetes, Tekton uses the Kubernetes control plane to run pipeline tasks. By using standard industry specifications, Tekton will work well with existing CI/CD tools such as Jenkins, Jenkins X, Skaffold, Knative, and now OpenShift.</p> <p>Source of images and information on this page: https://cloud.redhat.com/learn/topics/ci-cd</p>"},{"location":"lab009/lab009-1/","title":"Using OpenShift Pipelines","text":"<p>In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line <code>oc</code> installed, so you don\u2019t have to install it on your Windows VM terminal.</p> <ol> <li> <p>Open a Terminal session.</p> </li> <li> <p>ssh into the Linux Guest server:</p> <pre><code>ssh userNN@192.168.176.61\n</code></pre> <p>Where <code>NN</code> is your user number.</p> </li> <li> <p>When prompted, enter your password: <code>p@ssw0rd</code> and hit enter.</p> Example Output <p></p> </li> <li> <p>In Firefox, navigate to the following URL to request an API token:</p> <p>https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request</p> </li> <li> <p>Enter your OpenShift credentials when prompted.</p> <ul> <li> <p>Username: <code>userNN</code></p> </li> <li> <p>Password: <code>p@ssw0rd</code></p> </li> </ul> </li> <li> <p>Click the \u201cDisplay Token\u201d hyperlink.</p> <p></p> </li> <li> <p>Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d.</p> <p></p> </li> <li> <p>Paste this command back in your terminal session and press enter.</p> <pre><code>oc login --token=&lt;YOUR_TOKEN_HERE&gt; --server=https://api.atsocppa.dmz:6443\n</code></pre> <p>Important</p> <p>If you\u2019re prompted to use an insecure connection, type Y and hit enter.</p> Example Output <pre><code>user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443\nLogged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided.\n\nYou have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"user01-project\".\n</code></pre> <p>You are now logged into the cluster via the command line, and you are told which project you are using.</p> <p>If you\u2019re in a project other than userNN-project, use the following command to move into it: <code>oc project userNN-project</code>, where NN is your user number.</p> </li> </ol>"},{"location":"lab009/lab009-2/","title":"Cloning the GitHub Repository and Viewing its Contents","text":"<p>In the terminal session, you should have been automatically placed in your home directory <code>/home/userNN</code> (where NN is your user number).</p> <ol> <li> <p>Run the command <code>pwd</code> to check your current working directory.</p> Example Output <pre><code>user01@lab061:~$ pwd\n/home/user01\n</code></pre> </li> <li> <p>If you are in any other directory, change into the correct home directory using the command:</p> <pre><code>cd /home/userNN\n</code></pre> <p>(Where <code>NN</code> is your user number).</p> Example Output <pre><code>user01@lab061:~$ cd /home/user01\nuser01@lab061:~$ pwd\n/home/user01\n</code></pre> <ol> <li>In your home directory, clone the OpenShift Pipelines repository using the command:</li> </ol> <pre><code>git clone https://github.com/mmondics/openshift-pipelines-s390x \n</code></pre> Example Output <pre><code>user01@lab061:~$ git clone https://github.com/mmondics/openshift-pipelines-s390x\nCloning into 'openshift-pipelines-s390x'...\nremote: Enumerating objects: 25, done.\nremote: Counting objects: 100% (25/25), done.\nremote: Compressing objects: 100% (21/21), done.\nremote: Total 25 (delta 5), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (25/25), done.\nChecking connectivity... done\n</code></pre> </li> <li> <p>This will create a new directory called <code>openliberty-pipelines-s390x</code>. Change into this directory using the command:</p> <pre><code>cd openshift-pipelines-s390x\n</code></pre> </li> <li> <p>List its contents using the command:</p> <pre><code>ls -l\n</code></pre> Example Output <pre><code>user01@lab061:~$ cd openliberty-operator-ocpz\nuser01@lab061:~/openliberty-operator-ocpz$ ls -l\ntotal 16\n-rw-r--r-- 1 user00 users   48 Mar 16 14:20 README.md\ndrwxr-xr-x 2 user00 users 4096 Mar 16 14:20 pipeline\n-rw-r--r-- 1 user00 users  251 Mar 22 13:23 pipeline-cleanup.sh\ndrwxr-xr-x 2 user00 users 4096 Mar 16 14:20 resources\ndrwxr-xr-x 2 user00 users 4096 Mar 16 14:20 tasks\n</code></pre> <p>If you navigate to the GitHub repository in a web browser https://github.com/mmondics/openshift-pipelines-s390x, you will notice that the sub-directories in your Linux session reflect the folders contained in the repository.</p> File Description README.MD Contains   the content displayed on the GitHub page for this repository. You can read through   this README file if you want to get more information about this lab. pipeline Directory   containing the YAML file that will be used to create a Pipeline. pipeline-cleanup.sh Shell   script that will delete all objects created in this lab. resources Directory   containing the YAML file that will create a PersistentVolumeClaim in   the cluster. tasks Directory   containing YAML files to create various Tasks that make up a pipeline. </li> </ol>"},{"location":"lab009/lab009-3/","title":"Understanding and Deploying Tasks","text":"<p>A Task defines a series of steps that run in a desired order and complete a set amount of build work. Every Task runs as a Pod on your OpenShift cluster with each step as its own container. Tasks have one single responsibility so they can be reused across different Pipelines or in multiple places within a single Pipeline.</p> <p>The repository you pulled includes the YAML files needed to create three Tasks. Let\u2019s take a look at one of them.</p> <ol> <li> <p>From the openshift-pipelines-s390x directory, run the command:</p> <pre><code>cat tasks/hello.yaml\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ cat tasks/hello.yaml\napiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: hello\nspec:\nsteps:\n    - name: say-hello\n    image: registry.access.redhat.com/ubi8/ubi\n    command:\n        - /bin/text\n    args: ['-c', 'echo Hello World']\n</code></pre> <p>This file will create a Kubernetes Task object called hello that is made up of one step. That step has its own name, image, command, and args associated with it. As explained above, once created, this Task will create one Pod that includes one Container.</p> </li> <li> <p>Create the Task using the command:</p> <pre><code>oc create -f tasks/hello.yaml\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ oc create -f tasks/hello.yaml\ntask.tekton.dev/hello created\n</code></pre> <p>The Task is now created in your project and can be run using Tekton, the CI/CD tool that OpenShift Pipelines are based on.</p> </li> <li> <p>Run the <code>hello</code> task using the command:</p> <pre><code>tkn task start --showlog hello\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ tkn task start --showlog hello\nTaskRun started: hello-run-xvr92\nWaiting for logs to be available...\n[say-hello] Hello World\n</code></pre> <p>Running the <code>tkn task start</code> command created a new Kubernetes resource called a TaskRun. TaskRuns are automatically created for each Task that is run in a Pipeline, but as you can see, they can also be manually created by running a Task. This can be useful for debugging a single Task in a Pipeline.</p> </li> <li> <p>Your Pipeline will consist of three Tasks total. Create the remaining Tasks using the commands:</p> <pre><code>oc create -f tasks/apply_manifest_task.yaml\n</code></pre> <p>and</p> <pre><code>oc create -f tasks/update_deployment_task.yaml\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ oc create -f tasks/apply_manifest_task.yaml\ntask.tekton.dev/apply-manifests created\nuser01@lab061:~/openshift-pipelines-s390x$ oc create -f tasks/update_deployment_task.yaml\ntask.tekton.dev/update-deployment created\n</code></pre> </li> <li> <p>You have now created three Tasks that will be plumbed together to create a Pipeline. To see them, run the command:</p> <pre><code>tkn task ls\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ tkn task ls\nNAME                DESCRIPTION   AGE\napply-manifests                   7 minutes ago\nhello                             8 minutes ago\nupdate-deployment                 7 minutes ago\n</code></pre> <p>You will also need a Workspace in which your will run all of the Tasks associated with your Pipeline. This will be a shared space across each Task, TaskRun, Pipeline, and PipelineRun that you associate with the Workspace. With a Workspace, you can store Task inputs and outputs, share data among Tasks, use it as a mount point for credentials held in Secrets, create a cache of build artifacts that speed up jobs, and more.</p> </li> <li> <p>In our case, we will be using a PersistentVolumeClaim as our Workspace. Create the PVC using the command:</p> <pre><code>oc create -f resources/persistent_volume_claim.yaml\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ oc create -f resources/persistent_volume_claim.yaml\npersistentvolumeclaim/source-pvc created\n</code></pre> </li> </ol> <p>In the next section, you will create a Pipeline that uses the Tasks and Workspace you just created to pull the source code of an application from GitHub and then builds and deploys it in a container on OpenShift.</p>"},{"location":"lab009/lab009-4/","title":"Understanding and Deploying Pipelines","text":"<p>A Pipeline consists of a series of Tasks that are executed to construct complex workflows that automate the build, deployment, and delivery of applications. It is a collection of PipelineResources, parameters, and one or more Tasks.</p> <p>Below is a diagram of the Pipeline you will be creating.</p> <p></p> <p>The repository you pulled provides the YAML file necessary to generate this Pipeline.</p> <ol> <li> <p>Take a look at the YAML by using the command:</p> <pre><code>cat pipeline/pipeline.yaml\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ cat tasks/hello.yaml\napiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nname: build-and-deploy\nspec:\nworkspaces:\n- name: shared-workspace\nparams:\n- name: deployment-name\n    type: string\n    description: name of the deployment to be patched\n- name: git-url\n    type: string\n    description: url of the git repo for the code of deployment\n- name: git-revision\n    type: string\n    description: revision to be used from repo of the code for deployment\n    default: \"master\"\n- name: IMAGE\n    type: string\n    description: image to be build from the code\ntasks:\n- name: fetch-repository\n    taskRef:\n    name: git-clone\n    kind: ClusterTask\n    workspaces:\n    - name: output\n    workspace: shared-workspace\n    params:\n    - name: url\n    value: $(params.git-url)\n    - name: subdirectory\n    value: \"\"\n    - name: deleteExisting\n    value: \"true\"\n    - name: revision\n    value: $(params.git-revision)\n- name: build-image\n    taskRef:\n    name: buildah\n    kind: ClusterTask\n    params:\n    - name: TLSVERIFY\n    value: \"false\"\n    - name: IMAGE\n    value: $(params.IMAGE)\n    workspaces:\n    - name: source\n    workspace: shared-workspace\n    runAfter:\n    - fetch-repository\n- name: apply-manifests\n    taskRef:\n    name: apply-manifests\n    workspaces:\n    - name: source\n    workspace: shared-workspace\n    runAfter:\n    - build-image\n- name: update-deployment\n    taskRef:\n    name: update-deployment\n    params:\n    - name: deployment\n    value: $(params.deployment-name)\n    - name: IMAGE\n    value: $(params.IMAGE)\n    runAfter:\n    - apply-manifests\n</code></pre> <p>The Tasks included in this pipeline and their responsibilities are as follows:</p> <ul> <li> <p>fetch-repository clones the source code of the application from a GitHub repository based on the git-url and git-revision parameters.</p> </li> <li> <p>build-image builds the container image of the application using Buildah.</p> </li> <li> <p>apply-manifests deploys the application to OpenShift by running the oc apply command on the new container image with the provided parameters.</p> </li> <li> <p>update-deployment will update the application in OpenShift with the oc patch command when changes are needed.</p> </li> </ul> <p>You will notice that there are no references to the GitHub repository or the image registry that will be pushed to in the pipeline. This is because Pipelines are designed to be generic and re-used in different situations or to deploy different applications. Pipelines abstract away the specific parameters that can be passed into the Pipeline. When triggering the Pipeline, you will provide different GitHub repositories and images to be used when executed.</p> <p>Also notice that the execution order of Tasks can be determined by dependencies defined between Tasks via inputs and outputs, or explicitly ordered via runAfter.</p> </li> <li> <p>Create the Pipeline with the command:</p> <pre><code>oc create -f pipeline/pipeline.yaml\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ oc create -f pipeline/pipeline.yaml\npipeline.tekton.dev/build-and-deploy created\n</code></pre> <p>Although we are using pre-built YAML files to simplify the creation of these resources, everything in this lab could also be done in the OpenShift console in a browser.</p> </li> <li> <p>Take a look at the graphical representation of your Pipeline by accessing the cluster at the URL: https://console-openshift-console.apps.atsocppa.dmz/</p> <p>Username: <code>userNN</code> (where NN is your user number)</p> <p>Password: <code>p@ssw0rd</code></p> </li> <li> <p>Navigate to the Developer Perspective -&gt; Pipelines -&gt; select your userNN Project.</p> <p></p> </li> <li> <p>Click your new Pipeline called <code>build-and-deploy</code>.</p> <p></p> <p>The framework of your Pipeline has been created, and you can see the four Tasks that make up your Pipeline.</p> <p>Information</p> <p>If you remember making the apply-manifests and update-deployment Tasks, but not the \u201cfetch-repository\u201d and \u201cbuild-image\u201d Tasks -- you aren\u2019t wrong. These are ClusterTasks that come pre-built into OpenShift.</p> </li> </ol> <p>In the next section you will trigger a PipelineRun to execute your Pipeline and the Tasks it contains.</p>"},{"location":"lab009/lab009-5/","title":"Running the Pipeline","text":"<p>Let\u2019s use this Pipeline to create an application. To demonstrate the re-usability of OpenShift Pipelines, we will be creating both a frontend and a backend with the same Pipeline you created in the previous step.</p> <p>We\u2019ll also demonstrate the flexibility provided by OpenShift Pipelines that lets you use them from either the web console or the command line.</p> <p>Let\u2019s create the backend application with the Tekton CLI in your terminal.</p> <p>Now that you have all of the building blocks in place, you can start the Pipeline with the following command. The command will run the Pipeline and pass in parameters to:</p> <ul> <li> <p>Use the shared workspace and the PersistentVolumeClaim you created</p> </li> <li> <p>Create the deployment named vote-api</p> </li> <li> <p>Build a container image from the source code at the given GitHub repository</p> </li> <li> <p>Push that container image into the OpenShift internal registry and tag it for your project</p> </li> <li> <p>Show the log so you can follow its progress Note that the forward slash  simply breaks the command into multiple lines for readability.</p> </li> </ul>"},{"location":"lab009/lab009-5/#creating-the-backend-application-through-the-cli","title":"Creating the Backend Application through the CLI","text":"<ol> <li> <p>Run the following command:</p> <pre><code>tkn pipeline start build-and-deploy \\\n -w name=shared-workspace,claimName=source-pvc \\\n -p deployment-name=vote-api \\\n -p git-url=https://github.com/mmondics/pipelines-vote-api.git \\\n -p IMAGE=image-registry.openshift-image-registry.svc:5000/userNN-project/pipelines-vote-api --showlog \n</code></pre> <p> Important </p> <p>Make sure you change the one instance of <code>NN</code> to your team number in the command above.</p> Expand for Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ tkn pipeline start build-and-deploy \\\n&gt; -w name=shared-workspace,claimName=source-pvc \\\n&gt; -p deployment-name=vote-api \\\n&gt; -p git-url=https://github.com/mmondics/vote-api.git \\\n&gt; -p IMAGE=image-registry.openshift-image-registry.svc:5000/userNN-project/vote-api \\\n&gt; --showlog\nPipelineRun started: build-and-deploy-run-75zqv\nWaiting for logs to be available...\n[fetch-respository : clone] + CHECKOUT_DIR=/workspace/output/\n[fetch-respository : clone] + [[ true == \\t\\r\\u\\e ]]\n[fetch-respository : clone] + cleandir\n[fetch-respository : clone] + [[ -d /workspace/output/ ]]\n[fetch-respository : clone] + rm -rf /workspace/output//Dockerfile /workspace/output//LICENSE /workspace/output//README.md /workspace/output//go.mod /workspace/output//go.sum /workspace/output//k8s /workspace/output//main.go /workspace/output//vendor\n[fetch-respository : clone] + rm -rf /workspace/output//.git /workspace/output//.gitignore\n[fetch-respository : clone] + rm -rf '/workspace/output//..?*'\n[fetch-respository : clone] + test -z ''\n[fetch-respository : clone] + test -z ''\n[fetch-respository : clone] + test -z ''\n[fetch-respository : clone] + /ko-app/git-init -url https://github.com/mmondics/vote-api.git -revision master -refspec '' -path /workspace/output/ -sslVerify=true -submodules=true -depth 1\n[fetch-respository : clone] {\"level\":\"info\",\"ts\":1616101272.5251348,\"caller\":\"git/git.go:165\",\"msg\":\"Successfully cloned https://github.com/mmondics/vote-api.git @ a08f579f6135293358b9423a3370e725ae1380cc (grafted, HEAD, origin/master) in path /workspace/output/\"}\n[fetch-respository : clone] {\"level\":\"info\",\"ts\":1616101272.6701891,\"caller\":\"git/git.go:203\",\"msg\":\"Successfully initialized and updated submodules in path /workspace/output/\"}\n[fetch-respository : clone] + cd /workspace/output/\n[fetch-respository : clone] ++ git rev-parse HEAD\n[fetch-respository : clone] + RESULT_SHA=a08f579f6135293358b9423a3370e725ae1380cc\n[fetch-respository : clone] + EXIT_CODE=0\n[fetch-respository : clone] + '[' 0 '!=' 0 ']'\n[fetch-respository : clone] + echo -n a08f579f6135293358b9423a3370e725ae1380cc\n[fetch-respository : clone] + echo -n https://github.com/mmondics/vote-api.git\n\n[build-image : build] + buildah --storage-driver=vfs bud --format=oci --tls-verify=false --no-cache -f ./Dockerfile -t image-registry.openshift-image-registry.svc:5000/user00-project/vote-api .\n[build-image : build] STEP 1: FROM image-registry.openshift-image-registry.svc:5000/openshift/golang:latest AS builder\n[build-image : build] Getting image source signatures\n[build-image : build] Copying blob sha256:ff637d5a66cba4903fc7d9343b0f9dbb4e1bf8ada19bd3934ea0edfb85dc4\n[build-image : build] Copying blob sha256:f7fb0662b957bcb1b5007f9b5502af4da4c13e17b7bc2eff4f02c3e5ec08e\n[build-image : build] Copying blob sha256:35aab756d1a095511ab75eeca5aa77a37fa62a258f3fa5bcfb37ad604e369\n[build-image : build] Copying blob sha256:7cc70ce0e0ee7fe5f8ea22894ad8c2f962f1dfdd00d05de91a32181c89179\n[build-image : build] Copying blob sha256:73986f838dc404255946f6aa282b0aeabc420faa4f21b572e1de735498edf\n[build-image : build] Copying config sha256:9e8f033b036bdb224dc931cfcaaf532da6a6ae7d779e8a09c52eed12305\n[build-image : build] Writing manifest to image destination\n[build-image : build] Storing signatures\n[build-image : build] STEP 2: WORKDIR /build\n[build-image : build] STEP 3: ADD . /build/\n[build-image : build] STEP 4: RUN export GARCH=\"$(uname -m)\" &amp;&amp; if [[ ${GARCH} == \"s390x\" ]]; then export GARCH=\"s390x\"; fi &amp;&amp; GOOS=linux GOARCH=${GARCH} CGO_ENABLED=0 go build -mod=vendor -o api-server \n[build-image : build] STEP 5: FROM scratch\n[build-image : build] STEP 6: WORKDIR /app\n[build-image : build] STEP 7: COPY --from=builder /build/api-server /app/api-server\n[build-image : build] STEP 8: CMD [ \"/app/api-server\" ]\n[build-image : build] STEP 9: COMMIT image-registry.openshift-image-registry.svc:5000/user00-project/vote-api\n[build-image : build] --&gt; 36faca61f94\n[build-image : build] 36faca61f941af886128abd8792753095eaac7c1041084e222f426243ed50ecc\n\n[build-image : push] + buildah --storage-driver=vfs push --tls-verify=false --digestfile /workspace/source/image-digest image-registry.openshift-image-registry.svc:5000/user00-project/vote-api docker://image-registry.openshift-image-registry.svc:5000/user00-project/vote-api\n[build-image : push] + buildah --storage-driver=vfs push --tls-verify=false --digestfile /workspace/source/image-digest image-registry.openshift-image-registry.svc:5000/user00-project/vote-api docker://image-registry.openshift-image-registry.svc:5000/user00-project/vote-api\n[build-image : push] Getting image source signatures\n[build-image : push] Copying blob sha256:9eda1116f7414b98e397f94cc650fd50890c2d97fa47925e02b83df7726119\n[build-image : push] Copying config sha256:36faca61f941af886128abd8792753095eaac7c1041084e222f426243ed5\n[build-image : push] Writing manifest to image destination\n[build-image : push] Storing signatures\n\n[build-image : digest-to-results] + cat /workspace/source/image-digest\n[build-image : digest-to-results] + tee /tekton/results/IMAGE_DIGEST\n[build-image : digest-to-results] sha256:a7d730f92530c2f10891c55ba86a44e4fcc907436831c99733779ffb0d0fe8\n\n[apply-manifests : apply] Applying manifests in k8s directory\n[apply-manifests : apply] deployment.apps \"vote-api\" created\n[apply-manifests : apply] service \"vote-api\" created\n[apply-manifests : apply] -----------------------------------\n\n[update-deployment : patch] deployment.apps \"vote-api\" patched\n</code></pre> <p>If you see the final <code>deployment.apps \u201cvote-api\u201d patched</code> line, your PipelineRun was successful and your backend application is now deployed in OpenShift.</p> </li> <li> <p>Look at your running application Pod by issuing the command:</p> <pre><code>oc get pod\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ oc get pod\nNAME                                                           READY   STATUS      RESTARTS   AGE\nbuild-and-deploy-run-sgtc7-apply-manifests-9p6mv-pod-95jhw     0/1     Completed   0          9m52s\nbuild-and-deploy-run-sgtc7-build-image-6kh6n-pod-pkvgx         0/3     Completed   0          12m\nbuild-and-deploy-run-sgtc7-fetch-repository-flxfx-pod-p6nzh    0/1     Completed   0          13m\nbuild-and-deploy-run-sgtc7-update-deployment-hgxrz-pod-htqpf   0/1     Completed   0          9m33s\nvote-api-6765569bfb-v4jlh                                      1/1     Running     0          9m20s\n</code></pre> <p>You should see one running Pod and four completed Pods. The running Pod is your application that the Pipeline pulled from GitHub, containerized, pushed into the internal OpenShift repository, and started. The completed Pods were created to complete the Tasks defined in the Pipeline, and each is made up of one container per step in the Task.</p> <p>Looking at the READY column, you can see that most of the Pods have one container, with the exception of the build-image Pod that has three.</p> </li> <li> <p>The Tekton CLI also provides a way to check your Pipelines and PipelineRuns by running:</p> <pre><code>tkn pipeline ls\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ tkn pipeline ls\nNAME               AGE             LAST RUN                     STARTED         DURATION    STATUS\nbuild-and-deploy   4 minutes ago   build-and-deploy-run-2q5fp   4 minutes ago   3 minutes   Succeeded\n</code></pre> </li> </ol> <p>Since we have successfully run the Pipeline in the CLI, let\u2019s trigger a run from the console in the next section.</p>"},{"location":"lab009/lab009-5/#creating-the-frontend-application-through-the-console","title":"Creating the Frontend Application through the Console","text":"<p>Let\u2019s create the frontend portion of our application by running the Pipeline from the OpenShift console.</p> <ol> <li> <p>If you\u2019ve closed out of the OpenShift console in your web browser, go back to https://console-openshift-console.apps.atsocppa.dmz/</p> </li> <li> <p>Navigate to the Developer Perspective -&gt; Pipelines -&gt; and select your userNN Project.</p> <p></p> <p>The main Pipelines page displays the same information returned from the <code>tkn pipeline ls</code> command.</p> </li> <li> <p>Click your build-and-deploy Pipeline and then click the Actions -&gt; Start button.</p> <p></p> <p>This will open a new window that prompts you for the parameters with which to start your second PipelineRun. This window is the GUI equivalent to the multi-line <code>tkn pipeline start</code> command that we entered in the CLI PipelineRun.</p> </li> <li> <p>Enter the following parameters:</p> <ul> <li> <p>deployment name: <code>vote-ui</code></p> </li> <li> <p>git-url: <code>https://github.com/mmondics/pipelines-vote-ui.git</code></p> </li> <li> <p>git-revision: <code>master</code></p> </li> <li> <p>IMAGE: <code>image-registry.openshift-image-registry.svc:5000/userNN-project/vote-ui</code></p> </li> <li> <p>shared-workspace: <code>PersistentVolumeClaim</code> -&gt; <code>source-pvc</code></p> </li> </ul> <p>Important</p> <p>Make sure you change the one instance of <code>NN</code> in the IMAGE field to your user number.</p> </li> <li> <p>Then click start.</p> <p></p> <p>You will be taken to the page for your PipelineRun and shown the graphical representation of the running Pipeline.</p> <p></p> </li> <li> <p>Click the logs tab to follow what\u2019s happening in more detail like you saw in the CLI.</p> <p></p> </li> </ol> <p>When you see the PipelineRun has Succeeded and the <code>deployment.apps \u201cvote-ui\u201d has been patched</code>, your frontend application is also up and running.</p> <p>With both your backend and frontend applications are running, in the next section we\u2019ll access it in a browser.</p>"},{"location":"lab009/lab009-6/","title":"Accessing the Pipeline in a Browser","text":"<p>Your application is accessible via its route.</p> <ol> <li> <p>In the OpenShift console, navigate to the Topology page in the Developer Perspective and make sure you\u2019re in your userNN-project.</p> <p></p> <p>You should see two Icons with solid blue bars indicating your application pods are running without error.</p> </li> <li> <p>On the vote-ui icon, click the button in the top right corner to navigate to the application\u2019s exposed route.</p> <p></p> <p>This will open a new browser tab for your frontend application UI.</p> <p></p> </li> <li> <p>Click the box for your desired option.</p> <p>By casting your vote with the vote-ui frontend, you are invoking a REST API call and sending a POST request that is stored in the vote-api backend application.</p> <p>You can see this POST request reflected in the vote-api Pod logs.</p> </li> <li> <p>In your terminal session find the name of your vote-api Pod using the command:</p> <pre><code>oc get pods | grep Running\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ oc get pods | grep Running\nvote-api-6765569bfb-p2bhh                                      1/1     Running     0          65m\nvote-ui-6846f88f6f-rzzgt                                       1/1     Running     0          18m\n</code></pre> </li> <li> <p>Copy the full name for your vote-api Pod and view its logs with the command:</p> <pre><code>oc logs pod/vote-api-XXXXXXXXXX-YYYYY \n</code></pre> <p>Important</p> <p>Your randomly-generated Pod names will differ.</p> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ oc logs pod/vote-api-6765569bfb-p2bhh\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n- using env:    export GIN_MODE=release\n- using code:   gin.SetMode(gin.ReleaseMode)\n\n[GIN-debug] GET    /vote                     --&gt; main.setupRouter.func1 (3 handlers)\n[GIN-debug] POST   /vote                     --&gt; main.setupRouter.func2 (3 handlers)\n[GIN-debug] Listening and serving HTTP on :9000\n[GIN] 2021/03/22 - 16:18:18 | 200 |     179.658\u00b5s |    10.131.1.157 | POST     /vote\n[GIN] 2021/03/22 - 16:18:48 | 200 |     107.379\u00b5s |    10.131.1.157 | POST     /vote\n</code></pre> <p>You can see your POST requests at the <code>/vote</code> endpoint at the bottom, and more detail is stored in NFS by the PersistentVolumeClaim you created earlier.</p> </li> </ol> <p>In this lab, you have:</p> <ul> <li>Created Tasks that have specific responsibilities in the building and deploying of a containerized application onto an OpenShift on IBM Z cluster</li> <li>Created a Pipeline that combines these Tasks to one end-to-end process</li> <li>Ran the Pipeline twice -- once from the command line, and once from the OpenShift console -- to create a backend and a frontend application.</li> <li>Used the created applications to invoke a REST API call that is stored persistently in NFS storage.</li> </ul>"},{"location":"lab009/lab009-7/","title":"Cleaning Up","text":"<ol> <li> <p>When you\u2019re ready to finish the lab and delete the objects you created, return to your terminal and double check that you\u2019re in your own userNN-project with:</p> <pre><code>oc project\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ oc project\nUsing project \"user00-project\" on server \"https://api.atsocpd2.dmz:6443\".\n</code></pre> </li> <li> <p>Run the uninstall script:</p> <pre><code>./pipeline-cleanup.sh\n</code></pre> Example Output <pre><code>user01@lab061:~/openshift-pipelines-s390x$ ./pipeline-cleanup.sh\nRunning oc delete all --all\npod \"build-and-deploy-run-6hgg7-apply-manifests-ksns5-pod-zvnq8\" deleted\npod \"build-and-deploy-run-6hgg7-build-image-4bstq-pod-sd2xv\" deleted\npod \"build-and-deploy-run-6hgg7-fetch-repository-jgqml-pod-ld4nj\" deleted\npod \"build-and-deploy-run-6hgg7-update-deployment-qhmxh-pod-c56gj\" deleted\npod \"build-and-deploy-run-fnx5s-apply-manifests-k8v7f-pod-88fpm\" deleted\npod \"build-and-deploy-run-fnx5s-build-image-4xknh-pod-kq4zk\" deleted\npod \"build-and-deploy-run-fnx5s-fetch-repository-m5pmr-pod-8m5gt\" deleted\npod \"build-and-deploy-run-fnx5s-update-deployment-fgq9s-pod-2s4vw\" deleted\npod \"hello-run-l2skb-pod-vzvrj\" deleted\npod \"vote-api-6765569bfb-p59vj\" deleted\npod \"vote-ui-6846f88f6f-z7zp9\" deleted\nservice \"vote-api\" deleted\nservice \"vote-ui\" deleted\ndeployment.apps \"vote-api\" deleted\ndeployment.apps \"vote-ui\" deleted\nreplicaset.apps \"vote-ui-566848fff4\" deleted\nreplicaset.apps \"vote-ui-6846f88f6f\" deleted\nimagestream.image.openshift.io \"vote-api\" deleted\nimagestream.image.openshift.io \"vote-ui\" deleted\nroute.route.openshift.io \"vote-ui\" deleted\nDeleting Pipeline &amp; resources\npipeline.tekton.dev \"build-and-deploy\" deleted\npipelinerun.tekton.dev \"build-and-deploy-run-6hgg7\" deleted\npipelinerun.tekton.dev \"build-and-deploy-run-fnx5s\" deleted\ntask.tekton.dev \"apply-manifests\" deleted\ntask.tekton.dev \"hello\" deleted\ntask.tekton.dev \"update-deployment\" deleted\ntaskrun.tekton.dev \"hello-run-l2skb\" deleted\nDeleting PVC\npersistentvolumeclaim \"source-pvc\" deleted\nRemoving Images\n</code></pre> </li> </ol>"},{"location":"lab010/lab010-1/","title":"OpenShift Service Mesh","text":"<p>Red Hat OpenShift Service Mesh (OSSM) provides a platform for behavioral insight and operational control over your networked microservices in a service mesh. With OSSM, you can connect, secure, and monitor microservices in your OpenShift Container Platform environment.</p> <p>A Service Mesh is the network of microservices that make up applications and the interactions between those microservices. When a Service Mesh grows in size and complexity, it can become harder to understand and manage. Take, for example, an application made up of 5 microservices that is managed from the OpenShift Console and/or Command Line.</p> <p></p> <p>Based on the open source Istio project, OSSM adds a transparent layer on existing microservice applications without requiring any changes to the application code. You add OSSM support to services by deploying a sidecar proxy to relevant microservices in the mesh that intercepts all network communication between microservices. See the figure below.</p> <p></p> <p>OpenShift Service Mesh gives you an easy way to create a network of deployed services that provide:</p> <ul> <li>Discovery</li> <li>Load balancing</li> <li>Service-to-service authentication</li> <li>Failure recovery</li> <li>Metrics</li> <li>Monitoring</li> </ul> <p>OpenShift Service Mesh also provides more complex operational functions including:</p> <ul> <li>A/B testing</li> <li>Canary releases</li> <li>Rate limiting</li> <li>Access control</li> <li>End-to-end authentication</li> </ul> <p>In this lab, we will be exploring many of the OSSM features above.</p>"},{"location":"lab010/lab010-10/","title":"Wrap Up &amp; Clean Up","text":"<p>In this lab, you have explored many of the features that come as part of OpenShift Service Mesh. OSSM is an extremely powerful OpenShift add-on, and we were not able to fit all of its features into this lab. You can find more information &amp; tutorials at the following documentation links:</p> <ul> <li>OpenShift Service Mesh: https://docs.openshift.com/container-platform/4.7/service_mesh/v2x/ossm-about.html</li> <li>Istio: https://istio.io/latest/docs/</li> <li>Kiali: https://kiali.io/documentation/</li> <li> <p>Jaeger: https://www.jaegertracing.io/docs/1.24/</p> </li> <li> <p>When you\u2019re ready to clean up and finish this lab, run the following script to delete all the resources from your OpenShift project.</p> <pre><code>./cleanup.sh\n</code></pre> Example Output <pre><code>user01@lab061:~/istio-s390x$ ./cleanup.sh \nserviceaccount \"bookinfo-details\" deleted\nserviceaccount \"bookinfo-productpage\" deleted\nserviceaccount \"bookinfo-ratings\" deleted\nserviceaccount \"bookinfo-reviews\" deleted\ndeployment.apps \"details-v1\" deleted\ndeployment.apps \"productpage-v1\" deleted\ndeployment.apps \"ratings-v1\" deleted\ndeployment.apps \"reviews-v1\" deleted\ndeployment.apps \"reviews-v2\" deleted\ndeployment.apps \"reviews-v3\" deleted\nservice \"details\" deleted\nservice \"productpage\" deleted\nservice \"ratings\" deleted\nservice \"reviews\" deleted\ngateway.networking.istio.io \"bookinfo-gateway\" deleted\nvirtualservice.networking.istio.io \"bookinfo\" deleted\ndestinationrule.networking.istio.io \"details\" deleted\ndestinationrule.networking.istio.io \"productpage\" deleted\ndestinationrule.networking.istio.io \"ratings\" deleted\ndestinationrule.networking.istio.io \"reviews\" deleted\nCleanup Complete\n</code></pre> </li> </ul>"},{"location":"lab010/lab010-2/","title":"OpenShift Service Mesh Architecture","text":"<p>OpenShift Service Mesh is logically split into a data plane and a control plane:</p> <p>The data plane is a set of intelligent proxies deployed as sidecars. These proxies intercept and control all inbound and outbound network communication between microservices in the service mesh.</p> <ul> <li>Envoy proxy intercepts all inbound and outbound traffic for all services in the service mesh. Envoy is deployed as a sidecar to the relevant service in the same pod.</li> </ul> <p>The control plane manages and configures Istiod to enforce proxies to route traffic.</p> <p>Istiod provides service discovery, configuration and certificate management. It converts high-level routing rules to Envoy configurations and propagates them to the sidecars at runtime.</p>"},{"location":"lab010/lab010-3/","title":"Log into OpenShift Using the CLI","text":"<p>In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line <code>oc</code> installed, so you don\u2019t have to install it on your Windows VM terminal.</p> <ol> <li> <p>Open a Terminal session</p> </li> <li> <p>ssh into the Linux Guest server:</p> <pre><code>ssh userNN@192.168.176.61\n</code></pre> <p>Where <code>NN</code> is your user number.</p> </li> <li> <p>When prompted, enter your password: <code>p@ssw0rd</code> and hit enter.</p> Example Output <p></p> </li> <li> <p>In Firefox, navigate to the following URL to request an API token:</p> <p>https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request</p> </li> <li> <p>Enter your OpenShift credentials when prompted.</p> <ul> <li> <p>Username: <code>userNN</code></p> </li> <li> <p>Password: <code>p@ssw0rd</code></p> </li> </ul> </li> <li> <p>Click the \u201cDisplay Token\u201d hyperlink.</p> <p></p> </li> <li> <p>Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d.</p> <p></p> </li> <li> <p>Paste this command back in your terminal session and press enter.</p> <pre><code>oc login --token=&lt;YOUR_TOKEN_HERE&gt; --server=https://api.atsocppa.dmz:6443\n</code></pre> <p>Important</p> <p>If you\u2019re prompted to use an insecure connection, type Y and hit enter.</p> Example Output <pre><code>user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443\nLogged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided.\n\nYou have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"user01-project\".\n</code></pre> <p>You are now logged into the cluster via the command line, and you are told which project you are using.</p> <p>If you\u2019re in a project other than userNN-project, use the following command to move into it: <code>oc project userNN-project</code>, where NN is your user number.</p> </li> </ol>"},{"location":"lab010/lab010-4/","title":"Cloning the GitHub Repository and Reviewing its Contents","text":"<p>In the terminal session, you should have been automatically placed in your home directory <code>/home/userNN</code> (where NN is your user number).</p> <ol> <li> <p>Run the command <code>pwd</code> to check your current working directory.</p> Example Output <pre><code>user01@lab061:~$ pwd\n/home/user01\n</code></pre> </li> <li> <p>If you are in any other directory, change into the correct home directory using the command:</p> <pre><code>cd /home/userNN\n</code></pre> <p>Where NN is your user number.</p> Example Output <pre><code>user01@lab061:~$ cd /home/user01\nuser01@lab061:~$ pwd\n/home/user01\n</code></pre> </li> <li> <p>In your home directory, clone the OpenShift Service Mesh repository using the command:</p> <pre><code>git clone https://github.com/mmondics/istio-s390x -b ocp-wildfire\n</code></pre> Example Output <pre><code>user01@lab061:~$ git clone https://github.com/mmondics/istio-s390x -b ocp-wildfire\nCloning into 'istio-s390x'...\nremote: Enumerating objects: 171, done.\nremote: Counting objects: 100% (171/171), done.\nremote: Compressing objects: 100% (98/98), done.\nremote: Total 171 (delta 61), reused 159 (delta 52), pack-reused 0\nReceiving objects: 100% (171/171), 332.76 KiB | 0 bytes/s, done.\nResolving deltas: 100% (61/61), done.\nChecking connectivity... done.\n</code></pre> </li> <li> <p>This will create a new directory called istio-s390x. Change into this directory using the command:</p> <pre><code>cd istio-s390x \n</code></pre> </li> <li> <p>Then list its contents using the command:</p> <pre><code>ls -l\n</code></pre> Example Output <pre><code>user01@lab061:~$ cd Istio-s390x\nuser01@lab061:~/istio-s390x$ total 32\n-rw-r--r-- 1 user01 users 1306 Jun 24 12:54 README.md\n-rwxr-xr-x 1 user01 users 4029 Jun 24 12:54 build_push_update_images.sh\ndrwxr-xr-x 2 user01 users 4096 Jun 24 12:54 networking\ndrwxr-xr-x 3 user01 users 4096 Jun 24 12:54 platform\ndrwxr-xr-x 2 user01 users 4096 Jun 24 12:54 policy\ndrwxr-xr-x 8 user01 users 4096 Jun 24 12:54 src\n-rw-r--r-- 1 user01 users 6329 Jun 24 12:54 swagger.yaml\n</code></pre> <p>If you navigate to the GitHub in a web browser https://github.com/mmondics/istio-s390x/tree/ocp-wildfire, you will notice that the sub-directories in your Linux session reflect the folders contained in the repository.</p> File Description README.md Contains   the content displayed on the GitHub page for this repository. You can read   through this README file if you want to get more information about this lab. build_push_update_images.sh Directory   containing a shell script that was used to create the container images used   in this lab. You will not be using this script, but it is here for anyone who   wishes to update images to newer versions in the future. networking Directory   container various YAML files for networking components such as gateways,   virtualservices, destinationrules, and more. platform Directory   containing various YAML files that will create the application deployments,   services, serviceaccounts, and more. policy Directory   containing a YAML file that will create envoyfilters in order to dynamically   rate-limit the traffic to the service mesh application. src Directory   containing the source files used to build each container image used in this   lab. These source files will not be used in this lab. swagger.yaml A   YAML file that defines and documents the structure of the APIs used in this   lab. You will not be interacting with this file directly. </li> </ol>"},{"location":"lab010/lab010-5/","title":"Deploying an Application on the Service Mesh","text":"<p>The first thing we will do is deploy our application - Bookinfo. Bookinfo is a sample application provided by Istio, the upstream project from which OpenShift Service Mesh is built.</p> <p>The application displays information about a book, similar to a single catalog entry of an online bookstore. Displayed on the page is a description of the book, book details (ISBN, number of pages, and so on), and a few book reviews.</p> <p>The Bookinfo application is broken into four separate microservices:</p> <ul> <li>productpage. The productpage microservice calls the details and reviews microservices to populate the page.</li> <li>details. The details microservice contains book information.</li> <li>reviews. The reviews microservice contains book reviews. It also calls the ratings microservice.</li> <li>ratings. The ratings microservice contains book ranking information that accompanies a book review.</li> </ul> <p>There are 3 versions of the reviews microservice:</p> <ul> <li>Version v1 doesn\u2019t call the ratings service.</li> <li>Version v2 calls the ratings service and displays each rating as 1 to 5 black stars.</li> <li>Version v3 calls the ratings service and displays each rating as 1 to 5 red stars.</li> </ul> <p>The end-to-end architecture of the application is shown below.</p> <p></p> <p>Information</p> <p>This application is polyglot, i.e., the microservices are written in different languages. It\u2019s worth noting that these microservices have no dependencies on the Istio or OSSM, but make an interesting Service Mesh example, particularly because of the multitude of microservices, languages and versions for the reviews microservice.</p> <p>To run Bookinfo on the Service Mesh requires no changes to the application itself. You simply need to run the application in a Service Mesh-enabled environment, with Envoy sidecars injected alongside each microservice.</p> <p>Our OpenShift environment is Service Mesh enabled. OSSM is already installed in our OpenShift cluster and is configured to watch for Service Mesh-enabled deployments to appear in your project, userNN-project. The Service Mesh knows which project to watch through an instance of the Istio Service Mesh Member Roll custom resource. Your userNN ID is not able to access this custom resource, but it is displayed below for your reference.</p> Example Output <pre><code>user01@lab061:~/istio-s390x$ oc describe ServiceMeshMemberRoll/default\nName:         default\nNamespace:    istio-system\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  maistra.io/v1\nKind:         ServiceMeshMemberRoll\nSpec:\nMembers:\n    user01-project\n    user02-project\n    user03-project\n    user04-project\n    user05-project\nStatus:\nAnnotations:\n    Configured Member Count:  5/5\nConditions:\n    Last Transition Time:  2021-06-28T19:04:47Z\n    Message:               All namespaces have been configured successfully\n    Reason:                Configured\n    Status:                True\n    Type:                  Ready\nConfigured Members:\n    user01-project\n    user02-project\n    user03-project\n    user04-project\n    user05-project\nMesh Generation:          1\nMesh Reconciled Version:  2.0.6-2.el8-1\nObserved Generation:      6\nPending Members:\nEvents:  &lt;none&gt;\n</code></pre> Note <p>The screenshot above has been trimmed down for brevity. User projects up to user30-project are configured.</p> <p>This custom resource will watch each member project and automatically inject Envoy sidecars when a deployment is created with the annotation <code>sidecar.istio.io/inject: \"true\"</code>.</p> <p>The deployments contained in the GitHub repository you pulled already have this annotation configured.</p> <ol> <li> <p>From the Istio-s390x directory, view the application deployments with the command:</p> <pre><code>cat platform/kube/bookinfo.yaml\n</code></pre> <p>You will find that this YAML file contains a Service, a ServiceAccount, and a Deployment for each of the microservices described previously. Find a section of the YAML file that has kind: Deployment, and you will see the <code>sidecar.istio.io/inject: \"true\"</code> annotation in its spec section.</p> Example Output <pre><code>    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n    name: details-v1\n    labels:\n        app: details\n        version: v1\n    spec:\n    replicas: 1\n    selector:\n        matchLabels:\n        app: details\n        version: v1\n    template:\n        metadata:\n        annotations:\n            sidecar.istio.io/inject: \"true\"\n        labels:\n            app: details\n            version: v1\n        spec:\n        serviceAccountName: bookinfo-details\n        containers:\n        - name: details\n            image: quay.io/mmondics/examples-bookinfo-details-v1:1.16.2\n            imagePullPolicy: IfNotPresent\n            ports:\n            - containerPort: 9080\n</code></pre> <p>Once created, this YAML file will set up the entire Bookinfo application for you and the Envoy proxy sidecars will be automatically injected for the Istio control plane to interact with.</p> </li> <li> <p>Create the application by running the following command:</p> <pre><code>oc create -f platform/kube/bookinfo.yaml\n</code></pre> Example Output <pre><code>    user01@lab061:~/istio-s390x$ oc create -f platform/kube/bookinfo.yaml\n    oc create -f platform/kube/bookinfo.yaml \n    service/details created\n    serviceaccount/bookinfo-details created\n    deployment.apps/details-v1 created\n    service/ratings created\n    serviceaccount/bookinfo-ratings created\n    deployment.apps/ratings-v1 created\n    service/reviews created\n    serviceaccount/bookinfo-reviews created\n    deployment.apps/reviews-v1 created\n    deployment.apps/reviews-v2 created\n    deployment.apps/reviews-v3 created\n    service/productpage created\n    serviceaccount/bookinfo-productpage created\n    deployment.apps/productpage-v1 created\n</code></pre> <p>Although not yet accessible, all of the application components should be up and running within a few seconds.</p> </li> <li> <p>Check that your four services were created with:</p> <pre><code>oc get services\n</code></pre> Example Output <pre><code>    user01@lab061:~/istio-s390x$ oc get services\n    NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\n    details       ClusterIP   172.30.237.245   &lt;none&gt;        9080/TCP   10s\n    productpage   ClusterIP   172.30.73.57     &lt;none&gt;        9080/TCP   10s\n    ratings       ClusterIP   172.30.241.30    &lt;none&gt;        9080/TCP   10s\n    reviews       ClusterIP   172.30.34.195    &lt;none&gt;        9080/TCP   10s\n</code></pre> </li> <li> <p>And check that your six pods have been created with:</p> <pre><code>oc get pods\n</code></pre> Example Output <pre><code>    user01@lab061:~/istio-s390x$ oc get services\n    NAME                              READY   STATUS    RESTARTS   AGE\n    details-v1-78cb8b797f-ndqdz       2/2     Running   0          45s\n    productpage-v1-568ddd75bf-rmqtf   2/2     Running   0          45s\n    ratings-v1-768dc65999-q5dft       2/2     Running   0          45s\n    reviews-v1-6cf69f46c9-wbtqm       2/2     Running   0          45s\n    reviews-v2-64fd74bbd7-hx4rd       2/2     Running   0          46s\n    reviews-v3-8cb65c475-wr22r        2/2     Running   0          46s\n</code></pre> <p>Each pod should have a STATUS: Running and should show Ready: 2/2. The 2/2 indicates that the pod includes two containers, and both are ready. In our case, each pod has one container for its application, and one container for the Envoy proxy, or sidecar.</p> Optional <p>You can list the containers in your productpage pod with the command:</p> <pre><code>oc get pod productpage-v1-xxxxx -o jsonpath={.spec.containers[*].name}\n</code></pre> <p>(where xxxxx is your randomly generated string of characters returned by oc get pods)</p> <pre><code>user01@lab061:~/istio-s390x$ oc get pod productpage-v1-78797f -o jsonpath={.spec.containers[*].name}\nproductpage istio-proxyuser01@lab061:~/istio-s390x$\n</code></pre> <p>Your two container names will be printed at the beginning of the next line - productpage, and Istio-proxy.</p> <p>productpage is the end user\u2019s primary ingress into the Bookinfo application. This is the microservice that will pull the data from the ratings, reviews, and details microservices and display them when requested by a user.</p> </li> <li> <p>Check that the productpage microservice is running correctly by issuing the command:</p> <pre><code>oc exec \"$(oc get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')\" -c ratings -- curl -sS productpage:9080/productpage | grep -o \"&lt;title&gt;.*&lt;/title&gt;\"\n</code></pre> Example Output <pre><code>    user01@lab061:~/istio-s390x$ oc exec \"$(oc get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')\" -c ratings -- curl -sS productpage:9080/productpage | grep -o \"&lt;title&gt;.*&lt;/title&gt;\"\n    &lt;title&gt;Simple Bookstore App&lt;/title&gt;\n</code></pre> <p>If you see <code>&lt;title&gt;Simple Bookstore App&lt;/title&gt;</code> returned, your Bookinfo application is working correctly, yet still inaccessible.</p> </li> </ol> <p>In the next section, we will create a Gateway to provide ingress into the Service Mesh application.</p>"},{"location":"lab010/lab010-6/","title":"Understanding and Deploying a Service Mesh Gateway and VirtualService","text":"<p>Gateways are used to manage inbound and outbound traffic for your mesh, letting you specify which traffic you want to enter or leave the mesh. Gateway configurations are applied to standalone Envoy proxies that are running at the edge of the mesh, rather than sidecar Envoy proxies running alongside your service workloads.</p> <p></p> <p>Unlike other mechanisms for controlling traffic entering your systems, such as the Kubernetes Ingress APIs, Istio gateways let you use the full power and flexibility of Istio\u2019s traffic routing. You can do this because Istio\u2019s Gateway resource just lets you configure layer 4-6 load balancing properties such as ports to expose, TLS settings, and so on. Then instead of adding application-layer traffic routing (L7) to the same API resource, you bind a regular Istio virtual service to the gateway. This lets you basically manage gateway traffic like any other data plane traffic in an Istio mesh.</p> <p>Gateways are primarily used to manage ingress traffic, but you can also configure egress gateways. An egress gateway lets you configure a dedicated exit node for the traffic leaving the mesh, letting you limit which services can or should access external networks, or to enable secure control of egress traffic to add security to your mesh, for example. We will only be deploying an ingress Gateway in this lab.</p> <p>Along with a Gateway, we will need a VirtualService. A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry.</p> <ol> <li> <p>You will find a YAML file for a Gateway and VirtualService in your terminal session. From the istio-s390x directory, view the YAML file with the command:</p> <pre><code>cat networking/bookinfo-gateway.yaml\n</code></pre> Example Output <pre><code>    apiVersion: networking.istio.io/v1alpha3\n    kind: Gateway\n    metadata:\n    name: bookinfo-gateway\n    spec:\n    selector:\n        istio: ingressgateway # use default controller\n    servers:\n    - port:\n        number: 80\n        name: http\n        protocol: HTTP\n        hosts:\n        - \"userNN-project.istio.apps.atsocppa.dmz\"\n    ---\n    apiVersion: networking.istio.io/v1alpha3\n    kind: VirtualService\n    metadata:\n    name: bookinfo\n    spec:\n    hosts:\n    - \"userNN-project.istio.apps.atsocppa.dmz\"\n    gateways:\n    - bookinfo-gateway\n    http:\n    - match:\n        - uri:\n            exact: /productpage\n        - uri:\n            prefix: /static\n        - uri:\n            exact: /login\n        - uri:\n            exact: /logout\n        - uri:\n            prefix: /api/v1/products\n        route:\n        - destination:\n            host: productpage\n            port:\n            number: 9080\n</code></pre> <p>You will notice that there are a few instances of userNN in the YAML file that must be edited to match your user number. You can quickly change these by entering the following command.</p> Expand for more information <p>Extra information for those interested\u2026</p> <p>The hosts field lists the VirtualService\u2019s hosts - in other words, the user-addressable destination or destinations that these routing rules apply to. This is the address or addresses used when sending requests to the service.</p> <p>The virtual service hostname can be an IP address, a DNS name, or, depending on the platform, a short name (such as a Kubernetes service short name) that resolves to a fully qualified domain name (FQDN). You can also use wildcard (\u201d*\u201d) prefixes, letting you create a single set of routing rules for all matching services.</p> <p>The http section contains the virtual service\u2019s routing rules, describing match conditions and actions for routing HTTP/1.1, HTTP2, and gRPC traffic sent to the destination(s) specified in the hosts field (you can also use tcp and tls sections to configure routing rules for TCP and unterminated TLS traffic). A routing rule consists of the destination where you want the traffic to go and zero or more match conditions, depending on your use case.</p> <p>The route section\u2019s destination field specifies the actual destination for traffic that matches this condition. Unlike the VirtuallService\u2019s host(s), the destination\u2019s host must be a real destination that exists in Istio\u2019s service registry or Envoy won\u2019t know where to send traffic to it. This can be a mesh service with proxies or a non-mesh service added using a service entry. In this case we\u2019re running on Kubernetes and the host name is a Kubernetes service name.</p> </li> <li> <p>Make sure that you change  to the correct number, i.e. 01 for user01 <pre><code>sed -i 's/NN/&lt;YOUR_USER_NUMBER&gt;/g' networking/bookinfo-gateway.yaml\n</code></pre> <li> <p>Create the Gateway and VirtualService with the following command:</p> <pre><code>oc create -f networking/bookinfo-gateway.yaml\n</code></pre> Example Output <pre><code>user01@lab061:~/istio-s390x$ oc create -f networking/bookinfo-gateway.yaml\ngateway.networking.istio.io/bookinfo-gateway created\nvirtualservice.networking.istio.io/bookinfo created\n</code></pre> </li> <li> <p>And view the new objects with the command:</p> <pre><code>oc get gateway,virtualservice\n</code></pre> <p>Note</p> <p>This command will not work if there is a space after the comma.</p> Example Output <pre><code>user01@lab061:~/istio-s390x$ oc get gateway,virtualservice\nNAME                       AGE\ngateway/bookinfo-gateway   42s\n\nNAME                      GATEWAYS               HOST               \nvirtualservice/bookinfo   [\"bookinfo-gateway\"]   [\"user15-project.istio.apps.atsocppa.dmz\"]\n</code></pre> <p>With this Gateway and VirtualService, you are now able to access the application.</p> </li> <li> <p>First, identify your Gateway URL by entering the following command in your terminal:</p> <pre><code>export GATEWAY_URL=$(oc get virtualservice bookinfo -o jsonpath='{.spec.hosts[0]}')\n</code></pre> </li> <li> <p>Next, enter the following command to print your productpage URL:</p> <pre><code>echo \"http://$GATEWAY_URL/productpage\"\n</code></pre> Example Output <pre><code>user01@lab061:~/istio-s390x$ export GATEWAY_URL=$(oc get virtualservice bookinfo -o jsonpath='{.spec.hosts[0]}')\nuser01@lab061:~/istio-s390x$ echo \"http://$GATEWAY_URL/productpage\"\nuser01@lab061:~/istio-s390x$ http://user15-project.istio.apps.atsocppa.dmz/productpage\n</code></pre> </li> <li> <p>Copy the URL that is returned, and paste it into a web browser</p> <p>Hint</p> <p>The URL will be similar to http://userNN-project.istio.apps.atsocppa.dmz/productpage, (where NN is your user number).</p> <p></p> <p>If all is working correctly, the overall productpage is shown, which displays information about William Shakespeare\u2019s play, The Comedy of Errors. The details on the left are returned by the details microservice, the text for the two reviews is provided by the ratings microservice, and the star ratings (or lack thereof) are returned by one of the three reviews microservices, depending on which was called by the ratings microservice.</p> <p>You now have an application made up of six microservices written in four different languages deployed on the OpenShift Service Mesh and can now take full advantage of its features. We will look at a subset of them in the following sections.</p> </li>"},{"location":"lab010/lab010-7/","title":"Traffic Management","text":"<p>Istio\u2019s traffic routing rules let you easily control the flow of traffic and API calls between services. Istio simplifies configuration of service-level properties like circuit breakers, timeouts, and retries, and makes it easy to set up important tasks like A/B testing, canary rollouts, and staged rollouts with percentage-based traffic splits. It also provides out-of-box failure recovery features that help make your application more robust against failures of dependent services or the network.</p> <ol> <li> <p>Refresh the productpage a few times and you will see either black stars, red stars, or no stars.</p> <p>This is because, by default, Istio uses a round-robin load balancing policy, where each service instance in the instance pool gets a request in turn. Istio also supports the following models, which you can specify in destination rules for requests to a particular service or service subset.</p> <ul> <li>Random: Requests are forwarded at random to instances in the pool.</li> <li>Weighted: Requests are forwarded to instances in the pool according to a specific percentage.</li> <li>Least requests: Requests are forwarded to instances with the least number of requests.</li> </ul> <p>Along with VirtualServices, DestinationRules are a key part of Istio\u2019s traffic routing functionality. You can think of virtual services as how you route your traffic to a given destination, and then you use destination rules to configure what happens to traffic for that destination. Destination rules are applied after virtual service routing rules are evaluated, so they apply to the traffic\u2019s \u201creal\u201d destination.</p> </li> <li> <p>There is a DestinationRule YAML file provided in the networking directory. Take a look at it with:</p> <pre><code>cat networking/destination-rule-all.yaml\n</code></pre> Example Output <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\nname: productpage\nspec:\nhost: productpage\nsubsets:\n- name: v1\n    labels:\n    version: v1\n---\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\nname: reviews\nspec:\nhost: reviews\nsubsets:\n- name: v1\n    labels:\n    version: v1\n- name: v2\n    labels:\n    version: v2\n- name: v3\n    labels:\n    version: v3\n---\n    More cut from screenshot     \n</code></pre> <p>This will be our baseline DestinationRules with no special routing or load balancing included. These DesintationRules simply describe the various versions of each microservce (v1, v2, v3).</p> </li> <li> <p>Create the DestinationRules with the command:</p> <pre><code>oc create -f networking/destination-rule-all.yaml\n</code></pre> Example Output <pre><code>user01@lab061:~/istio-s390x$ oc create -f networking/destination-rule-all.yaml\ndestinationrule.networking.istio.io/productpage created\ndestinationrule.networking.istio.io/reviews created\ndestinationrule.networking.istio.io/ratings created\ndestinationrule.networking.istio.io/details created\n</code></pre> <p>Now that OpenShift knows which versions of each microservice are available, we can use Istio to control the version routing.</p> <p>For example, each of the microservices in Bookinfo application include a version v1. We can deploy a new VirtualService that routes all traffic to the v1 microservices.</p> </li> <li> <p>Look at the v1-specific VirtualService with the command:</p> <pre><code>cat networking/virtual-service-all-v1.yaml\n</code></pre> Example Output <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: productpage\nspec:\nhosts:\n- productpage\nhttp:\n- route:\n    - destination:\n        host: productpage\n        subset: v1\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: reviews\nspec:\nhosts:\n- reviews\nhttp:\n- route:\n    - destination:\n        host: reviews\n        subset: v1\n---\n            More cut from screenshot\n</code></pre> <p>And notice that v1 is the only version specified for each microservice.</p> </li> <li> <p>Create the new VirtualService with the command:</p> <pre><code>oc apply -f networking/virtual-service-all-v1.yaml\n</code></pre> Example Output <pre><code>user01@lab061:~/istio-s390x$ oc apply -f networking/virtual-service-all-v1.yaml \nvirtualservice.networking.istio.io/productpage created\nvirtualservice.networking.istio.io/reviews created\nvirtualservice.networking.istio.io/ratings created\nvirtualservice.networking.istio.io/details created\n</code></pre> <p>You have just configured Istio to route all traffic to the v1 version of each microservice, most importantly the reviews microservice that decides which stars are displayed on the productpage.</p> </li> <li> <p>Back in your web browser, refresh the productpage a few times.</p> <p></p> <p>You should notice now that no matter how many times you refresh, no stars will be displayed, because the new VirtualService only allows you to reach v1 of reviews.</p> <p></p> <p>You can also control the route configuration so that all traffic from a specific user is routed to a specific service version. In this case, all traffic from a user named Jason will be routed to the service reviews:v2.</p> </li> <li> <p>Run the following command to enable user-based routing:</p> <pre><code>oc apply -f networking/virtual-service-reviews-test-v2.yaml\n</code></pre> </li> <li> <p>In your web browser, log into the productpage as user: <code>Jason</code> with password: <code>Jason</code>.</p> </li> <li> <p>Refresh the productpage a few times again and notice that Jason is only able to reach v2 of the reviews microservice, which displays black stars.</p> <p></p> </li> <li> <p>On the productpage, sign out, then sign in with a different user: <code>Richard</code> with password: <code>Richard</code>.</p> </li> <li> <p>Refresh the productpage a few times again and notice that Richard can only reach v1 of the reviews microservice, which does not display stars.</p> </li> </ol> <p>In this task, you used Istio to send 100% of the traffic to the v1 version of each of the Bookinfo services. You then set a rule to selectively send traffic to version v2 of the reviews service based on a custom end-user header (for Jason) added to the request by the productpage service.</p>"},{"location":"lab010/lab010-8/","title":"Application Observability with Kiali","text":"<p>Kiali provides visibility into your service mesh by showing you the microservices in your service mesh, and how they are connected.</p> <p>Kiali provides an interactive graph view of your namespace in real time that provides visibility into features like circuit breakers, request rates, latency, and even graphs of traffic flows. Kiali offers insights about components at different levels, from Applications to Services and Workloads, and can display the interactions with contextual information and charts on the selected graph node or edge. Kiali also provides the ability to validate your Istio configurations, such as gateways, destination rules, virtual services, mesh policies, and more.</p> <ol> <li> <p>Navigate to the Kiali console located at: https://kiali-istio-system.apps.atsocppa.dmz</p> </li> <li> <p>Log in with your OpenShift credentials.</p> </li> <li> <p>View the overview of your mesh in the Overview page that appears immediately after you log in.</p> <p>The Overview page displays all the namespaces that have services in the mesh.</p> <p></p> <p>The namespaces shown are those previously discussed in the Service Mesh Member Roll.</p> </li> </ol>"},{"location":"lab010/lab010-8/#validating-istio-configuration-with-kiali","title":"Validating Istio Configuration with Kiali","text":"<p>Kiali can validate your Istio resources to ensure they follow proper conventions and semantics. Any problems detected in the configuration of your Istio resources can be flagged as errors or warnings depending on the severity of the incorrect configuration.</p> <p>You might have noticed on the Kiali overview page that there is an error somewhere within your userNN-project Istio configuration.</p> <p></p> <ol> <li> <p>Let\u2019s find and fix that error. In the left-side menu of the Kiali console, navigate to Istio Config, and then filter to your userNN-project namespace.</p> <p></p> <p>You should notice an error icon in the Configuration column for your details microservice.</p> </li> <li> <p>Click the details hyperlink to drill down into the microservice and scroll to the bottom of the YAML file.</p> <p></p> </li> <li> <p>There is an error with v2 of the details DestinationRule. For more specificity, hover over the red X to the left of the YAML file.</p> <p></p> <p>You will see the error code KIA0203 This subset\u2019s labels are not found in any matching host. Essentially, the details DestinationRule is failing to find the v2 host for the details microservice.</p> <p>If you look back at our Bookinfo application architecture, there is only supposed to be one version of the details microservice - v1 is the only version that exists.</p> </li> <li> <p>Correct the error by deleting lines 30-32 of the YAML file, resulting in the following:</p> <p></p> </li> <li> <p>Click on the Istio Config tab in the left-side menu again to navigate back to the configuration page for your project.</p> <p>You should no longer have any configuration issues in your Istio configuration.</p> <p></p> </li> </ol>"},{"location":"lab010/lab010-8/#validating-service-mesh-application-configuration-with-kiali","title":"Validating Service Mesh Application Configuration with Kiali","text":"<p>Along with identifying issues with the Istio configuration, Kiali can also identify issues with the applications running on the Service Mesh.</p> <ol> <li> <p>In your terminal session, introduce an invalid configuration of a service port name with the following command:</p> <pre><code>oc patch service details --type json -p \\\n'[{\"op\":\"replace\",\"path\":\"/spec/ports/0/name\", \"value\":\"foo\"}]'\n</code></pre> Example Output <pre><code>user01@lab061:~/istio-s390x$ oc patch service details --type json -p \\\n&gt; '[{\"op\":\"replace\",\"path\":\"/spec/ports/0/name\", \"value\":\"foo\"}]'\nservice/details patched\n</code></pre> <p>If you see the <code>service/details patched</code> message as in the image above, your patch was successful.</p> </li> <li> <p>Back in the Kiali web console, navigate to the Services page from the left side menu.</p> <p></p> <p>You will notice that you now have an error icon under the Configuration column for the details service.</p> </li> <li> <p>Click the details hyperlink and then click the Network option under Service Info.</p> <p></p> </li> <li> <p>Hover over the error icon to display a tool tip describing the error.</p> <p></p> <p>This error is telling you that your port name does not follow the correct syntax.</p> </li> <li> <p>Back in your terminal session, run the following command to correct the port name:</p> <pre><code>    oc patch service details--type json -p \\\n    '[{\"op\":\"replace\",\"path\":\"/spec/ports/0/name\", \"value\":\"http\"}]'\n</code></pre> Example Output <pre><code>user01@lab061:~/istio-s390x$ oc patch service details --type json -p \\\n&gt; '[{\"op\":\"replace\",\"path\":\"/spec/ports/0/name\", \"value\":\"http\"}]'\nservice/details patched\n</code></pre> </li> <li> <p>Back in the Kiali console, click the blue refresh button and see that your error has been fixed.</p> <p></p> </li> </ol> <p>Now that our Service Mesh is configured correctly, along with the application running on top of it, let\u2019s explore some of the other features that Kiali provides.</p>"},{"location":"lab010/lab010-8/#viewing-your-service-mesh-applications-with-kiali","title":"Viewing Your Service Mesh Applications with Kiali","text":"<ol> <li> <p>To view your namespace graph, Select the Graph option in the left side menu and select your project in the namespace dropdown. The page looks similar to:</p> <p></p> <p>The graph represents traffic flowing through the service mesh for a period of time, generated using Istio telemetry.</p> </li> <li> <p>Let\u2019s generate some traffic into our Bookinfo application. In your terminal session, run the following command to continually send http request to the productpage:</p> <pre><code>watch -n 1 curl -o /dev/null -s -w %{http_code} $GATEWAY_URL/productpage\n</code></pre> Example Output <pre><code>user01@lab061:~/istio-s390x$ watch -n 1 curl -o /dev/null -s -w %{http_code} $GATEWAY_URL/productpage\nEvery 1.0s: curl -o /dev/null -s -w %{http_code} user01-project.istio.apps.atsocppa.dmz/productpage\n\n200\n</code></pre> <p>If you see a 200 return code as in the image above, you\u2019re now sending requests to the productpage every second.</p> <p> Important </p> <p>Note: you will want to leave this watch command running until otherwise directed.</p> </li> <li> <p>Open a second terminal session and connect to the environment as directed previously in this lab.</p> </li> <li> <p>To view a summary of metrics, select any node or edge in the graph to display its metric details in the summary details panel on the right.</p> <p>For example, click on the triangle representing the productpage service, and you should see a 100% success rate for both inbound and outbound traffic.</p> <p></p> </li> <li> <p>On this same page, click the Display dropdown and select Requests Percentage and Service Nodes, if they aren\u2019t already selected.</p> <p></p> <p>This will let you view the percentage of traffic to each workload in near real-time. For example, since there are three versions of the reviews microservice and traffic is being distributed in a round-robin fashion, you should see close to 33% traffic going from the productpage to each version of reviews.</p> <p></p> <p>Your percentages are likely not exactly 33.3%, as in the image above. More often, they will vary between 20% and 40%.</p> </li> </ol>"},{"location":"lab010/lab010-8/#managing-service-mesh-applications-with-kiali","title":"Managing Service Mesh Applications with Kiali","text":"<p>Kiali does not only allow visibility into your service mesh application, Kiali can also be used to directly interact with the application.</p> <ol> <li> <p>Click the reviews service represented by a triangle. In the right-side menu that pops up, click on the hyperlink for the reviews service.</p> <p></p> <p>You are taken to the Services page (instead of Graph, where you previously were). This shows expanded information about the reviews service, its properties and versions, metrics about its traffic, and more.</p> </li> <li> <p>Click the Actions dropdown in the top right of the page to see the traffic management options you have - Request Routing, Fault Injection, Traffic Shifting, and Request Timeouts.</p> <p>These should all be grayed out right now because these are managed through DestinationRules, and we already created one for the reviews service in a previous step.</p> </li> <li> <p>Delete the Traffic Routing from this Actions dropdown and confirm that you want to delete the DestinationRule: \u2018reviews\u2019.</p> <p></p> </li> <li> <p>Click the Actions dropdown again and you will notice that the options can now be selected and click the Traffic Shifting option.</p> <p></p> <p>From this page, you can create a new Traffic Shifting rule to manage how much traffic should be directed to each version of the reviews microservice.</p> </li> <li> <p>Slide the slider for reviews-v3 all the way to the left for 0%, and then make reviews-v1 and reviews-v2 50% and click create. Your page should look like the image below.</p> <p></p> </li> <li> <p>Navigate back to the Graph page from the left side menu.</p> <p>Over the next few minutes (and depending on the graph\u2019s refresh rate that you can edit in the top right of the Graph page) you will see that the percentage of traffic going to v3 of reviews will decrease towards 0%, while the traffic going to v1 and v2 will increase towards 50%.</p> <p></p> </li> <li> <p>In your web browser, navigate back to your bookinfo productpage and refresh the page a few times.</p> <p>No matter how many times you refresh, you will not see the red stars again. That is because no traffic can reach v3 of the reviews microservice, which is the version that provides red stars.</p> </li> </ol> <p>As you can tell from the past few sections, you can control Service Mesh applications either from the Command Line by creating VirtualServices and DestinationRules, or by using the Kiali GUI console. Using the Kiali console simply generates the VirtualServices and DestinationRules for you, however the Command Line offers greater flexibility, more control, and the ability to automate the creation of these rules.</p> <p>We will now move on from Kiali to Jaeger, the tool that OSSM uses for distributed tracing. At this point, feel free to explore Kiali and the other functions it provides. There are many things Kiali can do that we will not be covering in this lab. You can find more information in the Kiali documentation here: https://kiali.io/documentation/latest/features/</p>"},{"location":"lab010/lab010-9/","title":"Distributed Tracing with Jaeger","text":"<p>Distributed Tracing is the process of tracking the performance of individual microservices in an application by tracing the path of the service calls in the application. Each time a user takes action in an application, a request is executed that might require many microservices to interact to produce a response.</p> <p>Jaeger is an open source distributed tracing system used by OpenShift Service Mesh. With Jaeger, you can perform a trace that follows the path of a request through various microservices which make up an application.</p> <p>For our Bookinfo application, traces are generated when HTTP requests are made to the productpage microservice. This starts a cascade of requests to the other microservices in the Bookinfo mesh.</p> <ol> <li> <p>Before we start looking at traces, check that your watch command is still running in your terminal session.</p> Example Output <pre><code>Every 1.0s: curl -o /dev/null -s -w %{http_code} user01-project.istio.apps.atsocppa.dmz/productpage\n\n200\n</code></pre> <p>If you see the 200 status code return and the time in the top right is current, you are still sending requests to your productpage and will be able to generate traces.</p> <p>Note</p> <p>If your watch command has been stopped for whatever reason, start it again with the command:</p> <pre><code>watch -n 1 curl -o /dev/null -s -w %{http_code} $GATEWAY_URL/productpage\n</code></pre> <ol> <li>Navigate to the Jaeger console located at: https://jaeger-istio-system.apps.atsocppa.dmz</li> </ol> </li> <li> <p>Login with your OpenShift credentials.</p> <p></p> </li> <li> <p>In the Service dropdown, select <code>productpage.userNN-project</code>.</p> <p>If others are doing this same lab, it might be easier to search for your userNN-project.</p> <p></p> </li> <li> <p>Scroll to the bottom-left of the page and click the Find Traces button.</p> <p></p> <p>The rest of the page will be populated by a graph of your traces over time, and the traces that meet your search criteria. You will notice that some of your traces have much longer durations than others - don\u2019t worry about these. This is due to hard-coded http timeouts in the productpage python application code.</p> <p>The graph displayed at the top of the page has circles representing traces, with time on the x-axis and duration of the trace on the y-axis. The size of the circle represents how many spans make up the trace. A span is the logical unit of work associated with one microservice in the mesh.</p> <p></p> </li> <li> <p>Click one of the traces in the list below the graph. Select a trace that has 8 spans, as described in the left side of the box.</p> <p></p> <p>You will be taken to a page that looks like the following:</p> <p></p> <p>This graph shows how long each microservice took, when it started, when it ended, and includes detailed information about each span.</p> </li> <li> <p>Expand the span for your productpage, and then expand the Tags row.</p> <p></p> <p>Here you will find more information about the productpage span that may be helpful with debugging an application issue or latency.</p> </li> <li> <p>Click the back arrow in the top left of the page to navigate back to your project traces.</p> <p></p> <p>Jaeger includes a feature to compare two traces to one another.</p> </li> <li> <p>In your list of traces, find one trace that has 8 spans, and another that has 6 spans.</p> <p></p> </li> <li> <p>Click the checkboxes next to the names of the traces, and then Compare Traces to the top right of the traces list.</p> <p></p> <p>The resulting page shows the microservices that exist in trace A, trace B, and both traces you selected.</p> <p></p> <p>Your comparison will likely look different. Any gray nodes are microservices that exist in both traces at the same version. Any red nodes exist in Trace A, but not Trace B. Any green nodes exist in Trace B, but not Trace A. If you\u2019re comparing the Traces in the screenshot above, you can discern that the ratings microservice is not being called in Trace B, and that the versions of the reviews microservices are different in the two traces.</p> </li> </ol> Note <p>The distributed tracing sampling rate is set to sample 100% of traces in your Service Mesh by default. A high sampling rate consumes cluster resources and performance but is useful when debugging issues. Before you deploy Red Hat OpenShift Service Mesh in production, you would want to set the value to a smaller proportion of traces.</p>"}]}